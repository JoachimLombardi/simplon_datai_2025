{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the links, the titles and the paragraphs of the articles from the main article and sort them by similarity to the main article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'link': 'https://en.wikipedia.org/wiki/machine_learning',\n",
       " 'title': 'Machine learning',\n",
       " 'paragraph': 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\\n'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the main article\n",
    "requete = requests.get('https://en.wikipedia.org/wiki/machine_learning')\n",
    "page = BeautifulSoup(requete.text, 'html.parser')\n",
    "wiki_main = {}\n",
    "wiki_main['link'] = 'https://en.wikipedia.org/wiki/machine_learning'\n",
    "wiki_main['title'] = page.find('h1').text\n",
    "wiki_main['paragraph'] = page.find('p').text\n",
    "wiki_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the links of the articles\n",
    "links = page.find_all('a')\n",
    "http_links = [f\"{link.get('href')}\" for link in links if link.get('href') and link.get('href').startswith('/wiki')]  \n",
    "wiki_list = []\n",
    "wiki_dict_sans_doublon = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'link': '/wiki/Main_Page', 'title': 'Main Page', 'paragraph': 'Florence Petty (1\\xa0December 1870\\xa0– 18\\xa0November 1948) was a Scottish social worker, cookery writer and broadcaster. During the 1900s she undertook social work in the deprived area of Somers Town in North London, demonstrating for working-class women how to cook inexpensive and nutritious foods. Much of the instruction was done in their homes. She published cookery-related works aimed at those also involved in social work, and a cookery book and pamphlet aimed at the public. From 1914 until the mid-1940s she toured Britain giving lecture-demonstrations of cost-efficient and nutritious ways to cook, including dealing with food shortages during the First World War. In the late 1920s and early 1930s, she was a BBC broadcaster on food and budgeting. Petty worked until she was in her seventies. She is considered to be a pioneer of social work innovations. Her approach to teaching the use of cheap nutritious food was a precursor to the method adopted by the Ministry of Food during the Second World War. (Full\\xa0article...)\\n'}, {'link': '/wiki/Wikipedia:Contents', 'title': 'Wikipedia:Contents', 'paragraph': '\\n'}, {'link': '/wiki/Portal:Current_events', 'title': 'Portal:Current events', 'paragraph': 'Edit instructions\\n'}, {'link': '/wiki/Special:Random', 'title': 'Inger Wikström', 'paragraph': 'Inger Wikstrom (born 11 December 1939) is a Swedish pianist, composer and conductor.\\n'}, {'link': '/wiki/Help:Contents', 'title': 'Help:Contents', 'paragraph': '\\n\\n'}, {'link': '/wiki/Special:RecentChanges', 'title': 'Recent changes', 'paragraph': 'This is a list of recent changes to Wikipedia.\\n'}, {'link': '/wiki/Wikipedia:File_upload_wizard', 'title': 'Wikipedia:File upload wizard', 'paragraph': 'Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand copyright and the image use policy before proceeding.\\n'}, {'link': '/wiki/Special:MyContributions', 'title': 'User contributions for 2001:861:4441:AB40:B52C:4689:9091:A7BF', 'paragraph': 'No changes were found matching these criteria.\\n'}, {'link': '/wiki/Special:MyTalk', 'title': 'User talk:2001:861:4441:AB40:B52C:4689:9091:A7BF', 'paragraph': 'People on Wikipedia can use this talk page to post a public message about edits made from the IP address you are currently using.\\n'}, {'link': '/wiki/Machine_learning', 'title': 'Machine learning', 'paragraph': 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\\n'}, {'link': '/wiki/Special:WhatLinksHere/Machine_learning', 'title': 'Pages that link to \"Machine learning\"', 'paragraph': 'The following pages link to Machine learning \\n'}, {'link': '/wiki/Special:RecentChangesLinked/Machine_learning', 'title': 'Related changes', 'paragraph': 'Enter a page name to see changes on pages linked to or from that page. (To see members of a category, enter Category:Name of category). Changes to pages on your Watchlist are shown in bold with a green bullet. See more at Help:Related changes.\\n'}, {'link': '/wiki/Special:SpecialPages', 'title': 'Special pages', 'paragraph': 'This page contains a list of special pages. Most of the content of these pages is automatically generated and cannot be edited. To suggest a change to the parts that can be edited, find the appropriate text on Special:AllMessages and then request your change on the talk page of the message (using {{editprotected}} to draw the attention of administrators).\\n'}, {'link': '/wiki/Machine_Learning_(journal)', 'title': 'Machine Learning (journal)', 'paragraph': 'Machine Learning  is a peer-reviewed scientific journal, published since 1986.\\n'}, {'link': '/wiki/Statistical_learning_in_language_acquisition', 'title': 'Statistical learning in language acquisition', 'paragraph': 'Statistical learning is the ability for humans and other animals to extract statistical regularities from the world around them to learn about the environment. Although statistical learning is now thought to be a generalized learning mechanism, the phenomenon was first identified in human infant language acquisition.\\n'}, {'link': '/wiki/Data_mining', 'title': 'Data mining', 'paragraph': 'Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1]\\n'}, {'link': '/wiki/File:Neural_network_with_dark_background.png', 'title': 'File:Neural network with dark background.png', 'paragraph': 'Original file \\u200e(1,280 × 1,039 pixels, file size: 837 KB, MIME type: image/png)\\n'}, {'link': '/wiki/Supervised_learning', 'title': 'Supervised learning', 'paragraph': 'Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data on expected output values.[1]  An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\\n'}, {'link': '/wiki/Unsupervised_learning', 'title': 'Unsupervised learning', 'paragraph': 'Unsupervised learning is a paradigm in machine learning where, in contrast to supervised learning and semi-supervised learning, algorithms learn patterns exclusively from unlabeled data.\\n'}, {'link': '/wiki/Online_machine_learning', 'title': 'Online machine learning', 'paragraph': 'In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.\\n'}, {'link': '/wiki/Meta-learning_(computer_science)', 'title': 'Meta-learning (computer science)', 'paragraph': 'Meta learning[1][2]\\nis a subfield of machine learning where automatic learning algorithms are applied to metadata about machine learning experiments. As of 2017, the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to learn.[1]\\n'}, {'link': '/wiki/Semi-supervised_learning', 'title': 'Weak supervision', 'paragraph': 'Weak supervision is a paradigm in machine learning, the relevance and notability of which increased with the advent of large language models due to large amount of data required to train them.  It is characterized by using a combination of a small amount of human-labeled data (exclusively used in more expensive and time-consuming supervised learning paradigm), followed by a large amount of unlabeled data (used exclusively in unsupervised learning paradigm). In other words, the desired output values are provided only for a subset of the training data. The remaining data is unlabeled or imprecisely labeled. Intuitively, it can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam. Technically, it could be viewed as performing clustering and then labeling the clusters with the labeled data, pushing the decision boundary away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.\\n'}, {'link': '/wiki/Self-supervised_learning', 'title': 'Self-supervised learning', 'paragraph': 'Self-supervised learning (SSL) is a paradigm in machine learning where a model is trained on a task using the data itself to generate supervisory signals, rather than relying on external labels provided by humans. In the context of neural networks, self-supervised learning aims to leverage inherent structures or relationships within the input data to create meaningful training signals. SSL tasks are designed so that solving it requires capturing essential features or relationships in the data. The input data is typically augmented or transformed in a way that creates pairs of related samples. One sample serves as the input, and the other is used to formulate the supervisory signal. This augmentation can involve introducing noise, cropping, rotation, or other transformations. Self-supervised learning more closely imitates the way humans learn to classify objects.[1]\\n'}, {'link': '/wiki/Reinforcement_learning', 'title': 'Reinforcement learning', 'paragraph': 'Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\\n'}, {'link': '/wiki/Rule-based_machine_learning', 'title': 'Rule-based machine learning', 'paragraph': \"Rule-based machine learning (RBML) is a term in computer science intended to encompass any machine learning method that identifies, learns, or evolves 'rules' to store, manipulate or apply.[1][2][3] The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[clarification needed][citation needed]\\n\"}, {'link': '/wiki/Quantum_machine_learning', 'title': 'Quantum machine learning', 'paragraph': 'Quantum machine learning is the integration of quantum algorithms within machine learning programs.[1][2][3][4][5][6][7][8]\\n'}, {'link': '/wiki/Statistical_classification', 'title': 'Statistical classification', 'paragraph': 'In statistics, classification is the problem of identifying which of a set of categories (sub-populations) an observation (or observations) belongs to.  Examples are assigning a given email to the \"spam\" or \"non-spam\" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).\\n'}, {'link': '/wiki/Generative_model', 'title': 'Generative model', 'paragraph': 'In statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent,[a] but three major types can be distinguished, following Jebara (2004):\\n'}, {'link': '/wiki/Regression_analysis', 'title': 'Regression analysis', 'paragraph': \"In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome' or 'response' variable, or a 'label' in machine learning parlance) and one or more independent variables (often called 'predictors', 'covariates', 'explanatory variables' or 'features'). The most common form of regression analysis is linear regression, in which one finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared differences between the true data and that line (or hyperplane). For specific mathematical reasons (see linear regression), this allows the researcher to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values. Less common forms of regression use slightly different procedures to estimate alternative location parameters (e.g., quantile regression or Necessary Condition Analysis[1]) or estimate the conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression).\\n\"}, {'link': '/wiki/Cluster_analysis', 'title': 'Cluster analysis', 'paragraph': 'Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.\\n'}, {'link': '/wiki/Dimensionality_reduction', 'title': 'Dimensionality reduction', 'paragraph': 'Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with). Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics.[1]\\n'}, {'link': '/wiki/Density_estimation', 'title': 'Density estimation', 'paragraph': 'In statistics, probability density estimation or simply density estimation is the construction of an estimate, based on observed data, of an unobservable underlying probability density function.  The unobservable density function is thought of as the density according to which a large population is distributed; the data are usually thought of as a random sample from that population.[1]\\n'}, {'link': '/wiki/Anomaly_detection', 'title': 'Anomaly detection', 'paragraph': 'In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behaviour.[1] Such examples may arouse suspicions of being generated by a different mechanism,[2] or appear inconsistent with the remainder of that set of data.[3]\\n'}, {'link': '/wiki/Data_Cleaning', 'title': 'Data cleansing', 'paragraph': 'Data cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data.[1] Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting or a data quality firewall.\\n'}, {'link': '/wiki/Automated_machine_learning', 'title': 'Automated machine learning', 'paragraph': 'Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. \\n'}, {'link': '/wiki/Association_rule_learning', 'title': 'Association rule learning', 'paragraph': 'Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.[1] In any given transaction with a variety of items, association rules are meant to discover the rules that determine how or why certain items are connected.\\n'}, {'link': '/wiki/Semantic_analysis_(machine_learning)', 'title': 'Semantic analysis (machine learning)', 'paragraph': 'In machine learning, semantic analysis of a corpus is the task of building structures that approximate concepts from a large set of documents. It generally does not involve prior semantic understanding of the documents. A metalanguage based on predicate logic can analyze the speech of humans.[1]:\\u200a93-\\u200a Another strategy to understand the semantics of a text is symbol grounding. If language is grounded, it is equal to recognizing a machine readable meaning. For the restricted domain of spatial analysis, a computer based language understanding system was demonstrated.[2]:\\u200a123\\u200a\\n'}, {'link': '/wiki/Structured_prediction', 'title': 'Structured prediction', 'paragraph': 'Structured prediction or structured (output) learning is an umbrella term for supervised machine learning techniques that involves predicting structured objects, rather than scalar discrete or real values.[1]\\n'}, {'link': '/wiki/Feature_engineering', 'title': 'Feature engineering', 'paragraph': 'Feature engineering or feature extraction  or feature discovery is the process of extracting features (characteristics, properties, attributes) from raw data.[1] This can be done with deep learning networks such as convolutional neural networks that are able to learn features by themselves.[citation needed]\\n'}, {'link': '/wiki/Feature_learning', 'title': 'Feature learning', 'paragraph': 'In machine learning, feature learning or representation learning[2] is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform  a specific task.\\n'}, {'link': '/wiki/Learning_to_rank', 'title': 'Learning to rank', 'paragraph': 'Learning to rank[1] or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems.[2] Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. \"relevant\" or \"not relevant\") for each item. The goal of constructing the ranking model is to rank new, unseen lists in a similar way to rankings in the training data.\\n'}, {'link': '/wiki/Grammar_induction', 'title': 'Grammar induction', 'paragraph': 'Grammar induction (or grammatical inference)[1] is the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.\\n'}, {'link': '/wiki/Ontology_learning', 'title': 'Ontology learning', 'paragraph': \"Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process.\\n\"}, {'link': '/wiki/Multimodal_learning', 'title': 'Multimodal learning', 'paragraph': 'Multimodal learning, in context of machine learning, is deep learning from a combination of various modalities of data, often arising in real-world applications. An example of multi-modal data is data that combines text (typically represented as feature vector) with imaging data consisting of pixel intensities and annotation tags. As these modalities have fundamentally different statistical properties, combining them is non-trivial, which is why specialized modelling strategies and algorithms are required.\\n'}, {'link': '/wiki/Apprenticeship_learning', 'title': 'Apprenticeship learning', 'paragraph': 'In artificial intelligence, apprenticeship learning (or learning from demonstration or imitation learning) is the process of learning by observing an expert.[1][2] It can be viewed as a form of supervised learning, where the training dataset consists of task executions by a demonstration teacher.[2]\\n'}, {'link': '/wiki/Decision_tree_learning', 'title': 'Decision tree learning', 'paragraph': 'Decision tree learning is a supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations.\\n'}, {'link': '/wiki/Ensemble_learning', 'title': 'Ensemble learning', 'paragraph': 'In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.[1][2][3]\\nUnlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.\\n'}, {'link': '/wiki/Bootstrap_aggregating', 'title': 'Bootstrap aggregating', 'paragraph': 'Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.\\n'}, {'link': '/wiki/Boosting_(machine_learning)', 'title': 'Boosting (machine learning)', 'paragraph': 'In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance[1] in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones.[2] Boosting is based on the question posed by Kearns and Valiant (1988, 1989):[3][4] \"Can a set of weak learners create a single strong learner?\" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\\n'}, {'link': '/wiki/Random_forest', 'title': 'Random forest', 'paragraph': \"\\nRandom forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned.[1][2] Random decision forests correct for decision trees' habit of overfitting to their training set.[3]:\\u200a587–588\\u200a\\n\"}, {'link': '/wiki/K-nearest_neighbors_algorithm', 'title': 'k-nearest neighbors algorithm', 'paragraph': 'In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951,[1] and later expanded by Thomas Cover.[2] It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression:\\n'}, {'link': '/wiki/Linear_regression', 'title': 'Linear regression', 'paragraph': 'In statistics, linear regression is a linear approach for modelling a predictive relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables), which are measured without error. The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression.[1] This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.[2]If the explanatory variables are measured with error then errors-in-variables models are required, also known as measurement error models.\\n'}, {'link': '/wiki/Naive_Bayes_classifier', 'title': 'Naive Bayes classifier', 'paragraph': 'In statistics, naive Bayes classifiers are a family of linear \"probabilistic classifiers\" based on applying Bayes\\' theorem with strong (naive) independence assumptions between the features (see Bayes classifier). They are among the simplest Bayesian network models,[1] but coupled with kernel density estimation, they can achieve high accuracy levels.[2]\\n'}, {'link': '/wiki/Logistic_regression', 'title': 'Logistic regression', 'paragraph': 'In statistics, the logistic model (or logit model) is a statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables. In regression analysis, logistic regression[1] (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination). Formally, in binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled \"0\" and \"1\", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \"1\" can vary between 0 (certainly the value \"0\") and 1 (certainly the value \"1\"), hence the labeling;[2] the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See §\\xa0Background and §\\xa0Definition for formal mathematics, and §\\xa0Example for a worked example.\\n'}, {'link': '/wiki/Perceptron', 'title': 'Perceptron', 'paragraph': 'In machine learning, the perceptron (or McCulloch-Pitts neuron) is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.[1]  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.\\n'}, {'link': '/wiki/Relevance_vector_machine', 'title': 'Relevance vector machine', 'paragraph': 'In mathematics, a Relevance Vector Machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification.[1]\\nThe RVM has an identical functional form to the support vector machine, but provides probabilistic classification.\\n'}, {'link': '/wiki/Support_vector_machine', 'title': 'Support vector machine', 'paragraph': 'In machine learning, support vector machines (SVMs, also support vector networks[1]) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995,[1] Vapnik et al., 1997[citation needed]) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\\n'}, {'link': '/wiki/BIRCH', 'title': 'BIRCH', 'paragraph': 'BIRCH (balanced iterative reducing and clustering using hierarchies) is an unsupervised data mining algorithm used to perform hierarchical clustering over particularly large data-sets.[1] With modifications it can also be used to accelerate k-means clustering and Gaussian mixture modeling with the expectation–maximization algorithm.[2] An advantage of BIRCH is its ability to incrementally and dynamically cluster incoming, multi-dimensional metric data points in an attempt to produce the best quality clustering for a given set of resources (memory and time constraints). In most cases, BIRCH only requires a single scan of the database.\\n'}, {'link': '/wiki/CURE_algorithm', 'title': 'CURE algorithm', 'paragraph': 'CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases[citation needed]. Compared with K-means clustering it is more robust to outliers and able to identify clusters having non-spherical shapes and size variances.\\n'}, {'link': '/wiki/Hierarchical_clustering', 'title': 'Hierarchical clustering', 'paragraph': 'In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis that seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two categories:\\n'}, {'link': '/wiki/K-means_clustering', 'title': 'k-means clustering', 'paragraph': 'k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.\\n'}, {'link': '/wiki/Fuzzy_clustering', 'title': 'Fuzzy clustering', 'paragraph': 'Fuzzy clustering (also referred to as soft clustering or soft k-means) is a form of clustering in which each data point can belong to more than one cluster.\\n'}, {'link': '/wiki/Expectation%E2%80%93maximization_algorithm', 'title': 'Expectation–maximization algorithm', 'paragraph': 'In statistics, an expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables.[1] The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.\\n'}, {'link': '/wiki/DBSCAN', 'title': 'DBSCAN', 'paragraph': 'Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996.[1]\\nIt is a density-based clustering non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away).\\nDBSCAN is one of the most common, and most commonly cited, clustering algorithms.[2]\\n'}, {'link': '/wiki/OPTICS_algorithm', 'title': 'OPTICS algorithm', 'paragraph': \"Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based[1] clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and Jörg Sander.[2]\\nIts basic idea is similar to DBSCAN,[3] but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. To do so, the points of the database are (linearly) ordered such that spatially closest points become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster. This is represented as a dendrogram.\\n\"}, {'link': '/wiki/Mean_shift', 'title': 'Mean shift', 'paragraph': 'Mean shift is a non-parametric feature-space mathematical analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm.[1] Application domains include cluster analysis in computer vision and image processing.[2]\\n'}, {'link': '/wiki/Factor_analysis', 'title': 'Factor analysis', 'paragraph': 'Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors plus \"error\" terms, hence factor analysis can be thought of as a special case of errors-in-variables models.[1]\\n'}, {'link': '/wiki/Canonical_correlation', 'title': 'Canonical correlation', 'paragraph': 'In statistics, canonical-correlation analysis (CCA), also called canonical variates analysis, is a way of inferring information from cross-covariance matrices. If we have two vectors X\\xa0=\\xa0(X1,\\xa0...,\\xa0Xn) and Y\\xa0=\\xa0(Y1,\\xa0...,\\xa0Ym)  of random variables, and there are correlations among the variables, then canonical-correlation analysis will find linear combinations of X and Y which have maximum correlation with each other.[1] T. R. Knapp notes that \"virtually all of the commonly encountered parametric tests of significance can be treated as special cases of canonical-correlation analysis, which is the general procedure for investigating the relationships between two sets of variables.\"[2] The method was first introduced by Harold Hotelling in 1936,[3] although in the context of angles between flats the mathematical concept was published by Jordan in 1875.[4]\\n'}, {'link': '/wiki/Independent_component_analysis', 'title': 'Independent component analysis', 'paragraph': 'In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that at most one subcomponent is Gaussian and that the subcomponents are statistically independent from each other.[1] ICA is a special case of blind source separation. A common example application is the \"cocktail party problem\" of listening in on one person\\'s speech in a noisy room.[2]\\n'}, {'link': '/wiki/Linear_discriminant_analysis', 'title': 'Linear discriminant analysis', 'paragraph': \"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\\n\"}, {'link': '/wiki/Non-negative_matrix_factorization', 'title': 'Non-negative matrix factorization', 'paragraph': 'Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.\\n'}, {'link': '/wiki/Principal_component_analysis', 'title': 'Principal component analysis', 'paragraph': 'Principal component analysis (PCA) is a popular technique for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data. Formally, PCA is a statistical technique for reducing the dimensionality of a dataset. This is accomplished by linearly transforming the data into a new coordinate system where (most of) the variation in the data can be described with fewer dimensions than the initial data. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points. Principal component analysis has applications in many fields such as population genetics, microbiome studies, and atmospheric science.[1]'}, {'link': '/wiki/Proper_generalized_decomposition', 'title': 'Proper generalized decomposition', 'paragraph': \"The proper generalized decomposition (PGD) is an iterative numerical method for solving boundary value problems (BVPs), that is, partial differential equations constrained by a set of boundary conditions, such as the Poisson's equation or the Laplace's equation.\\n\"}, {'link': '/wiki/T-distributed_stochastic_neighbor_embedding', 'title': 't-distributed stochastic neighbor embedding', 'paragraph': 't-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map. It is based on Stochastic Neighbor Embedding originally developed by Geoffrey Hinton and Sam Roweis,[1] where Laurens van der Maaten proposed the t-distributed variant.[2] It is a nonlinear dimensionality reduction technique for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\\n'}, {'link': '/wiki/Sparse_dictionary_learning', 'title': 'Sparse dictionary learning', 'paragraph': 'Sparse dictionary learning (also known as sparse coding or SDL) is a representation learning method which aims at finding a sparse representation of the input data in the form of a linear combination of basic elements as well as those basic elements themselves. These elements are called atoms and they compose a dictionary. Atoms in the dictionary are not required to be orthogonal, and they may be an over-complete spanning set. This problem setup also allows the dimensionality of the signals being represented to be higher than the one of the signals being observed. The above two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal but also provide an improvement in sparsity and flexibility of the representation.\\n'}, {'link': '/wiki/Graphical_model', 'title': 'Graphical model', 'paragraph': 'A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statistics—particularly Bayesian statistics—and machine learning.\\n'}, {'link': '/wiki/Bayesian_network', 'title': 'Bayesian network', 'paragraph': 'A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). While it is one of several forms of causal notation, causal networks are special cases of Bayesian networks. Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.\\n'}, {'link': '/wiki/Conditional_random_field', 'title': 'Conditional random field', 'paragraph': 'Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighbouring\" samples, a CRF can take context into account. To do so, the predictions are modelled as a graphical model, which represents the presence of dependencies between the predictions. What kind of graph is used depends on the application. For example, in natural language processing, \"linear chain\" CRFs are popular, for which each prediction is dependent only on its immediate neighbours. In image processing, the graph typically connects locations to nearby and/or similar locations to enforce that they receive similar predictions.\\n'}, {'link': '/wiki/Random_sample_consensus', 'title': 'Random sample consensus', 'paragraph': 'Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method.[1] It is a non-deterministic algorithm in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are allowed. The algorithm was first published by Fischler and Bolles at SRI International in 1981. They used RANSAC to solve the Location Determination Problem (LDP), where the goal is to determine the points in the space that project onto an image into a set of landmarks with known locations.\\n'}, {'link': '/wiki/Local_outlier_factor', 'title': 'Local outlier factor', 'paragraph': 'In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.[1]\\n'}, {'link': '/wiki/Isolation_forest', 'title': 'Isolation forest', 'paragraph': 'Isolation Forest is an algorithm for data anomaly detection initially developed by Fei Tony Liu in 2008.[1] Isolation Forest detects anomalies using binary trees. The algorithm has a linear time complexity and a low memory requirement, which works well with high-volume data.[2][3]\\nIn essence, the algorithm relies upon the characteristics of anomalies, i.e., being few and different, in order to detect anomalies. No density estimation is performed in the algorithm. The algorithm is different from decision tree algorithms in that only the path-length measure or approximation is being used to generate the anomaly score, no leaf node statistics on class distribution or target value is needed.\\n'}, {'link': '/wiki/Deep_learning', 'title': 'Deep learning', 'paragraph': 'Deep learning is the subset of machine learning methods which are based on artificial neural networks with representation learning. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.[2]\\n'}, {'link': '/wiki/DeepDream', 'title': 'DeepDream', 'paragraph': 'DeepDream is a computer vision program created by Google engineer Alexander Mordvintsev that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dream-like appearance reminiscent of a psychedelic experience in the deliberately overprocessed images.[1][2][3]\\n'}, {'link': '/wiki/Feedforward_neural_network', 'title': 'Feedforward neural network', 'paragraph': 'A feedforward neural network (FNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers.[2] Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops,[2] in contrast to recurrent neural networks,[3] which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method[4][5][6][7][8] and are colloquially referred to as the \"vanilla\" neural networks.[9]\\n'}, {'link': '/wiki/Recurrent_neural_network', 'title': 'Recurrent neural network', 'paragraph': 'A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs[1][2][3] makes them applicable to tasks such as unsegmented, connected handwriting recognition[4] or speech recognition.[5][6] The term \"recurrent neural network\" is used to refer to the class of networks with an infinite impulse response, whereas \"convolutional neural network\" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior.[7] A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.\\n'}, {'link': '/wiki/Long_short-term_memory', 'title': 'Long short-term memory', 'paragraph': 'Long short-term memory (LSTM)[1] network is a recurrent neural network (RNN), aimed to deal with the vanishing gradient problem[2] present in traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus \"long short-term memory\".[1] It is applicable to classification, processing and predicting data based on time series, such as in handwriting,[3] speech recognition,[4][5] machine translation,[6][7] speech activity detection,[8] robot control,[9][10] video games,[11][12] and healthcare.[13]\\n'}, {'link': '/wiki/Gated_recurrent_unit', 'title': 'Gated recurrent unit', 'paragraph': \"Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al.[1] The GRU is like a long short-term memory (LSTM) with a gating mechanism to input or forget certain features,[2] but lacks a context vector or output gate, resulting in fewer parameters than LSTM.[3] \\nGRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM.[4][5] GRUs showed that gating is indeed helpful in general, and Bengio's team came to no concrete conclusion on which of the two gating units was better.[6][7]\\n\"}, {'link': '/wiki/Echo_state_network', 'title': 'Echo state network', 'paragraph': 'An echo state network (ESN)[1][2] is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can produce or reproduce specific temporal patterns. The main interest of this network is that although its behavior is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.\\n'}, {'link': '/wiki/Reservoir_computing', 'title': 'Reservoir computing', 'paragraph': 'Reservoir computing is a framework for computation derived from recurrent neural network theory that maps input signals into higher dimensional computational spaces through the dynamics of a fixed, non-linear system called a reservoir.[1] After the input signal is fed into the reservoir, which is treated as a \"black box,\" a simple readout mechanism is trained to read the state of the reservoir and map it to the desired output.[1] The first key benefit of this framework is that training is performed only at the readout stage, as the reservoir dynamics are fixed.[1] The second is that the computational power of naturally available systems, both classical and quantum mechanical, can be used to reduce the effective computational cost.[2]\\n'}, {'link': '/wiki/Restricted_Boltzmann_machine', 'title': 'Restricted Boltzmann machine', 'paragraph': 'A restricted Boltzmann machine (RBM) (also called a restricted Sherrington–Kirkpatrick model with external field or restricted stochastic Ising–Lenz–Little model) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.[1]\\n'}, {'link': '/wiki/Diffusion_model', 'title': 'Diffusion model', 'paragraph': 'In machine learning, diffusion models, also known as diffusion probabilistic models or score-based generative models, are a class of generative models. The goal of diffusion models is to learn a diffusion process that generates the probability distribution of a given dataset. It mainly consists of three major components: the forward process, the reverse process, and the sampling procedure.[1] Three examples of generic diffusion modeling frameworks used in computer vision are denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations.[2]\\n'}, {'link': '/wiki/Self-organizing_map', 'title': 'Self-organizing map', 'paragraph': 'A self-organizing map (SOM) or self-organizing feature map (SOFM) is an unsupervised machine learning technique used to produce a low-dimensional (typically two-dimensional) representation of a higher dimensional data set while preserving the topological structure of the data. For example, a data set with \\n\\n\\n\\np\\n\\n\\n{\\\\displaystyle p}\\n\\n variables measured in \\n\\n\\n\\nn\\n\\n\\n{\\\\displaystyle n}\\n\\n observations could be represented as clusters of observations with similar values for the variables. These clusters then could be visualized as a two-dimensional \"map\" such that observations in proximal clusters have more similar values than observations in distal clusters. This can make high-dimensional data easier to visualize and analyze.\\n'}, {'link': '/wiki/Convolutional_neural_network', 'title': 'Convolutional neural network', 'paragraph': 'Convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters (or kernel) optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections.[1][2] For example, for each neuron in the fully-connected layer 10,000 weights would be required for processing an image sized 100 × 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels,[3][4]  only 25 neurons are required to process 5x5-sized tiles.[5][6] Higher-layer features are extracted  from wider context windows, compared to lower-layer features.\\n'}, {'link': '/wiki/U-Net', 'title': 'U-Net', 'paragraph': 'U-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg.[1] The network is based on a fully convolutional neural network[2] whose architecture was modified and extended to work with fewer training images and to yield more precise segmentation. Segmentation of a 512\\xa0×\\xa0512 image takes less than a second on a modern GPU.\\n'}, {'link': '/wiki/Transformer_(machine_learning_model)', 'title': 'Transformer (machine-learning model)', 'paragraph': 'A transformer is a deep learning architecture, initially proposed in 2017, that relies on the parallel multi-head attention mechanism.[1] It is notable for requiring less training time than previous recurrent neural architectures, such as long short-term memory (LSTM),[2] and its later variation has been prevalently adopted for training large language models on large (language) datasets, such as the Wikipedia corpus and Common Crawl, by virtue of the parallelized processing of input sequence.[3]\\nInput text is split into n-grams encoded as tokens and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished. Though the transformer paper was published in 2017, the softmax-based attention mechanism was proposed in 2014 for machine translation,[4][5] and the Fast Weight Controller, similar to a transformer, was proposed in 1992.[6][7][8]\\n'}, {'link': '/wiki/Vision_transformer', 'title': 'Vision transformer', 'paragraph': 'A vision transformer (ViT) is a transformer designed for computer vision. Transformers were introduced in 2017,[1] and have found widespread use in natural language processing. In 2020, they were adapted for computer vision, yielding ViT.[2] The basic structure is to break down input images as a series of patches, then tokenized, before applying the tokens to a standard Transformer architecture.\\n'}, {'link': '/wiki/Spiking_neural_network', 'title': 'Spiking neural network', 'paragraph': 'Spiking neural networks (SNNs) are artificial neural networks that more closely mimic natural neural networks.[1] In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather transmit information only when a membrane potential—an intrinsic quality of the neuron related to its membrane electrical charge—reaches a specific value, called the threshold. When the membrane potential reaches the threshold, the neuron fires, and generates a signal that travels to other neurons which, in turn, increase or decrease their potentials in response to this signal. A neuron model  that fires at the moment of threshold crossing is also called a spiking neuron model.[2]\\n'}, {'link': '/wiki/Memtransistor', 'title': 'Memtransistor', 'paragraph': 'The memtransistor (a blend word from Memory Transfer Resistor) is an experimental multi-terminal passive electronic component that might be used in the construction of artificial neural networks.[1] It is a combination of the memristor and transistor technology.[2] This technology is different from the 1T-1R approach since the devices are merged into one single entity. Multiple memristers can be embedded with a single transistor, enabling it to more accurately model a neuron with its multiple synaptic connections. A neural network produced from these would provide hardware-based artificial intelligence with a good foundation.[1][3]\\n'}, {'link': '/wiki/Electrochemical_RAM', 'title': 'Electrochemical RAM', 'paragraph': 'Electrochemical Random-Access Memory (ECRAM) is a type of non-volatile memory (NVM) with multiple levels per cell (MLC) designed for deep learning analog acceleration.[1][2][3] An ECRAM cell is a three-terminal device composed of a conductive channel, an insulating electrolyte, an ionic reservoir, and metal contacts. The resistance of the channel is modulated by ionic exchange at the interface between the channel and the electrolyte upon application of an electric field. The charge-transfer process allows both for state retention in the absence of applied power, and for programming of multiple distinct levels, both differentiating ECRAM operation from that of a field-effect transistor (FET). The write operation is deterministic and can result in symmetrical potentiation and depression, making ECRAM arrays attractive for acting as artificial synaptic weights in physical implementations of artificial neural networks (ANN). The technological challenges include open circuit potential (OCP) and semiconductor foundry compatibility associated with energy materials. Universities, government laboratories, and corporate research teams have contributed to the development of ECRAM for analog computing. Notably, Sandia National Laboratories designed a lithium-based cell inspired by solid-state battery materials,[4] Stanford University built an organic proton-based cell,[5] and International Business Machines (IBM) demonstrated in-memory selector-free parallel programming for a logistic regression task in an array of metal-oxide ECRAM designed for insertion in the back end of line (BEOL).[6] In 2022, researchers at Massachusetts Institute of Technology built an inorganic, CMOS-compatible protonic technology that achieved near-ideal modulation characteristics using nanosecond fast pulses [7]\\n'}, {'link': '/wiki/Q-learning', 'title': 'Q-learning', 'paragraph': 'Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. It does not require a model of the environment (hence \"model-free\"), and it can handle problems with stochastic transitions and rewards without requiring adaptations [1].\\n'}, {'link': '/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action', 'title': 'State–action–reward–state–action', 'paragraph': 'State–action–reward–state–action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was proposed by Rummery and Niranjan in a technical note[1] with the name \"Modified Connectionist Q-Learning\" (MCQ-L). The alternative name SARSA, proposed by Rich Sutton, was only mentioned as a footnote.\\n'}, {'link': '/wiki/Temporal_difference_learning', 'title': 'Temporal difference learning', 'paragraph': 'Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods.[1]\\n'}, {'link': '/wiki/Multi-agent_reinforcement_learning', 'title': 'Multi-agent reinforcement learning', 'paragraph': 'Multi-agent reinforcement learning (MARL) is a sub-field of reinforcement learning. It focuses on studying the behavior of multiple learning agents that coexist in a shared environment.[1] Each agent is motivated by its own rewards, and does actions to advance its own interests; in some environments these interests are opposed to the interests of other agents, resulting in complex group dynamics.\\n'}, {'link': '/wiki/Self-play_(reinforcement_learning_technique)', 'title': 'Self-play', 'paragraph': 'Self-play is a technique for improving the performance of reinforcement learning agents. Intuitively, agents learn to improve their performance by playing \"against themselves\".\\n'}, {'link': '/wiki/Active_learning_(machine_learning)', 'title': 'Active learning (machine learning)', 'paragraph': 'Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs.[1][2][3] In statistics literature, it is sometimes also called optimal experimental design.[4] The information source is also called teacher or oracle.\\n'}, {'link': '/wiki/Human-in-the-loop', 'title': 'Human-in-the-loop', 'paragraph': 'Human-in-the-loop  or HITL is used in multiple contexts. It can be defined as a model requiring human interaction.[1][2] HITL is associated with modeling and simulation (M&S) in the live, virtual, and constructive taxonomy. HITL along with the related human-on-the-loop are also used in relation to lethal autonomous weapons.[3] Further, HITL is used in the context of machine learning.[4]\\n'}, {'link': '/wiki/Learning_curve_(machine_learning)', 'title': 'Learning curve (machine learning)', 'paragraph': \"In machine learning, a learning curve (or training curve) plots the optimal value of a model's loss function for a training set against this loss function evaluated on a validation data set with same parameters as produced the optimal function.[1] Synonyms include error curve, experience curve, improvement curve and generalization curve.[2]\\n\"}, {'link': '/wiki/Kernel_machines', 'title': 'Kernel method', 'paragraph': 'In machine learning, kernel machines are a class of algorithms for pattern analysis, whose best known member is the support-vector machine (SVM). These methods involve using linear classifiers to solve nonlinear problems.[1] The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over all pairs of data points computed using inner products. The feature map in kernel machines is infinite dimensional but only requires a finite dimensional matrix from user-input according to the Representer theorem. Kernel machines are slow to compute for datasets larger than a couple of thousand examples without parallel processing. \\n'}, {'link': '/wiki/Bias%E2%80%93variance_tradeoff', 'title': 'Bias–variance tradeoff', 'paragraph': \"In statistics and machine learning, the bias–variance tradeoff describes the relationship between a model's complexity, the accuracy of its predictions, and how well it can make predictions on previously unseen data that were not used to train the model. In general, as we increase the number of tunable parameters in a model, it becomes more flexible, and can better fit a training data set. It is said to have lower error, or bias. However, for more flexible models, there will tend to be greater variance to the model fit each time we take a set of samples to create a new training data set. It is said that there is greater variance in the model's estimated parameters.\\n\"}, {'link': '/wiki/Computational_learning_theory', 'title': 'Computational learning theory', 'paragraph': 'In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.[1]\\n'}, {'link': '/wiki/Empirical_risk_minimization', 'title': 'Empirical risk minimization', 'paragraph': 'Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an algorithm will work in practice (the true \"risk\") because we don\\'t know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the \"empirical\" risk).\\n'}, {'link': '/wiki/Occam_learning', 'title': 'Occam learning', 'paragraph': 'In computational learning theory, Occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to probably approximately correct (PAC) learning, where the learner is evaluated on its predictive power of a test set.\\n'}, {'link': '/wiki/Probably_approximately_correct_learning', 'title': 'Probably approximately correct learning', 'paragraph': 'In computational learning theory, probably approximately correct (PAC) learning is a framework for mathematical analysis of machine learning. It was proposed in 1984 by Leslie Valiant.[1]\\n'}, {'link': '/wiki/Statistical_learning_theory', 'title': 'Statistical learning theory', 'paragraph': 'Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis.[1][2][3] Statistical learning theory deals with the statistical inference problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics.\\n'}, {'link': '/wiki/Vapnik%E2%80%93Chervonenkis_theory', 'title': 'Vapnik–Chervonenkis theory', 'paragraph': 'Vapnik–Chervonenkis theory (also known as VC theory) was developed during 1960–1990 by Vladimir Vapnik and Alexey Chervonenkis. The theory is a form of computational learning theory, which attempts to explain the learning process from a statistical point of view.\\n'}, {'link': '/wiki/ECML_PKDD', 'title': 'ECML PKDD', 'paragraph': 'ECML PKDD, the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, is one of the leading[1][2] academic conferences on machine learning and knowledge discovery, held in Europe every year.\\n'}, {'link': '/wiki/Conference_on_Neural_Information_Processing_Systems', 'title': 'Conference on Neural Information Processing Systems', 'paragraph': 'The Conference and Workshop on Neural Information Processing Systems (abbreviated as NeurIPS and formerly NIPS)  is a machine learning and computational neuroscience conference held every December. The conference is currently a double-track meeting (single-track until 2015) that includes invited talks as well as oral and poster presentations of refereed papers, followed by parallel-track workshops that up to 2013 were held at ski resorts.\\n'}, {'link': '/wiki/International_Conference_on_Machine_Learning', 'title': 'International Conference on Machine Learning', 'paragraph': 'The International Conference on Machine Learning (ICML) is the leading international academic conference in machine learning. Along with NeurIPS and ICLR, it is one of the three primary conferences of high impact in machine learning and artificial intelligence research.[1] It is supported by the (IMLS). Precise dates vary year to year, but paper submissions are generally due at the end of January, and the conference is generally held the following July. The first ICML was held 1980 in Pittsburgh.[2][3]\\n'}, {'link': '/wiki/International_Conference_on_Learning_Representations', 'title': 'International Conference on Learning Representations', 'paragraph': 'The International Conference on Learning Representations (ICLR) is a machine learning conference typically held in late April or early May each year. The conference includes invited talks as well as oral and poster presentations of refereed papers. Since its inception in 2013, ICLR has employed an open peer review process to referee paper submissions (based on models proposed by Yann LeCun[1]). In 2019, there were 1591 paper submissions, of which 500 accepted with poster presentations (31%) and 24 with oral presentations (1.5%).[2]. In 2021, there were 2997 paper submissions, of which 860 were accepted (29%).[3].\\n'}, {'link': '/wiki/International_Joint_Conference_on_Artificial_Intelligence', 'title': 'International Joint Conference on Artificial Intelligence', 'paragraph': 'The International Joint Conference on Artificial Intelligence (IJCAI) is the leading conference in the field of artificial intelligence. The conference series has been organized by the nonprofit IJCAI Organization since 1969, making it the oldest premier AI conference series in the world.[1] It was held biennially in odd-numbered years from 1969 to 2015 and annually starting from 2016. More recently, IJCAI was held jointly every four years with ECAI since 2018 and PRICAI since 2020 to promote collaboration of AI researchers and practitioners. IJCAI covers a broad range of research areas in the field of AI. It is a large and highly selective conference, with only about 20% or less of the submitted papers accepted after peer review in the 5 years leading up to 2022.[2] A lower acceptance rate usually means better quality papers and a higher reputation conference.\\n'}, {'link': '/wiki/Journal_of_Machine_Learning_Research', 'title': 'Journal of Machine Learning Research', 'paragraph': 'The Journal of Machine Learning Research is a peer-reviewed open access scientific journal covering machine learning. It was established in 2000 and the first editor-in-chief was Leslie Kaelbling.[1] The current editors-in-chief are Francis Bach (Inria) and David Blei (Columbia University).\\n'}, {'link': '/wiki/List_of_datasets_in_computer_vision_and_image_processing', 'title': 'List of datasets in computer vision and image processing', 'paragraph': 'This is a list of datasets for machine learning research. It is part of the list of datasets for machine-learning research. These datasets consist primarily of images or videos for tasks such as object detection, facial recognition, and multi-label classification.\\n'}, {'link': '/wiki/Outline_of_machine_learning', 'title': 'Outline of machine learning', 'paragraph': 'The following outline is provided as an overview of and topical guide to machine learning:\\n'}, {'link': '/wiki/Template_talk:Machine_learning', 'title': 'Template talk:Machine learning', 'paragraph': 'This section title and contents seem pretty much random to me. How are contents chosen? One regression, one random clustering algorithm, 4 standard classificators; but no decision tree; which is probably the grandfather of all classificators. --Chire (talk) 12:41, 22 October 2013 (UTC)Reply[reply]\\n'}, {'link': '/wiki/Special:EditPage/Template:Machine_learning', 'title': 'Editing Template:Machine learning', 'paragraph': 'Copy and paste: – — ° ′ ″ ≈ ≠ ≤ ≥ ± − × ÷ ← → · § \\xa0 Sign your posts on talk pages: ~~~~ \\xa0 Cite your sources: <ref></ref> \\n'}, {'link': '/wiki/Outline_of_artificial_intelligence', 'title': 'Outline of artificial intelligence', 'paragraph': 'The following outline is provided as an overview of and topical guide to artificial intelligence:\\n'}, {'link': '/wiki/File:Artificial_intelligence_prompt_completion_by_dalle_mini.jpg', 'title': 'File:Artificial intelligence prompt completion by dalle mini.jpg', 'paragraph': 'Original file \\u200e(1,024 × 1,024 pixels, file size: 211 KB, MIME type: image/jpeg)\\n'}, {'link': '/wiki/Automated_planning_and_scheduling', 'title': 'Automated planning and scheduling', 'paragraph': 'Automated planning and scheduling, sometimes denoted as simply AI planning,[1] is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory.\\n'}, {'link': '/wiki/Computer_vision', 'title': 'Computer vision', 'paragraph': 'Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions.[1][2][3][4] Understanding in this context means the transformation of visual images (the input to the retina in the human analog) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\\n'}, {'link': '/wiki/General_game_playing', 'title': 'General game playing', 'paragraph': 'General game playing (GGP) is the design of artificial intelligence programs to be able to play more than one game successfully.[1][2][3] For many games like chess, computers are programmed to play these games using a specially designed algorithm, which cannot be transferred to another context. For instance, a chess-playing computer program cannot play checkers. General game playing is considered as a necessary milestone on the way to artificial general intelligence.[4]\\n'}, {'link': '/wiki/Knowledge_representation_and_reasoning', 'title': 'Knowledge representation and reasoning', 'paragraph': 'Knowledge representation and reasoning (KRR, KR&R, KR²) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can use to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. Knowledge representation incorporates findings from psychology[1] about how humans solve problems and represent knowledge in order to design formalisms that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from logic to automate various kinds of reasoning, such as the application of rules or the relations of sets and subsets.\\n'}, {'link': '/wiki/Natural_language_processing', 'title': 'Natural language processing', 'paragraph': 'Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\\n'}, {'link': '/wiki/AI_safety', 'title': 'AI safety', 'paragraph': 'AI safety is an interdisciplinary field concerned with preventing accidents, misuse, or other harmful consequences that could result from artificial intelligence (AI) systems. It encompasses machine ethics and AI alignment, which aim to make AI systems moral and beneficial, and AI safety encompasses technical problems including monitoring systems for risks and making them highly reliable. Beyond AI research, it involves developing norms and policies that promote safety.\\n'}, {'link': '/wiki/Symbolic_artificial_intelligence', 'title': 'Symbolic artificial intelligence', 'paragraph': 'In artificial intelligence, symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search.[1] Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems),  symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to seminal ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems.\\n'}, {'link': '/wiki/Evolutionary_algorithm', 'title': 'Evolutionary algorithm', 'paragraph': 'In computational intelligence (CI), an evolutionary algorithm (EA) is a subset of evolutionary computation,[1] a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.\\n'}, {'link': '/wiki/Situated_approach_(artificial_intelligence)', 'title': 'Situated approach (artificial intelligence)', 'paragraph': 'In artificial intelligence research, the situated approach builds agents that are designed to behave effectively successfully in their environment. This requires designing AI \"from the bottom-up\" by focussing on the basic perceptual and motor skills required to survive. The situated approach gives a much lower priority to abstract reasoning or problem-solving skills.\\n'}, {'link': '/wiki/Hybrid_intelligent_system', 'title': 'Hybrid intelligent system', 'paragraph': '\\nHybrid intelligent system denotes a software system which employs, in parallel, a combination of methods and techniques from artificial intelligence subfields, such as:\\n'}, {'link': '/wiki/Artificial_intelligence_in_healthcare', 'title': 'Artificial intelligence in healthcare', 'paragraph': 'Artificial intelligence in healthcare is a term used to describe the use of machine-learning algorithms and software, or artificial intelligence (AI), to copy human cognition in the analysis, presentation, and understanding of complex medical and health care data, or to exceed human capabilities by providing new ways to diagnose, treat, or prevent disease.[1][2] Specifically, AI is the ability of computer algorithms to approximate conclusions based solely on input data.\\n'}, {'link': '/wiki/Artificial_intelligence_in_government', 'title': 'Artificial intelligence in government', 'paragraph': 'Artificial intelligence (AI) has a range of uses in government. It can be used to further public policy objectives (in areas such as emergency services, health and welfare), as well as assist the public to interact with the government  (through the use of virtual assistants, for example). According to the Harvard Business Review, \"Applications of artificial intelligence to the public sector are broad and growing, with early experiments taking place around the world.\"[1] Hila Mehr from the Ash Center for Democratic Governance and Innovation at Harvard University notes that AI in government is not new, with postal services using machine methods in the late 1990s to recognise handwriting on envelopes to automatically route letters.[2] The use of AI in government comes with significant benefits, including efficiencies resulting in cost savings (for instance by reducing the number of front office staff), and reducing the opportunities for corruption.[3] However, it also carries risks.[citation needed][further explanation needed]\\n'}, {'link': '/wiki/Music_and_artificial_intelligence', 'title': 'Music and artificial intelligence', 'paragraph': 'Artificial intelligence and music (AIM) is a common subject in the International Computer Music Conference, the Computing Society Conference[1] and the International Joint Conference on Artificial Intelligence. The first International Computer Music Conference (ICMC) was held in 1974 at Michigan State University.[2] Current research includes the application of AI in music composition, performance, theory and digital sound processing.\\n'}, {'link': '/wiki/Artificial_intelligence_in_industry', 'title': 'Artificial intelligence in industry', 'paragraph': 'Industrial artificial intelligence, or industrial AI, usually refers to the application of artificial intelligence to industry. Unlike general artificial intelligence which is a frontier research discipline to build computerized systems that perform tasks requiring human intelligence, industrial AI is more concerned with the application of such technologies to address industrial pain-points for customer value creation, productivity improvement, cost reduction, site optimization, predictive analysis[1]  and insight discovery.[2]\\n'}, {'link': '/wiki/Machine_learning_in_physics', 'title': 'Machine learning in physics', 'paragraph': 'Applying classical methods of machine learning to the study of quantum systems is the focus of an emergent area of physics research. A basic example of this is quantum state tomography, where a quantum state is learned from measurement.[1] Other examples include learning Hamiltonians,[2][3] learning quantum phase transitions,[4][5] and automatically generating new quantum experiments.[6][7][8][9] Classical machine learning is effective at processing large amounts of experimental or calculated data in order to characterize an unknown quantum system, making its application useful in contexts including quantum information theory, quantum technologies development, and computational materials design. In this context, it can be used for example as a tool to interpolate pre-calculated interatomic potentials[10] or directly solving the Schrödinger equation with a variational method.[11]\\n'}, {'link': '/wiki/Philosophy_of_artificial_intelligence', 'title': 'Philosophy of artificial intelligence', 'paragraph': 'The philosophy of artificial intelligence is a branch of the philosophy of mind and the philosophy of computer science[1] that explores artificial intelligence and its implications for knowledge and understanding of intelligence, ethics, consciousness, epistemology, and free will.[2][3] Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; see artificial life) so the discipline is of considerable interest to philosophers.[4] These factors contributed to the emergence of the philosophy of artificial intelligence. \\n'}, {'link': '/wiki/AI_takeover', 'title': 'AI takeover', 'paragraph': 'An AI takeover is a hypothetical scenario in which artificial intelligence (AI) becomes the dominant form of intelligence on Earth, as computer programs or robots effectively take control of the planet away from the human species. Possible scenarios include replacement of the entire human workforce, takeover by a superintelligent AI, and the popular notion of a robot uprising. Stories of AI takeovers are very popular throughout science fiction. Some public figures, such as Stephen Hawking and Elon Musk, have advocated research into precautionary measures to ensure future superintelligent machines remain under human control.[1]\\n'}, {'link': '/wiki/Ethics_of_artificial_intelligence', 'title': 'Ethics of artificial intelligence', 'paragraph': 'The ethics of artificial intelligence is the branch of the ethics of technology specific to artificially intelligent systems.[1] It is sometimes divided into a concern with the moral behavior of humans as they design, make, use and treat artificially intelligent systems, and a concern with the behavior of machines, in machine ethics. \\n'}, {'link': '/wiki/Progress_in_artificial_intelligence', 'title': 'Progress in artificial intelligence', 'paragraph': 'Progress in artificial intelligence (AI) refers to the advances, milestones, and breakthroughs that have been achieved in the field of artificial intelligence over time. AI is a multidisciplinary branch of computer science that aims to create machines and systems capable of performing tasks that typically require human intelligence. Artificial intelligence applications have been used in a wide range of fields including medical diagnosis, economic-financial applications, robot control, law, scientific discovery, video games, and toys. However, many AI applications are not perceived as AI:  \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"[1][2] \"Many thousands of AI applications are deeply embedded in the infrastructure of every industry.\"[3] In the late 1990s and early 21st century, AI technology became widely used as elements of larger systems,[3][4] but the field was rarely credited for these successes at the time.\\n'}, {'link': '/wiki/AI_winter', 'title': 'AI winter', 'paragraph': '\\nIn the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1]  The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.\\n'}, {'link': '/wiki/Template:Artificial_intelligence', 'title': 'Template:Artificial intelligence', 'paragraph': 'This template shows topics in the area of artificial intelligence.\\n'}, {'link': '/wiki/Template_talk:Artificial_intelligence', 'title': 'Template talk:Artificial intelligence', 'paragraph': \"The current pic, which I have added to the template, is at top, and the previous one is at bottom. I do not think the old one was very good; it is an illustration of the contours of a human brain with a random circuit board overlaid on it. What circuit board? We don't know. It looks like there is supposed to be a pad for a CPU in the middle... and there is part of a ball grid array or something there... but there is also a gigantic randomly-shaped splotch of copper there, what is that for? I am confident that this is not an actual PCB, nor is it a plausible design for one, and I object to illustrating articles about artificial intelligence with a ridiculously fake image.\\n\"}, {'link': '/wiki/Computational_statistics', 'title': 'Computational statistics', 'paragraph': 'Computational statistics, or statistical computing, is the bond between statistics and computer science, and refers to the statistical methods that are enabled by using computational methods. It is the area of computational science (or scientific computing) specific to the mathematical science of statistics. This area is also developing rapidly, leading to calls that a broader concept of computing should be taught as part of general statistical education.[1]\\n'}, {'link': '/wiki/Large_language_model', 'title': 'Large language model', 'paragraph': 'A large language model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by using massive amounts of data to learn billions of parameters during training and consuming large computational resources during their training and operation.[1] LLMs are artificial neural networks (mainly transformers[2]) and are (pre-)trained using self-supervised learning and semi-supervised learning.\\n'}, {'link': '/wiki/Speech_recognition', 'title': 'Speech recognition', 'paragraph': '\\nSpeech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.\\n'}, {'link': '/wiki/Email_filtering', 'title': 'Email filtering', 'paragraph': 'Email filtering is the processing of email to organize it according to specified criteria. The term can apply to the intervention of human intelligence, but most often refers to the automatic processing of messages at an SMTP server, possibly applying anti-spam techniques. Filtering can be applied to incoming emails as well as to outgoing ones.\\n'}, {'link': '/wiki/Mathematical_optimization', 'title': 'Mathematical optimization', 'paragraph': 'Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criterion, from some set of available alternatives.[1] It is generally divided into two subfields: discrete optimization and continuous optimization. Optimization problems arise in all quantitative disciplines from computer science and engineering[2] to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.[3]\\n'}, {'link': '/wiki/Exploratory_data_analysis', 'title': 'Exploratory data analysis', 'paragraph': 'In statistics, exploratory data analysis (EDA) is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling and thereby contrasts traditional hypothesis testing. Exploratory data analysis has been promoted by John Tukey since 1970 to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA),[1][2] which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.\\n'}, {'link': '/wiki/Predictive_analytics', 'title': 'Predictive analytics', 'paragraph': 'Predictive analytics is a form of business analytics applying machine learning to generate a predictive model for certain business applications. As such, it encompasses a variety of statistical techniques from predictive modeling and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events.[1] It represents a major subset of machine learning applications; in some contexts, it is synonymous with machine learning.[2]\\n'}, {'link': '/wiki/Arthur_Samuel_(computer_scientist)', 'title': 'Arthur Samuel (computer scientist)', 'paragraph': 'Arthur Lee Samuel (December 5, 1901 – July 29, 1990)[3] was an American pioneer in the field of computer gaming and artificial intelligence.[2] He popularized the term \"machine learning\" in 1959.[4] The Samuel Checkers-playing Program was among the world\\'s first successful self-learning programs, and as such a very early demonstration of the fundamental concept of artificial intelligence (AI).[5] He was also a senior member in the TeX community who devoted much time giving personal attention to the needs of users and wrote an early TeX manual in 1983.[6]\\n'}, {'link': '/wiki/Raytheon_Company', 'title': 'Raytheon', 'paragraph': \"The Raytheon Company was a major U.S. defense contractor and industrial corporation with manufacturing concentrations in weapons and military and commercial electronics. It was previously involved in corporate and special-mission aircraft until early 2007. Raytheon was the world's largest producer of guided missiles.[3] In April 2020, the company merged with United Technologies Corporation to form Raytheon Technologies,[4] which, since July 2023, is known as RTX Corporation.\\n\"}, {'link': '/wiki/Goof', 'title': 'Goof', 'paragraph': 'A goof is a mistake. The term is also used in a number of specific senses: in cinema, it is an error or oversight during production that is visible in the released version of the film.\\n'}, {'link': '/wiki/Operational_definition', 'title': 'Operational definition', 'paragraph': 'An operational definition specifies concrete, replicable procedures designed to represent a construct. In the words of American psychologist S.S. Stevens (1935), \"An operation is the performance which we execute in order to make known a concept.\"[1][2] For example, an operational definition of \"fear\" (the construct) often includes measurable physiologic responses that occur in response to a perceived threat. Thus, \"fear\" might be operationally defined as specified changes in heart rate, galvanic skin response, pupil dilation, and blood pressure.[3]\\n'}, {'link': '/wiki/Computing_Machinery_and_Intelligence', 'title': 'Computing Machinery and Intelligence', 'paragraph': '\\n\\n\"Computing Machinery and Intelligence\" is a seminal paper written by Alan Turing on the topic of artificial intelligence. The paper, published in 1950 in Mind, was the first to introduce his concept of what is now known as the Turing test to the general public.\\n'}, {'link': '/wiki/File:AI_hierarchy.svg', 'title': 'File:AI hierarchy.svg', 'paragraph': 'Original file \\u200e(SVG file, nominally 399 × 399 pixels, file size: 8 KB)\\n'}, {'link': '/wiki/ADALINE', 'title': 'ADALINE', 'paragraph': 'ADALINE (Adaptive Linear Neuron or later Adaptive Linear Element) is an early single-layer artificial neural network and the name of the physical device that implemented this network.[1][2][3][4][5] The network uses memistors. It was developed by professor Bernard Widrow and his doctoral student Ted Hoff at Stanford University in 1960. It is based on the perceptron. It consists of a weight, a bias and a summation function.\\n'}, {'link': '/wiki/Generalized_linear_model', 'title': 'Generalized linear model', 'paragraph': 'In statistics, a generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\\n'}, {'link': '/wiki/Probabilistic_reasoning', 'title': 'Probabilistic logic', 'paragraph': 'Probabilistic logic (also probability logic and probabilistic reasoning) involves the use of probability and logic to deal with uncertain situations. Probabilistic logic extends traditional logic truth tables with probabilistic expressions. A difficulty of probabilistic logics is their tendency to multiply the computational complexities of their probabilistic and logical components.  Other difficulties include the possibility of counter-intuitive results, such as in case of belief fusion in Dempster–Shafer theory. Source trust and epistemic uncertainty about the probabilities they provide, such as defined in subjective logic, are additional elements to consider. The need to deal with a broad variety of contexts and issues has led to many different proposals.\\n'}, {'link': '/wiki/Automated_medical_diagnosis', 'title': 'Computer-aided diagnosis', 'paragraph': 'Computer-aided detection (CADe), also called computer-aided diagnosis (CADx), are systems that assist doctors in the interpretation of medical images. Imaging techniques in X-ray, MRI, Endoscopy, and ultrasound diagnostics yield a great deal of information that the radiologist or other medical professional has to analyze and evaluate comprehensively in a short time. CAD systems process digital images or videos for typical appearances and to highlight conspicuous sections, such as possible diseases, in order to offer input to support a decision taken by the professional.\\n'}, {'link': '/wiki/Expert_system', 'title': 'Expert system', 'paragraph': 'In artificial intelligence, an expert system is a computer system emulating the decision-making ability of a human expert.[1]\\nExpert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural code.[2] The first expert systems were created in the 1970s and then proliferated in the 1980s.[3] Expert systems were among the first truly successful forms of artificial intelligence (AI) software.[4][5][6][7][8] \\nAn expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.\\n'}, {'link': '/wiki/Inductive_logic_programming', 'title': 'Inductive logic programming', 'paragraph': 'Inductive logic programming (ILP) is a subfield of symbolic artificial intelligence  which uses logic programming as a uniform representation for examples, background knowledge and hypotheses.  The term \"inductive\" here refers to philosophical (i.e. suggesting a theory to explain observed facts) rather than mathematical (i.e. proving a property for all members of a well-ordered set) induction. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesised logic program which entails all the positive and none of the negative examples.\\n'}, {'link': '/wiki/Pattern_recognition', 'title': 'Pattern recognition', 'paragraph': 'Pattern recognition is the automated recognition of patterns and regularities in data. While similar, pattern recognition (PR) is not to be confused with pattern machines (PM) which may possess (PR) capabilities but their primary function is to distinguish and create emergent  pattern.   PR has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power.\\n'}, {'link': '/wiki/Information_retrieval', 'title': 'Information retrieval', 'paragraph': 'Information retrieval (IR) in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources.  Searches can be based on full-text or other content-based indexing. Information retrieval is the science[1] of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.\\n'}, {'link': '/wiki/Connectionism', 'title': 'Connectionism', 'paragraph': \"Connectionism (coined by Edward Thorndike in the 1930s) is the name of an approach to the study of human mental processes and cognition that utilizes mathematical models known as connectionist networks or artificial neural networks.[1] Connectionism has had many 'waves' since its beginnings.\\n\"}, {'link': '/wiki/David_Rumelhart', 'title': 'David Rumelhart', 'paragraph': 'David Everett Rumelhart (June 12, 1942 – March 13, 2011)[1] was an American psychologist who made many contributions to the formal analysis of human cognition, working primarily within the frameworks of mathematical psychology, symbolic artificial intelligence, and parallel distributed processing. He also admired formal linguistic approaches to cognition, and explored the possibility of formulating a formal grammar to capture the structure of stories.\\n'}, {'link': '/wiki/Backpropagation', 'title': 'Backpropagation', 'paragraph': 'As a machine-learning algorithm, backpropagation is a crucial step in a common method used to iteratively train a neural network model. It is used to calculate the necessary parameter adjustments, to gradually minimize error.\\n'}, {'link': '/wiki/Fuzzy_logic', 'title': 'Fuzzy logic', 'paragraph': '\\nFuzzy logic is a form of many-valued logic in which the truth value of variables may be any real number between 0 and 1. It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false.[1] By contrast, in Boolean logic, the truth values of variables may only be the integer values 0 or 1.\\n'}, {'link': '/wiki/Probability_theory', 'title': 'Probability theory', 'paragraph': 'Probability theory or probability calculus is the branch of mathematics concerned with probability. Although there are several different probability interpretations, probability theory treats the concept in a rigorous mathematical manner by expressing it through a set of axioms. Typically these axioms formalise probability in terms of a probability space, which assigns a measure taking values between 0 and 1, termed the probability measure, to a set of outcomes called the sample space. Any specified subset of the sample space is called an event.\\n'}, {'link': '/wiki/Knowledge_discovery', 'title': 'Knowledge extraction', 'paragraph': 'Knowledge extraction is the creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criterion is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data.\\n'}, {'link': '/wiki/Loss_function', 'title': 'Loss function', 'paragraph': 'In mathematical optimization and decision theory, a loss function or cost function (sometimes also called an error function) [1] is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its opposite (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. The loss function could include terms from several levels of the hierarchy.\\n'}, {'link': '/wiki/Generalization_(learning)', 'title': 'Generalization (learning)', 'paragraph': 'Generalization is the concept that humans, other animals, and artificial neural networks use past learning in present situations of learning if the conditions in the situations are regarded as similar.[1] The learner uses generalized patterns, principles, and other similarities between past experiences and novel experiences to more efficiently navigate the world.[2] For example, if a person has learned in the past that every time they eat an apple, their throat becomes itchy and swollen, they might assume they are allergic to all fruit. When this person is offered a banana to eat, they reject it upon assuming they are also allergic to it through generalizing that all fruits cause the same reaction. Although this generalization about being allergic to all fruit based on experiences with one fruit could be correct in some cases, it may not be correct in all. Both positive and negative effects have been shown in education through learned generalization and its contrasting notion of discrimination learning.\\n'}, {'link': '/wiki/Statistics', 'title': 'Statistics', 'paragraph': 'Statistics (from German: Statistik, orig. \"description of a state, a country\")[1][2] is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data.[3][4][5] In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.[6]\\n'}, {'link': '/wiki/Statistical_inference', 'title': 'Statistical inference', 'paragraph': 'Statistical inference is the process of using data analysis to infer properties of an underlying distribution of probability.[1] Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population.\\n'}, {'link': '/wiki/Sample_(statistics)', 'title': 'Sampling (statistics)', 'paragraph': 'In statistics, quality assurance, and survey methodology, sampling is the selection of a subset or a statistical sample (termed sample for short) of individuals from within a statistical population to estimate characteristics of the whole population. Statisticians attempt to collect samples that are representative of the population. Sampling has lower costs and faster data collection compared to recording data from the entire population, and thus, it can provide insights in cases where it is infeasible to measure an entire population. \\n'}, {'link': '/wiki/Michael_I._Jordan', 'title': 'Michael I. Jordan', 'paragraph': 'Michael Irwin Jordan ForMemRS[6] (born February 25, 1956) is an American scientist, professor at the University of California, Berkeley and researcher in machine learning, statistics, and artificial intelligence.[7][8][9]\\n'}, {'link': '/wiki/Leo_Breiman', 'title': 'Leo Breiman', 'paragraph': 'Leo Breiman (January 27, 1928 – July 5, 2005) was a distinguished statistician at the University of California, Berkeley. He was the recipient of numerous honors and awards,[citation needed] and was a member of the United States National Academy of Sciences.\\n'}, {'link': '/wiki/Medical_diagnostics', 'title': 'Medical diagnosis', 'paragraph': \"Medical diagnosis (abbreviated Dx,[1] Dx, or Ds) is the process of determining which disease or condition explains a person's symptoms and signs. It is most often referred to as diagnosis with the medical context being implicit. The information required for diagnosis is typically collected from a history and physical examination of the person seeking medical care. Often, one or more diagnostic procedures, such as medical tests, are also done during the process. Sometimes the posthumous diagnosis is considered a kind of medical diagnosis.\\n\"}, {'link': '/wiki/Theoretical_computer_science', 'title': 'Theoretical computer science', 'paragraph': 'Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, formal language theory, the lambda calculus and type theory.\\n'}, {'link': '/wiki/Errors_and_residuals', 'title': 'Errors and residuals', 'paragraph': 'In statistics and optimization, errors and residuals are two closely related and easily confused measures of the deviation of an observed value of an element of a statistical sample from its \"true value\" (not necessarily observable). The error of an observation is the deviation of the observed value from the true value of a quantity of interest (for example, a population mean). The residual is the difference between the observed value and the estimated value of the quantity of interest (for example, a sample mean). The distinction is most important in regression analysis, where the concepts are sometimes called the regression errors and regression residuals and where they lead to the concept of studentized residuals.\\nIn econometrics, \"errors\" are also called disturbances.[1][2][3]\\n'}, {'link': '/wiki/Overfitting', 'title': 'Overfitting', 'paragraph': 'In mathematical modeling, overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably\".[1] An overfitted model is a mathematical model that contains more parameters than can be justified by the data.[2] In a mathematical sense, these parameters represent the degree of a polynomial. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.[3]:\\u200a45\\u200a\\n'}, {'link': '/wiki/Time_complexity#Polynomial_time', 'title': 'Time complexity', 'paragraph': 'In theoretical computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a constant factor.\\n'}, {'link': '/wiki/Map_(mathematics)', 'title': 'Map (mathematics)', 'paragraph': 'In mathematics, a map or mapping is a function in its general sense.[1]  These terms may have originated as from the process of making a geographical map: mapping the Earth surface to a sheet of paper.[2]\\n'}, {'link': '/wiki/File:Svm_max_sep_hyperplane_with_margin.png', 'title': 'File:Svm max sep hyperplane with margin.png', 'paragraph': 'Original file \\u200e(800 × 862 pixels, file size: 78 KB, MIME type: image/png)\\n'}, {'link': '/wiki/Linear_classifier', 'title': 'Linear classifier', 'paragraph': \"In the field of machine learning, the goal of statistical classification is to use an object's characteristics to identify which class (or group) it belongs to.  A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics.  An object's characteristics are also known as feature values and are typically presented to the machine in a vector called a feature vector. Such classifiers work well for practical problems such as document classification, and more generally for problems with many variables (features), reaching accuracy levels comparable to non-linear classifiers while taking less time to train and use.[1]\\n\"}, {'link': '/wiki/Feature_vector', 'title': 'Feature (machine learning)', 'paragraph': 'In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon.[1] Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of \"feature\" is related to that of explanatory variable used in statistical techniques such as linear regression.\\n'}, {'link': '/wiki/Matrix_(mathematics)', 'title': 'Matrix (mathematics)', 'paragraph': 'In mathematics, a matrix (pl.: matrices) is a rectangular array or table of numbers, symbols, or expressions, arranged in rows and columns, which is used to represent a mathematical object or a property of such an object.\\n'}, {'link': '/wiki/Similarity_learning', 'title': 'Similarity learning', 'paragraph': 'Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.\\n'}, {'link': '/wiki/Ranking', 'title': 'Ranking', 'paragraph': 'A ranking is a relationship between a set of items such that, for any two items, the first is either \"ranked higher than\", \"ranked lower than\", or \"ranked equal to\" the second.[1] In mathematics, this is known as a weak order or total preorder of objects. It is not necessarily a total order of objects because two different objects can have the same ranking. The rankings themselves are totally ordered. For example, materials are totally preordered by hardness, while degrees of hardness are totally ordered. If two items are the same in rank it is considered a tie.\\n'}, {'link': '/wiki/Indel', 'title': 'Indel', 'paragraph': 'Indel (insertion-deletion) is a molecular biology term for an insertion or deletion of bases in the genome of an organism. Indels ≥ 50 bases in length are classified as structural variants.[1][2]\\n'}, {'link': '/wiki/Haplotype', 'title': 'Haplotype', 'paragraph': 'A haplotype (haploid genotype) is a group of alleles in an organism that are inherited together from a single parent.[1][2]\\n'}, {'link': '/wiki/Pan-genome', 'title': 'Pan-genome', 'paragraph': 'In the fields of molecular biology and genetics, a pan-genome (pangenome or supragenome) is the entire set of genes from all strains within a clade. More generally, it is the union of all the genomes of a clade.[2][3][4][5] The pan-genome can be broken down into a \"core pangenome\" that contains genes present in all individuals, a \"shell pangenome\" that contains genes present in two or more strains, and a \"cloud pangenome\" that contains genes only found in a single strain.[3][4][6] Some authors also refer to the cloud genome as \"accessory genome\" containing \\'dispensable\\' genes present in a subset of the strains and strain-specific genes.[2][3][4] Note that the use of the term \\'dispensable\\' has been questioned, at least in plant genomes, as accessory genes play \"an important role in genome evolution and in the complex interplay between the genome and the environment\".[5] The field of study of pangenomes is called pangenomics.[2]\\n'}, {'link': '/wiki/File:CLIPS.jpg', 'title': 'File:CLIPS.jpg', 'paragraph': 'Original file \\u200e(3,866 × 921 pixels, file size: 328 KB, MIME type: image/jpeg)\\n'}, {'link': '/wiki/Action_selection', 'title': 'Action selection', 'paragraph': 'Action selection is a way of characterizing the most basic problem of intelligent systems: what to do next. In artificial intelligence and computational cognitive science, \"the action selection problem\" is typically associated with intelligent agents and animats—artificial systems that exhibit complex behaviour in an agent environment. The term is also sometimes used in ethology or animal behavior.\\n'}, {'link': '/wiki/Operations_research', 'title': 'Operations research', 'paragraph': '\\nOperations research (British English: operational research) (U.S. Air Force Specialty Code: Operations Analysis), often shortened to the initialism OR, is a discipline that deals with the development and application of analytical methods to improve decision-making.[1] The term management science is occasionally used as a synonym.[2]\\n'}, {'link': '/wiki/Information_theory', 'title': 'Information theory', 'paragraph': 'Information theory is the mathematical study of the quantification, storage, and communication of information.[1] The field was originally established by the works of Harry Nyquist and Ralph Hartley, in the 1920s, and Claude Shannon in the 1940s.[2]:\\u200avii\\u200a The field, in  applied mathematics, is at the intersection of probability theory, statistics, computer science, statistical mechanics, information engineering, and electrical engineering.\\n'}, {'link': '/wiki/Simulation-based_optimization', 'title': 'Simulation-based optimization', 'paragraph': 'Simulation-based optimization (also known as simply simulation optimization) integrates optimization techniques into simulation modeling and analysis. Because of the complexity of the simulation, the objective function may become difficult and expensive to evaluate. Usually, the underlying simulation model is stochastic, so that the objective function must be estimated using statistical estimation techniques (called output analysis in simulation methodology).\\n'}, {'link': '/wiki/Swarm_intelligence', 'title': 'Swarm intelligence', 'paragraph': 'Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems.[1]\\n'}, {'link': '/wiki/Markov_decision_process', 'title': 'Markov decision process', 'paragraph': \"In mathematics, a Markov decision process (MDP) is a discrete-time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming. MDPs were known at least as early as the 1950s;[1] a core body of research on Markov decision processes resulted from Ronald Howard's 1960 book, Dynamic Programming and Markov Processes.[2] They are used in many disciplines, including robotics, automatic control, economics and manufacturing. The name of MDPs comes from the Russian mathematician Andrey Markov as they are an extension of Markov chains.\\n\"}, {'link': '/wiki/Dynamic_programming', 'title': 'Dynamic programming', 'paragraph': 'Dynamic programming is both a mathematical optimization method and an algorithmic paradigm. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics.\\n'}, {'link': '/wiki/Manifold_hypothesis', 'title': 'Manifold hypothesis', 'paragraph': 'The manifold hypothesis posits that many high-dimensional data sets that occur in the real world actually lie along low-dimensional latent manifolds inside that high-dimensional space.[1][2][3][4] As a consequence of the manifold hypothesis, many data sets that appear to initially require many variables to describe, can actually be described by a comparatively small number of variables, likened to the local coordinate system of the underlying manifold. It is suggested that this principle underpins the effectiveness of machine learning algorithms in describing high-dimensional data sets by considering a few common features.\\n'}, {'link': '/wiki/Manifold', 'title': 'Manifold', 'paragraph': 'In mathematics, a manifold is a topological space that locally resembles Euclidean space near each point. More precisely, an \\n\\n\\n\\nn\\n\\n\\n{\\\\displaystyle n}\\n\\n-dimensional manifold, or \\n\\n\\n\\nn\\n\\n\\n{\\\\displaystyle n}\\n\\n-manifold for short, is a topological space with the property that each point has a neighborhood that is homeomorphic to an open subset of \\n\\n\\n\\nn\\n\\n\\n{\\\\displaystyle n}\\n\\n-dimensional Euclidean space.\\n'}, {'link': '/wiki/Manifold_learning', 'title': 'Nonlinear dimensionality reduction', 'paragraph': 'Nonlinear dimensionality reduction, also known as manifold learning, refers to various related techniques that aim to project high-dimensional data onto lower-dimensional latent manifolds, with the goal of either visualizing the data in the low-dimensional space, or learning the mapping (either from the high-dimensional space to the low-dimensional embedding or vice versa) itself.[1][2] The techniques described below can be understood as generalizations of linear decomposition methods used for dimensionality reduction, such as singular value decomposition and principal component analysis.\\n'}, {'link': '/wiki/Manifold_regularization', 'title': 'Manifold regularization', 'paragraph': 'In machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is smooth: data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition.\\n'}, {'link': '/wiki/Topic_modeling', 'title': 'Topic model', 'paragraph': 'In statistics and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear approximately equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document\\'s balance of topics is.\\n'}, {'link': '/wiki/Multilayer_perceptron', 'title': 'Multilayer perceptron', 'paragraph': 'A multilayer perceptron (MLP) is a misnomer for a modern feedforward artificial neural network, consisting of fully connected neurons with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not linearly separable.[1] It is a misnomer because the original perceptron used a Heaviside step function, instead of a nonlinear kind of activation function (used by modern networks).\\n'}, {'link': '/wiki/Matrix_decomposition', 'title': 'Matrix decomposition', 'paragraph': 'In the mathematical discipline of linear algebra, a matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. There are many different matrix decompositions; each finds use among a particular class of problems.\\n'}, {'link': '/wiki/Sparse_coding', 'title': 'Neural coding', 'paragraph': 'Neural coding (or neural representation) is a neuroscience field concerned with characterising the hypothetical relationship between the stimulus and the individual or ensemble neuronal responses and the relationship among the electrical activity of the neurons in the ensemble.[1][2] Based on the theory that\\nsensory and other information is represented in the brain by networks of neurons, it is thought that neurons can encode both digital and analog information.[3]\\n'}, {'link': '/wiki/Multilinear_subspace_learning', 'title': 'Multilinear subspace learning', 'paragraph': 'Multilinear subspace learning is an approach for disentangling the causal factor of data formation and performing  dimensionality reduction.[1][2][3][4][5]   \\nThe Dimensionality reduction can be performed on a data tensor that contains a collection of observations have been vectorized,[1] or observations that are treated as matrices and concatenated into a data tensor.[6][7]  Here are some examples of data tensors whose observations are vectorized  or whose observations are matrices concatenated into data tensor images (2D/3D), video sequences (3D/4D), and hyperspectral cubes (3D/4D).\\n'}, {'link': '/wiki/Tensor', 'title': 'Tensor', 'paragraph': 'In mathematics, a tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects related to a vector space. Tensors may map between different objects such as vectors, scalars, and even other tensors. There are many types of tensors, including scalars and vectors (which are the simplest tensors), dual vectors, multilinear maps between vector spaces, and even some operations such as the dot product. Tensors are defined independent of any basis, although they are often referred to by their components in a basis related to a particular coordinate system; those components form an array, which can be thought of as a high-dimensional matrix. \\n'}, {'link': '/wiki/Basis_function', 'title': 'Basis function', 'paragraph': 'In mathematics,  a basis function is an element of a particular basis for a function space. Every function in the function space can be represented as a linear combination of basis functions, just as every vector in a vector space can be represented as a linear combination of basis vectors.\\n'}, {'link': '/wiki/Sparse_matrix', 'title': 'Sparse matrix', 'paragraph': 'In numerical analysis and scientific computing, a sparse matrix or sparse array is a matrix in which most of the elements are zero.[1] There is no strict definition regarding the proportion of zero-value elements for a matrix to qualify as sparse but a common criterion is that the number of non-zero elements is roughly equal to the number of rows or columns. By contrast, if most of the elements are non-zero, the matrix is considered dense.[1] The number of zero-valued elements divided by the total number of elements (e.g., m × n for an m × n matrix) is sometimes referred to as the sparsity of the matrix.\\n'}, {'link': '/wiki/Strongly_NP-hard', 'title': 'Strong NP-completeness', 'paragraph': 'In computational complexity, strong NP-completeness is a property of computational problems that is a special case of NP-completeness. A general computational problem may have numerical parameters.  For example, the input to the bin packing problem is a list of objects of specific sizes and a size for the bins that must contain the objects—these object sizes and bin size are numerical parameters.\\n'}, {'link': '/wiki/Heuristic', 'title': 'Heuristic', 'paragraph': \"A heuristic (/hjʊˈrɪstɪk/; from Ancient Greek  εὑρίσκω (heurískō)\\xa0'to find, discover'), or heuristic technique, is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision.[1][2]\\n\"}, {'link': '/wiki/K-SVD', 'title': 'k-SVD', 'paragraph': 'In applied mathematics, k-SVD is a dictionary learning algorithm for creating a dictionary for sparse representations, via a singular value decomposition approach. k-SVD is a generalization of the k-means clustering method, and it works by iteratively alternating between sparse coding the input data based on the current dictionary, and updating the atoms in the dictionary to better fit the data. It is structurally related to the expectation maximization (EM) algorithm.[1][2] k-SVD can be found widely in use in applications such as image processing, audio processing, biology, and document analysis.\\n'}, {'link': '/wiki/Bank_fraud', 'title': 'Bank fraud', 'paragraph': 'Bank fraud is the use of potentially illegal means to obtain money, assets, or other property owned or held by a financial institution, or to obtain money from depositors by fraudulently posing as a bank or other financial institution.[1] In many instances, bank fraud is a criminal offence. While the specific elements of particular banking fraud laws vary depending on jurisdictions, the term bank fraud applies to actions that employ a scheme or artifice, as opposed to bank robbery or theft. For this reason, bank fraud is sometimes considered a white-collar crime.[2]\\n'}, {'link': '/wiki/Robot_learning', 'title': 'Robot learning', 'paragraph': 'Robot learning is a research field at the intersection of machine learning and robotics. It studies techniques allowing a robot to acquire novel skills or adapt to its environment through learning algorithms. The embodiment of the robot, situated in a physical embedding, provides at the same time specific difficulties (e.g. high-dimensionality, real time constraints for collecting data and learning) and opportunities for guiding the learning process (e.g. sensorimotor synergies, motor primitives).\\n'}, {'link': '/wiki/Learning_classifier_system', 'title': 'Learning classifier system', 'paragraph': 'Learning classifier systems, or LCS, are a paradigm of rule-based machine learning methods that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning).[2]  Learning classifier systems seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions (e.g. behavior modeling,[3] classification,[4][5] data mining,[5][6][7] regression,[8] function approximation,[9] or game strategy).  This approach allows complex solution spaces to be broken up into smaller, simpler parts.\\n'}, {'link': '/wiki/Artificial_immune_system', 'title': 'Artificial immune system', 'paragraph': \"In artificial intelligence, artificial immune systems (AIS) are a class of computationally intelligent, rule-based machine learning systems inspired by the principles and processes of the vertebrate immune system. The algorithms are typically modeled after the immune system's characteristics of learning and memory for use in problem-solving.\\n\"}, {'link': '/wiki/Rakesh_Agrawal_(computer_scientist)', 'title': 'Rakesh Agrawal (computer scientist)', 'paragraph': \"Rakesh Agrawal (हिन्दी - राकेश अग्रवाल) is a computer scientist who until recently was a Technical Fellow at the Microsoft Search Labs.[1] Rakesh is well known for developing fundamental data mining concepts and technologies and pioneering key concepts in data privacy, including Hippocratic Database, Sovereign Information Sharing, and Privacy-Preserving Data Mining. IBM's commercial data mining product, Intelligent Miner, grew out of his work. His research has been incorporated into other IBM products, including DB2 Mining Extender, DB2 OLAP Server and WebSphere Commerce Server, and has influenced several other commercial and academic products, prototypes and applications. His other technical contributions include Polyglot object-oriented type system, Alert active database system, Ode (Object database and environment), Alpha (extension of relational databases with generalized transitive closure), Nest distributed system, transaction management, and database machines.\\n\"}, {'link': '/wiki/Tomasz_Imieli%C5%84ski', 'title': 'Tomasz Imieliński', 'paragraph': 'Tomasz Imieliński (born July 11, 1954, in Toruń, Poland) is a Polish-American computer scientist, most known in the areas of data mining, mobile computing, data extraction, and search engine technology. He is currently a professor of computer science at Rutgers University in New Jersey, United States.\\n'}, {'link': '/wiki/Point-of-sale', 'title': 'Point of sale', 'paragraph': 'The point of sale (POS) or point of purchase (POP) is the time and place at which a retail transaction is completed.  At the point of sale, the merchant calculates the amount owed by the customer, indicates that amount, may prepare an invoice for the customer (which may be a cash register printout), and indicates the options for the customer to make payment.  It is also the point at which a customer makes a payment to the merchant in exchange for goods or after provision of a service.  After receiving payment, the merchant may issue a receipt for the transaction, which is usually printed but can also be dispensed with or sent electronically.[1][2][3]\\n'}, {'link': '/wiki/Pricing', 'title': 'Pricing', 'paragraph': \"Pricing is the process whereby a business sets the price at which it will sell its products and services, and may be part of the business's marketing plan. In setting prices, the business will take into account the price at which it could acquire the goods, the manufacturing cost, the marketplace, competition, market condition, brand, and quality of product.\\n\"}, {'link': '/wiki/Product_placement', 'title': 'Product placement', 'paragraph': 'Product placement, also known as embedded marketing,[1][2][3][4] is a marketing technique where references to specific brands or products are incorporated into another work, such as a film or television program, with specific promotional intent.  Much of this is done by loaning products, especially when expensive items, such as vehicles, are involved.[5]  In 2021, the agreements between brand owners and films and television programs were worth more than US$20 billion.[5]\\n'}, {'link': '/wiki/Market_basket_analysis', 'title': 'Affinity analysis', 'paragraph': 'Affinity analysis falls under the umbrella term of data mining which uncovers meaningful correlations between different entities according to their co-occurrence in a data set. In almost all systems and processes, the application of affinity analysis can extract significant knowledge about the unexpected trends[citation needed]. In fact, affinity analysis takes advantages of studying attributes that go together which helps uncover the hidden pattens in a big data through generating association rules. Association rules mining procedure is two-fold: first, it finds all frequent attributes in a data set and, then generates association rules satisfying some predefined criteria, support and confidence, to identify the most important relationships in the frequent itemset. The first step in the process is to count the co-occurrence of attributes in the data set. Next, a subset is created called the frequent itemset. The association rules mining takes the form of if a condition or feature (A) is present then another condition or feature (B) exists. The first condition or feature (A) is called antecedent and the latter (B) is known as consequent. This process is repeated until no additional frequent itemsets are found.\\xa0 There are two important metrics for performing the association rules mining technique: support and confidence. Also, a priori algorithm is used to reduce the search space for the problem.[1]\\n'}, {'link': '/wiki/Intrusion_detection', 'title': 'Intrusion detection system', 'paragraph': 'An intrusion detection system (IDS; also intrusion prevention system or IPS) is a device or software application that monitors a network or systems for malicious activity or policy violations.[1] Any intrusion activity or violation is typically reported either to an administrator or collected centrally using a security information and event management (SIEM) system. A SIEM system combines outputs from multiple sources and uses alarm filtering techniques to distinguish malicious activity from false alarms.[2]\\n'}, {'link': '/wiki/Continuous_production', 'title': 'Continuous production', 'paragraph': 'Continuous production is a flow production method used to manufacture, produce, or process materials without interruption.  Continuous production is called a continuous process or a continuous flow process because the materials, either dry bulk or fluids that are being processed are continuously in motion, undergoing chemical reactions or subject to mechanical or heat treatment.  Continuous processing is contrasted with batch production.\\n'}, {'link': '/wiki/Sequence_mining', 'title': 'Sequential pattern mining', 'paragraph': 'Sequential pattern mining is a topic of data mining concerned with finding statistically relevant patterns between data examples where the values are delivered in a sequence.[1][2] It is usually presumed that the values are discrete, and thus time series mining is closely related, but usually considered a different activity.  Sequential pattern mining is a special case of structured data mining.\\n'}, {'link': '/wiki/Piecewise', 'title': 'Piecewise', 'paragraph': 'In mathematics, a piecewise-defined function (also called a piecewise function, a hybrid function, or definition by cases) is a function defined by multiple sub-functions, where each sub-function applies to a different interval in the domain.[1][2][3] Piecewise definition is actually a way of expressing the function, rather than a characteristic of the function itself.\\n'}, {'link': '/wiki/Logic_programming', 'title': 'Logic programming', 'paragraph': 'Logic programming is a programming, database and knowledge-representation and reasoning paradigm which is based on formal logic. A program, database or knowledge base in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain.  Major logic programming language families include Prolog, Answer Set Programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:\\n'}, {'link': '/wiki/Entailment', 'title': 'Logical consequence', 'paragraph': 'Logical consequence (also entailment) is a fundamental concept in logic which describes the relationship between statements that hold true when one statement logically follows from one or more statements. A valid logical argument is one in which the conclusion is entailed by the premises, because the conclusion is the consequence of the premises. The philosophical analysis of logical consequence involves the questions: In what sense does a conclusion follow from its premises? and What does it mean for a conclusion to be a consequence of premises?[1] All of philosophical logic is meant to provide accounts of the nature of logical consequence and the nature of logical truth.[2]\\n'}, {'link': '/wiki/Inductive_programming', 'title': 'Inductive programming', 'paragraph': 'Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.\\n'}, {'link': '/wiki/Functional_programming', 'title': 'Functional programming', 'paragraph': 'In computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions. It is a declarative programming paradigm in which function definitions are trees of expressions that map values to other values, rather than a sequence of imperative statements which update the running state of the program.\\n'}, {'link': '/wiki/Ehud_Shapiro', 'title': 'Ehud Shapiro', 'paragraph': \"Ehud Shapiro (Hebrew: אהוד שפירא; born 1955) is an Israeli scientist, artist, and entrepreneur, who is Professor of Computer Science and Biology at the Weizmann Institute of Science.[2] With international reputation, he made fundamental contributions to many scientific disciplines,[3] laying in each a long-term research agenda by asking a novel basic question and offering a first step towards answering it, including how to computerize the process of scientific discovery, by providing an algorithmic interpretation to Karl Popper's methodology of conjectures and refutations;[4][5] how to automate program debugging, by algorithms for fault localization;[6] how to unify parallel, distributed, and systems programming with a high-level logic-based programming language;[7] how to use the metaverse as a foundation for social networking;[8] how to devise molecular computers that can function as smart programmable drugs;[9][10] how to uncover the human cell lineage tree, via single-cell genomics;[11][12] how to support digital democracy, by devising an alternative architecture to the digital realm.[13][14]\\n\"}, {'link': '/wiki/Statistical_model', 'title': 'Statistical model', 'paragraph': 'A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process.[1] When referring specifically to probabilities, the corresponding term is probabilistic model.\\n'}, {'link': '/wiki/File:Colored_neural_network.svg', 'title': 'File:Colored neural network.svg', 'paragraph': 'Original file \\u200e(SVG file, nominally 296 × 356 pixels, file size: 206 KB)\\n'}, {'link': '/wiki/Neuron', 'title': 'Neuron', 'paragraph': 'Within a nervous system, a neuron, neurone, or nerve cell is an electrically excitable cell that fires electric signals called action potentials across a neural network. Neurons communicate with other cells via synapses, which are specialized connections that commonly use minute amounts of chemical neurotransmitters to pass the electric signal from the presynaptic neuron to the target cell through the synaptic gap. \\n'}, {'link': '/wiki/Artificial_neuron', 'title': 'Artificial neuron', 'paragraph': \"An artificial neuron is a mathematical function conceived as a model of biological neurons in a neural network. Artificial neurons are the elementary units of artificial neural networks.[1] The artificial neuron receives one or more inputs (representing excitatory postsynaptic potentials and inhibitory postsynaptic potentials at neural dendrites) and sums them to produce an output (or activation, representing a neuron's action potential which is transmitted along its axon). Usually, each input is separately weighted (representing the synaptic weight), and the sum is often added to a term known as a bias (loosely corresponding to the threshold potential), before being passed through a non-linear function known as an activation function or transfer function[clarification needed]. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often monotonically increasing, continuous, differentiable and bounded. Non-monotonic, unbounded and oscillating activation functions with multiple zeros that outperform sigmoidal and ReLU-like activation functions on many tasks have also been recently explored. The thresholding function has inspired building logic gates referred to as threshold logic; applicable to building logic circuits resembling brain processing. For example, new devices such as memristors have been extensively used to develop such logic in recent times.[2]\\n\"}, {'link': '/wiki/Biological_neural_network', 'title': 'Neural circuit', 'paragraph': 'A neural circuit (also known as a biological neural network BNNs) is a population of neurons interconnected by synapses to carry out a specific function when activated.[1] Multiple neural circuits interconnect with one another to form large scale brain networks.[2]\\n'}, {'link': '/wiki/Synapse', 'title': 'Synapse', 'paragraph': 'In the nervous system, a synapse[1] is a structure that permits a neuron (or nerve cell) to pass an electrical or chemical signal to another neuron or to the target effector cell.\\n'}, {'link': '/wiki/Real_number', 'title': 'Real number', 'paragraph': 'In mathematics, a real number is a number that can be used to measure a continuous one-dimensional quantity such as a distance, duration or temperature. Here, continuous means that pairs of values can have arbitrarily small differences.[a] Every real number can be almost uniquely represented by an infinite decimal expansion.[b][1]\\n'}, {'link': '/wiki/Weight_(mathematics)', 'title': 'Weighting', 'paragraph': 'The process of weighting involves emphasizing the contribution of particular aspects of a phenomenon (or of a set of data) over others to an outcome or result; thereby highlighting those aspects in comparison to others in the analysis. That is, rather than each variable in the data set contributing equally to the final result, some of the data is adjusted to make a greater contribution than others. This is analogous to the practice of adding (extra) weight to one side of a pair of scales in order to favour either the buyer or seller.\\n'}, {'link': '/wiki/Social_network', 'title': 'Social network', 'paragraph': 'A social network is a social structure made up of a set of social actors (such as individuals or organizations), sets of dyadic ties, and other social interactions between actors. The social network perspective provides a set of methods for analyzing the structure of whole social entities as well as a variety of theories explaining the patterns observed in these structures.[1] The study of these structures uses social network analysis to identify local and global patterns, locate influential entities, and examine network dynamics.\\n'}, {'link': '/wiki/File:Decision_Tree.jpg', 'title': 'File:Decision Tree.jpg', 'paragraph': 'Decision_Tree.jpg \\u200e(457 × 473 pixels, file size: 17 KB, MIME type: image/jpeg)\\n'}, {'link': '/wiki/Predictive_modeling', 'title': 'Predictive modelling', 'paragraph': 'Predictive modelling uses statistics to predict outcomes.[1] Most often the event one wants to predict is in the future, but predictive modelling can be applied to any type of unknown event, regardless of when it occurred. For example, predictive models are often used to detect crimes and identify suspects, after the crime has taken place.[2]\\n'}, {'link': '/wiki/Leaf_node', 'title': 'Tree (data structure)', 'paragraph': 'In computer science, a tree is a widely used abstract data type that represents a hierarchical tree structure with a set of connected nodes. Each node in the tree can be connected to many children (depending on the type of tree), but must be connected to exactly one parent,[1] except for the root node, which has no parent (i.e., the root node as the top-most node in the tree hierarchy). These constraints mean there are no cycles or \"loops\" (no node can be its own ancestor), and also that each child can be treated like the root node of its own subtree, making recursion a useful technique for tree traversal. In contrast to linear data structures, many trees cannot be represented by relationships between neighboring nodes (parent and children nodes of a node under consideration if they exists) in a single straight line (called edge or link between two adjacent nodes).\\n'}, {'link': '/wiki/Logical_conjunction', 'title': 'Logical conjunction', 'paragraph': 'In logic, mathematics and linguistics, and (\\n\\n\\n\\n∧\\n\\n\\n{\\\\displaystyle \\\\wedge }\\n\\n) is the truth-functional operator of conjunction or logical conjunction. The logical connective of this operator is typically represented as \\n\\n\\n\\n∧\\n\\n\\n{\\\\displaystyle \\\\wedge }\\n\\n[1] or \\n\\n\\n\\n&\\n\\n\\n{\\\\displaystyle \\\\&}\\n\\n or \\n\\n\\n\\nK\\n\\n\\n{\\\\displaystyle K}\\n\\n (prefix) or \\n\\n\\n\\n×\\n\\n\\n{\\\\displaystyle \\\\times }\\n\\n or \\n\\n\\n\\n⋅\\n\\n\\n{\\\\displaystyle \\\\cdot }\\n\\n[2] in which \\n\\n\\n\\n∧\\n\\n\\n{\\\\displaystyle \\\\wedge }\\n\\n is the most modern and widely used.\\n'}, {'link': '/wiki/Decision_making', 'title': 'Decision-making', 'paragraph': 'In psychology, decision-making (also spelled decision making and decisionmaking) is regarded as the cognitive process resulting in the selection of a belief or a course of action among several possible alternative options. It could be either rational or irrational. The decision-making process is a reasoning process based on assumptions of values, preferences and beliefs of the decision-maker.[1] Every decision-making process produces a final choice, which may or may not prompt action.\\n'}, {'link': '/wiki/Probabilistic_classification', 'title': 'Probabilistic classification', 'paragraph': 'In machine learning, a probabilistic classifier is a classifier that is able to predict, given an observation of an input, a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to. Probabilistic classifiers provide classification that can be useful in its own right[1] or when combining classifiers into ensembles.\\n'}, {'link': '/wiki/Binary_classifier', 'title': 'Binary classification', 'paragraph': 'Binary classification is the task of classifying the elements of a set into two groups (each called class) on the basis of a classification rule. Typical binary classification problems include:\\n'}, {'link': '/wiki/Platt_scaling', 'title': 'Platt scaling', 'paragraph': \"In machine learning, Platt scaling or Platt calibration is a way of transforming the outputs of a classification model into a probability distribution over classes. The method was invented by John Platt in the context of support vector machines,[1]\\nreplacing an earlier method by Vapnik,\\nbut can be applied to other classification models.[2]\\nPlatt scaling works by fitting a logistic regression model to a classifier's scores.\\n\"}, {'link': '/wiki/File:Linear_regression.svg', 'title': 'File:Linear regression.svg', 'paragraph': 'Original file \\u200e(SVG file, nominally 438 × 289 pixels, file size: 71 KB)\\n'}, {'link': '/wiki/Ordinary_least_squares', 'title': 'Ordinary least squares', 'paragraph': 'In statistics, ordinary least squares (OLS) is a type of linear least squares method for choosing the unknown parameters in a linear regression model (with fixed level-one effects of a linear function of a set of explanatory variables) by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the input dataset and the output of the (linear) function of the independent variable.\\n'}, {'link': '/wiki/Regularization_(mathematics)', 'title': 'Regularization (mathematics)', 'paragraph': 'In mathematics, statistics, finance,[1] computer science, particularly in machine learning and inverse problems, regularization is a process that changes the result answer to be \"simpler\". It is often used to obtain results for ill-posed problems or to prevent overfitting.[2]\\n'}, {'link': '/wiki/Ridge_regression', 'title': 'Ridge regression', 'paragraph': 'Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated.[1] It has been used in many fields including econometrics, chemistry, and engineering.[2] Also known as Tikhonov regularization, named for Andrey Tikhonov, it is a method of regularization of ill-posed problems.[a] It is particularly useful to mitigate the problem of multicollinearity in linear regression, which commonly occurs in models with large numbers of parameters.[3] In general, the method provides improved efficiency in parameter estimation problems in exchange for a tolerable amount of bias (see bias–variance tradeoff).[4]\\n'}, {'link': '/wiki/Polynomial_regression', 'title': 'Polynomial regression', 'paragraph': 'In statistics, polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y\\xa0|x). Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function E(y\\xa0|\\xa0x) is linear in the unknown parameters that are estimated from the data.  For this reason, polynomial regression is considered to be a special case of multiple linear regression.\\n'}, {'link': '/wiki/Kernel_regression', 'title': 'Kernel regression', 'paragraph': 'In statistics, kernel regression is a non-parametric technique to estimate the conditional expectation of a random variable. The objective is to find a non-linear relation between a pair of random variables X and Y.\\n'}, {'link': '/wiki/File:SimpleBayesNetNodes.svg', 'title': 'File:SimpleBayesNetNodes.svg', 'paragraph': 'Original file \\u200e(SVG file, nominally 246 × 128 pixels, file size: 5 KB)\\n'}, {'link': '/wiki/Random_variables', 'title': 'Random variable', 'paragraph': \"A random variable (also called random quantity, aleatory variable, or stochastic variable) is a mathematical formalization of a quantity or object which depends on random events.[1] The term 'random variable' can be misleading as its mathematical definition is not actually random nor a variable,[2] but rather it is a function from possible outcomes (e.g., the possible upper sides of a flipped coin such as heads \\n\\n\\n\\nH\\n\\n\\n{\\\\displaystyle H}\\n\\n and tails \\n\\n\\n\\nT\\n\\n\\n{\\\\displaystyle T}\\n\\n) in a sample space (e.g., the set \\n\\n\\n\\n{\\nH\\n,\\nT\\n}\\n\\n\\n{\\\\displaystyle \\\\{H,T\\\\}}\\n\\n) to a measurable space (e.g., \\n\\n\\n\\n{\\n−\\n1\\n,\\n1\\n}\\n\\n\\n{\\\\displaystyle \\\\{-1,1\\\\}}\\n\\n in which 1 is corresponding to \\n\\n\\n\\nH\\n\\n\\n{\\\\displaystyle H}\\n\\n and −1 is corresponding to \\n\\n\\n\\nT\\n\\n\\n{\\\\displaystyle T}\\n\\n, respectively), often to the real numbers.\\n\"}, {'link': '/wiki/Conditional_independence', 'title': 'Conditional independence', 'paragraph': 'In probability theory, conditional independence describes situations wherein an observation is irrelevant or redundant when evaluating the certainty of a hypothesis. Conditional independence is usually formulated in terms of conditional probability, as a special case where the probability of the hypothesis given the uninformative observation is equal to the probability without. If \\n\\n\\n\\nA\\n\\n\\n{\\\\displaystyle A}\\n\\n is the hypothesis, and \\n\\n\\n\\nB\\n\\n\\n{\\\\displaystyle B}\\n\\n and \\n\\n\\n\\nC\\n\\n\\n{\\\\displaystyle C}\\n\\n are observations, conditional independence can be stated as an equality:\\n'}, {'link': '/wiki/Inference', 'title': 'Inference', 'paragraph': 'Inferences are steps in reasoning, moving from premises to logical consequences; etymologically, the word infer means to \"carry forward\". Inference is theoretically traditionally divided into deduction and induction, a distinction that in Europe dates at least to Aristotle (300s BCE). Deduction is inference deriving logical conclusions from premises known or assumed to be true, with the laws of valid inference being studied in logic. Induction is inference from particular evidence to a universal conclusion. A third type of inference is sometimes distinguished, notably by Charles Sanders Peirce, contradistinguishing abduction from induction.\\n'}, {'link': '/wiki/Peptide_sequence', 'title': 'Protein primary structure', 'paragraph': 'Protein primary structure is the linear sequence of amino acids in a peptide or protein.[1] By convention, the primary structure of a protein is reported starting from the amino-terminal (N) end to the carboxyl-terminal (C) end. Protein biosynthesis is most commonly performed by ribosomes in cells. Peptides can also be synthesized in the laboratory. Protein primary structures can be directly sequenced, or inferred from DNA sequences.\\n'}, {'link': '/wiki/Dynamic_Bayesian_network', 'title': 'Dynamic Bayesian network', 'paragraph': 'A dynamic Bayesian network (DBN) is a Bayesian network (BN) which relates variables to each other over adjacent time steps. \\n'}, {'link': '/wiki/Influence_diagram', 'title': 'Influence diagram', 'paragraph': 'An influence diagram (ID) (also called a relevance diagram, decision diagram or a decision network) is a compact graphical and mathematical representation of a decision situation.  It is a generalization of a Bayesian network, in which not only probabilistic inference problems but also decision making problems (following the maximum expected utility criterion) can be modeled and solved.\\n'}, {'link': '/wiki/Gaussian_processes', 'title': 'Gaussian process', 'paragraph': 'In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.\\n'}, {'link': '/wiki/File:Regressions_sine_demo.svg', 'title': 'File:Regressions sine demo.svg', 'paragraph': 'Original file \\u200e(SVG file, nominally 900 × 450 pixels, file size: 582 KB)\\n'}, {'link': '/wiki/Stochastic_process', 'title': 'Stochastic process', 'paragraph': 'In probability theory and related fields, a stochastic (/stəˈkæstɪk/) or random process is a mathematical object usually defined as a sequence of random variables, where the index of the sequence has the interpretation of time. Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner. Examples include the growth of a bacterial population, an electrical current fluctuating due to thermal noise, or the movement of a gas molecule.[1][4][5] Stochastic processes have applications in many disciplines such as biology,[6] chemistry,[7] ecology,[8] neuroscience,[9] physics,[10] image processing, signal processing,[11] control theory,[12] information theory,[13] computer science,[14] and telecommunications.[15] Furthermore, seemingly random changes in financial markets have motivated the extensive use of stochastic processes in finance.[16][17][18]\\n'}, {'link': '/wiki/Multivariate_normal_distribution', 'title': 'Multivariate normal distribution', 'paragraph': 'In probability theory and statistics, the multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions.  One definition is that a random vector is said to be k-variate normally distributed if every linear combination of its k components has a univariate normal distribution. Its importance derives mainly from the multivariate central limit theorem. The multivariate normal distribution is often used to describe, at least approximately, any set of (possibly) correlated real-valued random variables each of which clusters around a mean value.\\n'}, {'link': '/wiki/Covariance_function', 'title': 'Covariance function', 'paragraph': 'In probability theory and statistics, the covariance function describes how much two random variables change together (their covariance) with varying spatial or temporal separation. For a random field or stochastic process Z(x) on a domain D, a covariance function C(x,\\xa0y) gives the covariance of the values of the random field at the two locations x and y:\\n'}, {'link': '/wiki/Bayesian_optimization', 'title': 'Bayesian optimization', 'paragraph': 'Bayesian optimization is a sequential design strategy for global optimization of black-box functions[1][2][3]  that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions.\\n'}, {'link': '/wiki/Hyperparameter_optimization', 'title': 'Hyperparameter optimization', 'paragraph': 'In machine learning, hyperparameter optimization[1] or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\\n'}, {'link': '/wiki/Search_algorithm', 'title': 'Search algorithm', 'paragraph': 'In computer science, a search algorithm is an algorithm designed to solve a search problem. Search algorithms work to retrieve information stored within particular data structure, or calculated in the search space of a problem domain, with either discrete or continuous values.\\n'}, {'link': '/wiki/Heuristic_(computer_science)', 'title': 'Heuristic (computer science)', 'paragraph': 'In mathematical optimization and computer science, heuristic (from Greek εὑρίσκω \"I find, discover\") is a technique designed for problem solving more quickly when classic methods are too slow for finding an exact or approximate solution, or when classic methods fail to find any exact solution in a search space.  This is achieved by trading optimality, completeness, accuracy, or precision for speed.  In a way, it can be considered a shortcut.\\n'}, {'link': '/wiki/Mutation_(genetic_algorithm)', 'title': 'Mutation (genetic algorithm)', 'paragraph': 'Mutation is a genetic operator used to maintain genetic diversity of the chromosomes of a population of a genetic or, more generally, an evolutionary algorithm (EA). It is analogous to biological mutation.\\n'}, {'link': '/wiki/Crossover_(genetic_algorithm)', 'title': 'Crossover (genetic algorithm)', 'paragraph': 'In genetic algorithms and evolutionary computation, crossover, also called recombination, is a genetic operator used to combine the genetic information of two parents to generate new offspring. It is one way to stochastically generate new solutions from an existing population, and is analogous to the crossover that happens during sexual reproduction in biology. Solutions can also be generated by cloning an existing solution, which is analogous to asexual reproduction. Newly generated solutions may be mutated before being added to the population.\\n'}, {'link': '/wiki/Chromosome_(genetic_algorithm)', 'title': 'Chromosome (genetic algorithm)', 'paragraph': 'In genetic algorithms (GA), or more general, evolutionary algorithms (EA), a chromosome (also sometimes called a genotype) is a set of parameters which define a proposed solution of the problem that the evolutionary algorithm is trying to solve. The set of all solutions, also called individuals according to the biological model, is known as the population.[1][2] The genome of an individual consists of one, more rarely of several,[3][4] chromosomes and corresponds to the genetic representation of the task to be solved. A chromosome is composed of a set of genes, where a gene consists of one or more semantically connected parameters, which are often also called decision variables. They determine one or more phenotypic characteristics of the individual or at least have an influence on them.[2]  In the basic form of genetic algorithms, the chromosome is represented as a binary string,[5] while in later variants[6][7] and in EAs in general, a wide variety of other data structures are used.[8][9][10]\\n'}, {'link': '/wiki/Dempster%E2%80%93Shafer_theory', 'title': 'Dempster–Shafer theory', 'paragraph': 'The theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory (DST), is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. First introduced by Arthur P. Dempster[1] in the context of statistical inference, the theory was later developed by Glenn Shafer into a general framework for modeling epistemic uncertainty—a mathematical theory of evidence.[2][3] The theory allows one to combine evidence from different sources and arrive at a degree of belief (represented by a mathematical object called belief function) that takes into account all the available evidence.\\n'}, {'link': '/wiki/Possibility_theory', 'title': 'Possibility theory', 'paragraph': 'Possibility theory is a mathematical theory for dealing with certain types of uncertainty and is an alternative to probability theory. It uses measures of possibility and necessity between 0 and 1, ranging from impossible to possible and unnecessary to necessary, respectively. Professor Lotfi Zadeh first introduced possibility theory in 1978 as an extension of his theory of fuzzy sets and fuzzy logic. Didier Dubois and Henri Prade further contributed to its development. Earlier, in the 1950s, economist G. L. S. Shackle proposed the min/max algebra to describe degrees of potential surprise.\\n'}, {'link': '/wiki/Imprecise_probability', 'title': 'Imprecise probability', 'paragraph': 'Imprecise probability generalizes probability theory to allow for partial probability specifications, and is applicable when information is scarce, vague, or conflicting, in which case a unique probability distribution may be hard to identify. Thereby, the theory aims to represent the available knowledge more accurately.  Imprecision is useful for dealing with expert elicitation, because:\\n'}, {'link': '/wiki/Uncertainty_quantification', 'title': 'Uncertainty quantification', 'paragraph': 'Uncertainty quantification (UQ) is the science of quantitative characterization and estimation of uncertainties in both computational and real world applications. It tries to determine how likely certain outcomes are if some aspects of the system are not exactly known. An example would be to predict the acceleration of a human body in a head-on crash with another car: even if the speed was exactly known, small differences in the manufacturing of individual cars, how tightly every bolt has been tightened, etc., will lead to different results that can only be predicted in a statistical sense.\\n'}, {'link': '/wiki/Sensor', 'title': 'Sensor', 'paragraph': 'A sensor is a device that produces an output signal for the purpose of sensing a physical phenomenon.\\n'}, {'link': '/wiki/Federated_learning', 'title': 'Federated learning', 'paragraph': 'Federated learning (also known as collaborative learning) is a machine learning technique that trains an algorithm via multiple independent sessions, each using its own dataset. This approach stands in contrast to traditional centralized machine learning techniques where local datasets are merged into one training session, as well as to approaches that assume that local data samples are identically distributed.\\n'}, {'link': '/wiki/Distributed_artificial_intelligence', 'title': 'Distributed artificial intelligence', 'paragraph': 'Distributed Artificial Intelligence (DAI) also called Decentralized Artificial Intelligence[1] is a subfield of artificial intelligence research dedicated to the development of distributed solutions for problems. DAI is closely related to and a predecessor of the field of multi-agent systems. \\n'}, {'link': '/wiki/Computational_anatomy', 'title': 'Computational anatomy', 'paragraph': 'Computational anatomy is an interdisciplinary field of biology focused on quantitative investigation and modelling of anatomical shapes variability.[1][2] It involves the development and application of mathematical, statistical and data-analytical methods for modelling and simulation of biological structures.\\n'}, {'link': '/wiki/Adaptive_website', 'title': 'Adaptive website', 'paragraph': \"An adaptive website is a website that builds a model of user activity and modifies the information and/or presentation of information to the user in order to better address the user's needs.[1]\\n\"}, {'link': '/wiki/Affective_computing', 'title': 'Affective computing', 'paragraph': \"Affective computing is the study and development of systems and devices that can recognize, interpret, process, and simulate human affects. It is an interdisciplinary field spanning computer science, psychology, and cognitive science.[1] While some core ideas in the field may be traced as far back as to early philosophical inquiries into emotion,[2] the more modern branch of computer science originated with Rosalind Picard's 1995 paper[3] on affective computing and her book Affective Computing[4] published by MIT Press.[5][6] One of the motivations for the research is the ability to give machines emotional intelligence, including to simulate empathy. The machine should interpret the emotional state of humans and adapt its behavior to them, giving an appropriate response to those emotions.\\n\"}, {'link': '/wiki/Astroinformatics', 'title': 'Astroinformatics', 'paragraph': 'Astroinformatics is an interdisciplinary field of study involving the combination of astronomy, data science, machine learning, informatics, and information/communications technologies.[2][3] The field is closely related to astrostatistics.\\n'}, {'link': '/wiki/Automated_decision-making', 'title': 'Automated decision-making', 'paragraph': 'Automated decision-making (ADM) involves the use of data, machines and algorithms to make decisions in a range of contexts, including public administration, business, health, education, law, employment, transport, media and entertainment, with varying degrees of human oversight or intervention. ADM involves large-scale data from a range of sources, such as databases, text, social media, sensors, images or speech, that is processed using various technologies including computer software, algorithms, machine learning, natural language processing, artificial intelligence, augmented intelligence and robotics. The increasing use of automated decision-making systems (ADMS) across a range of contexts presents many benefits and challenges to human society requiring consideration of the technical, legal, ethical, societal, educational, economic and health consequences.[1][2][3]\\n'}, {'link': '/wiki/Behaviorism', 'title': 'Behaviorism', 'paragraph': \"\\nBehaviorism (also spelled behaviourism)[1] is a systematic approach to understanding the behavior of humans and other animals.[2] It assumes that behavior is either a reflex evoked by the pairing of certain antecedent stimuli in the environment, or a consequence of that individual's history, including especially reinforcement and punishment contingencies, together with the individual's current motivational state and controlling stimuli. Although behaviorists generally accept the important role of heredity in determining behavior, they focus primarily on environmental events.\\n\"}, {'link': '/wiki/Cheminformatics', 'title': 'Cheminformatics', 'paragraph': 'Cheminformatics (also known as chemoinformatics) refers to the use of physical chemistry theory with computer and information science techniques—so called \"in silico\" techniques—in application to a range of descriptive and prescriptive problems in the field of chemistry, including in its applications to biology and related molecular fields. Such in silico techniques are used, for example, by pharmaceutical companies and in academic settings to aid and inform the process of drug discovery, for instance in the design of well-defined combinatorial libraries of synthetic compounds, or to assist in structure-based drug design. The methods can also be used in chemical and allied industries, and such fields as environmental science and pharmacology, where chemical processes are involved or studied.[1]\\n'}, {'link': '/wiki/Climate_Science', 'title': 'Climatology', 'paragraph': 'Climatology (from Greek κλίμα, klima, \"slope\"; and -λογία, -logia) or climate science is the scientific study of Earth\\'s climate, typically defined as weather conditions averaged over a period of at least 30 years.[1] Climate concerns the atmospheric condition during an extended to indefinite period of time; weather is the condition of the atmosphere during a relative brief period of time. The main topics of research are the study of climate variability, mechanisms of climate changes and modern climate change.[2][3] This topic of study is regarded as part of the atmospheric sciences and a subdivision of physical geography, which is one of the Earth sciences. Climatology includes some aspects of oceanography and biogeochemistry.\\n'}, {'link': '/wiki/Data_quality', 'title': 'Data quality', 'paragraph': 'Data quality refers to the state of qualitative or quantitative pieces of information. There are many definitions of data quality, but data is generally considered high quality if it is \"fit for [its] intended uses in operations, decision making and planning\".[1][2][3] Moreover, data is deemed of high quality if it correctly represents the real-world construct to which it refers. Furthermore, apart from these definitions, as the number of data sources increases, the question of internal data consistency becomes significant, regardless of fitness for use for any particular external purpose. People\\'s views on data quality can often be in disagreement, even when discussing the same set of data used for the same purpose. When this is the case, data governance is used to form agreed upon definitions and standards for data quality. In such cases, data cleansing, including standardization, may be required in order to ensure data quality.[4]\\n'}, {'link': '/wiki/Computational_economics', 'title': 'Computational economics', 'paragraph': 'Computational economics is an interdisciplinary research discipline that involves computer science, economics, and management science.[1]  This subject encompasses computational modeling of economic systems. Some of these areas are unique, while others established areas of economics by allowing robust data analytics and solutions of problems that would be arduous to research without computers and associated numerical methods.[2]\\n'}, {'link': '/wiki/Financial_market', 'title': 'Financial market', 'paragraph': 'A financial market is a market in which people trade financial securities and derivatives at low transaction costs. Some of the securities include stocks and bonds, raw materials and precious metals, which are known in the financial markets as commodities.\\n'}, {'link': '/wiki/Internet_fraud', 'title': 'Internet fraud', 'paragraph': 'Internet fraud is a type of cybercrime fraud or deception which makes use of the Internet and could involve hiding of information or providing incorrect information for the purpose of tricking victims out of money, property, and inheritance.[1] Internet fraud is not considered a single, distinctive crime but covers a range of illegal and illicit actions that are committed in cyberspace.[1] It is, however, differentiated from theft since, in this case, the victim voluntarily and knowingly provides the information, money or property to the perpetrator.[2] It is also distinguished by the way it involves temporally and spatially separated offenders.[3]\\n'}, {'link': '/wiki/Knowledge_graph_embedding', 'title': 'Knowledge graph embedding', 'paragraph': \"In representation learning, knowledge graph embedding (KGE), also referred to as knowledge representation learning (KRL), or multi-relation learning,[1] is a machine learning task of learning a low-dimensional representation of a knowledge graph's entities and relations while preserving their semantic meaning.[1][2][3]  Leveraging their embedded representation, knowledge graphs (KGs) can be used for various applications such as link prediction, triple classification, entity recognition, clustering, and relation extraction.[1][4]\\n\"}, {'link': '/wiki/Computational_linguistics', 'title': 'Computational linguistics', 'paragraph': 'Computational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon linguistics, computer science, artificial intelligence, mathematics, logic, philosophy, cognitive science, cognitive psychology, psycholinguistics, anthropology and neuroscience, among others.\\n'}, {'link': '/wiki/Machine_learning_control', 'title': 'Machine learning control', 'paragraph': 'Machine learning control (MLC) is a subfield of machine learning, intelligent control and control theory\\nwhich solves optimal control problems with methods of machine learning.\\nKey applications are complex nonlinear systems\\nfor which linear control theory methods are not applicable.\\n'}, {'link': '/wiki/Machine_perception', 'title': 'Machine perception', 'paragraph': 'Machine perception is the capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them.[1][2][3] The basic method that the computers take in and respond to their environment is through the attached hardware.  Until recently input was limited to a keyboard, or a mouse, but advances in technology, both in hardware and software, have allowed computers to take in sensory input in a way similar to humans.[1][2]\\n'}, {'link': '/wiki/Natural-language_understanding', 'title': 'Natural-language understanding', 'paragraph': 'Natural-language understanding (NLU) or natural-language interpretation (NLI)[1] is a subtopic  of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem.[2]\\n'}, {'link': '/wiki/Robot_locomotion', 'title': 'Robot locomotion', 'paragraph': 'Robot locomotion is the collective name for the various methods that robots use to transport themselves from place to place.\\n'}, {'link': '/wiki/Sentiment_analysis', 'title': 'Sentiment analysis', 'paragraph': '\\nSentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. With the rise of deep language models, such as RoBERTa, also more difficult data domains can be analyzed, e.g., news texts where authors typically express their opinion/sentiment less explicitly.[1]\\n'}, {'link': '/wiki/Software_engineering', 'title': 'Software engineering', 'paragraph': 'Software engineering is an engineering-based approach to software development.[1][2][3]\\nA software engineer is a person who applies the engineering design process to design, develop, test, maintain, and evaluate computer software. The term programmer is sometimes used as a synonym, but may emphasize software implementation over design and can also lack connotations of engineering education or skills.[4]\\n'}, {'link': '/wiki/Structural_health_monitoring', 'title': 'Structural health monitoring', 'paragraph': 'Structural health monitoring (SHM) involves the observation and analysis of a system over time using periodically sampled response measurements to monitor changes to the material and geometric properties of engineering structures such as bridges and buildings.\\n'}, {'link': '/wiki/Syntactic_pattern_recognition', 'title': 'Syntactic pattern recognition', 'paragraph': 'Syntactic pattern recognition or structural pattern recognition is a form of pattern recognition, in which each object can be represented by a variable-cardinality set of symbolic, nominal features. This allows for representing pattern structures, taking into account more complex interrelationships between attributes than is possible in the case of flat, numerical feature vectors of fixed dimensionality, that are used in statistical classification.\\n'}, {'link': '/wiki/Automated_theorem_proving', 'title': 'Automated theorem proving', 'paragraph': 'Automated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs. Automated reasoning over mathematical proof was a major impetus for the development of computer science.\\n'}, {'link': '/wiki/Tomographic_reconstruction', 'title': 'Tomographic reconstruction', 'paragraph': 'Tomographic reconstruction is a type of multidimensional inverse problem where the challenge is to yield an estimate of a specific system from a finite number of projections. The mathematical basis for tomographic imaging was laid down by Johann Radon. A notable example of applications is the reconstruction of computed tomography (CT) where cross-sectional images of patients are obtained in non-invasive manner. Recent developments have seen the Radon transform and its inverse used for tasks related to realistic object insertion required for testing and evaluating computed tomography use in airport security.[1]\\n'}, {'link': '/wiki/User_behavior_analytics', 'title': 'User behavior analytics', 'paragraph': \"User behavior analytics (UBA) or user and entity behavior analytics (UEBA),[1] is the concept of analyzing the behavior of users, subjects, visitors, etc. for a specific purpose.[2] It allows cybersecurity tools to build a profile of each individual's normal activity, by looking at patterns of human behavior, and then highlighting deviations from that profile (or anomalies) that may indicate a potential compromise.[3][4][5]\\n\"}, {'link': '/wiki/Netflix_Prize', 'title': 'Netflix Prize', 'paragraph': 'The Netflix Prize was an open competition for the best collaborative filtering algorithm to predict user ratings for films, based on previous ratings without any other information about the users or films, i.e. without the users being identified except by numbers assigned for the contest.\\n'}, {'link': '/wiki/Ensemble_Averaging', 'title': 'Ensemble averaging (machine learning)', 'paragraph': 'In machine learning, particularly in the creation of artificial neural networks, ensemble averaging is the process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models \"average out.\"\\n'}, {'link': '/wiki/Springer_Nature', 'title': 'Springer Nature', 'paragraph': \"Springer Nature or the Springer Nature Group[1][2] is a German-British academic publishing company created by the May 2015 merger of Springer Science+Business Media and Holtzbrinck Publishing Group's Nature Publishing Group, Palgrave Macmillan, and Macmillan Education.[3]\\n\"}, {'link': '/wiki/Watson_(computer)', 'title': 'IBM Watson', 'paragraph': \"IBM Watson is a computer system capable of answering questions posed in natural language.[1] It was developed in IBM's DeepQA project by a research team led by principal investigator David Ferrucci.[2] Watson was named after IBM's founder and first CEO, industrialist Thomas J. Watson.[3][4]\\n\"}, {'link': '/wiki/Fairness_(machine_learning)', 'title': 'Fairness (machine learning)', 'paragraph': \"Fairness in machine learning refers to the various attempts at correcting algorithmic bias in automated decision processes based on machine learning models. Decisions made by computers after a machine-learning process may be considered unfair if they were based on variables considered sensitive. Examples of these kinds of variable include gender, ethnicity, sexual orientation, disability and more. As it is the case with many ethical concepts, definitions of fairness and bias are always controversial. In general, fairness and bias are considered relevant when the decision process impacts people's lives. In machine learning, the problem of algorithmic bias is well known and well studied. Outcomes may be skewed by a range of factors and thus might be considered unfair with respect to certain groups or individuals. An example would be the way social media sites deliver personalized news to consumers.\\n\"}, {'link': '/wiki/Explainable_artificial_intelligence', 'title': 'Explainable artificial intelligence', 'paragraph': 'Explainable AI (XAI), often overlapping with Interpretable AI, or Explainable Machine Learning (XML), either refers to an AI system over which it is possible for humans to retain intellectual oversight, or to the methods to achieve this.[1] The main focus is usually on the reasoning behind the decisions or predictions made by the AI[2] which are made more understandable and transparent.[3] XAI counters the \"black box\" tendency of machine learning, where even the AI\\'s designers cannot explain why it arrived at a specific decision.[4][5]\\n'}, {'link': '/wiki/File:Overfitted_Data.png', 'title': 'File:Overfitted Data.png', 'paragraph': 'Overfitted_Data.png \\u200e(377 × 256 pixels, file size: 14 KB, MIME type: image/png)\\n'}, {'link': '/wiki/Adversarial_machine_learning', 'title': 'Adversarial machine learning', 'paragraph': 'Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks.[1] A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.[2]\\n'}, {'link': '/wiki/Backdoor_(computing)', 'title': 'Backdoor (computing)', 'paragraph': 'A backdoor is a typically covert method of bypassing normal authentication or encryption in a computer, product, embedded device (e.g. a home router), or its embodiment (e.g. part of a cryptosystem, algorithm, chipset, or even a \"homunculus computer\"—a tiny computer-within-a-computer such as that found in Intel\\'s AMT technology).[1][2] Backdoors are most often used for securing remote access to a computer, or obtaining access to plaintext in cryptosystems. From there it may be used to gain access to privileged information like passwords, corrupt or delete data on hard drives, or transfer information within autoschediastic networks.\\n'}, {'link': '/wiki/Algorithmic_transparency', 'title': 'Algorithmic transparency', 'paragraph': 'Algorithmic transparency is the principle that the factors that influence the decisions made by algorithms should be visible, or transparent, to the people who use, regulate, and are affected by systems that employ those algorithms. Although the phrase was coined in 2016 by Nicholas Diakopoulos and Michael Koliska about the role of algorithms in deciding the content of digital journalism services,[1] the underlying principle dates back to the 1970s and the rise of automated systems for scoring consumer credit.\\n'}, {'link': '/wiki/White-box_testing', 'title': 'White-box testing', 'paragraph': 'White-box testing (also known as clear box testing, glass box testing, transparent box testing, and structural testing) is a method of software testing that tests internal structures or workings of an application, as opposed to its functionality (i.e. black-box testing). In white-box testing, an internal perspective of the system is used to design test cases. The tester chooses inputs to exercise paths through the code and determine the expected outputs. This is analogous to testing nodes in a circuit, e.g. in-circuit testing (ICT).\\nWhite-box testing can be applied at the unit, integration and system levels of the software testing process. Although traditional testers tended to think of white-box testing as being done at the unit level, it is used for integration and system testing more frequently today. It can test paths within a unit, paths between units during integration, and between subsystems during a system–level test. Though this method of test design can uncover many errors or problems, it has the potential to miss unimplemented parts of the specification or missing requirements. Where white-box testing is design-driven,[1] that is, driven exclusively by agreed specifications of how each component of software is required to behave (as in DO-178C and ISO 26262 processes), white-box test techniques can accomplish assessment for unimplemented or missing requirements.\\n'}, {'link': '/wiki/Cross-validation_(statistics)', 'title': 'Cross-validation (statistics)', 'paragraph': \"Cross-validation,[2][3][4] sometimes called rotation estimation[5][6][7] or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.\\nCross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.  In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set).[8][9] The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias[10] and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).\\n\"}, {'link': '/wiki/Bootstrapping_(statistics)', 'title': 'Bootstrapping (statistics)', 'paragraph': 'Bootstrapping is any test or metric that uses random sampling with replacement (e.g. mimicking the sampling process), and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates.[1][2] This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.[3][4]\\n'}, {'link': '/wiki/Sensitivity_and_specificity', 'title': 'Sensitivity and specificity', 'paragraph': 'In medicine and statistics, sensitivity and specificity mathematically describe the accuracy of a test that reports the presence or absence of a medical condition. If individuals who have the condition are considered \"positive\" and those who do not are considered \"negative\", then sensitivity is a measure of how well a test can identify true positives and specificity is a measure of how well a test can identify true negatives:\\n'}, {'link': '/wiki/False_positive_rate', 'title': 'False positive rate', 'paragraph': 'In statistics, when performing multiple comparisons, a false positive ratio (also known as fall-out or false alarm ratio) is the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positives) and the total number of actual negative events (regardless of classification).\\n'}, {'link': '/wiki/False_negative_rate', 'title': 'False positives and false negatives', 'paragraph': 'A false positive is an error in binary classification in which a test result incorrectly indicates the presence of a condition (such as a disease when the disease is not present), while a false negative is the opposite error, where the test result incorrectly indicates the absence of a condition when it is actually present. These are the two kinds of errors in a binary test, in contrast to the two kinds of correct result (a true positive and a true negative). They are also known in medicine as a false positive (or false negative) diagnosis, and in statistical classification as a false positive (or false negative) error.[1]\\n'}, {'link': '/wiki/Total_operating_characteristic', 'title': 'Total operating characteristic', 'paragraph': 'The total operating characteristic (TOC)  is a statistical method to compare a Boolean variable versus a rank variable. TOC can measure the ability of an index variable to diagnose either presence or absence of a characteristic. The diagnosis of presence or absence depends on whether the value of the index is above a threshold. TOC considers multiple possible thresholds. Each threshold generates a two-by-two contingency table, which contains four entries: hits, misses, false alarms, and correct rejections.[1]\\n'}, {'link': '/wiki/Receiver_operating_characteristic', 'title': 'Receiver operating characteristic', 'paragraph': 'Sources: Fawcett (2006),[1] Piryonesi and El-Diraby (2020),[2]\\nPowers (2011),[3] Ting (2011),[4] CAWCR,[5] D. Chicco & G. Jurman (2020, 2021, 2023),[6][7][8]  Tharwat (2018).[9] Balayla (2020)[10]\\n'}, {'link': '/wiki/Toronto_Declaration', 'title': 'Toronto Declaration', 'paragraph': 'The Toronto Declaration: Protecting the Rights to Equality and Non-Discrimination in Machine Learning Systems is a declaration that advocates responsible practices for machine learning practitioners and governing bodies. It is a joint statement issued by groups including Amnesty International and Access Now, with other notable signatories including Human Rights Watch and The Wikimedia Foundation.[1] It was published at RightsCon on May 16, 2018.[2][3]\\n'}, {'link': '/wiki/Machine_ethics', 'title': 'Machine ethics', 'paragraph': 'Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence, otherwise known as artificial intelligent agents.[1] Machine ethics differs from other ethical fields related to engineering and technology.  Machine ethics should not be confused with computer ethics, which focuses on human use of computers. It should also be distinguished from the philosophy of technology, which concerns itself with the grander social effects of technology.[2]\\n'}, {'link': '/wiki/Data_collection', 'title': 'Data collection', 'paragraph': 'Data collection or data gathering is the process of gathering and measuring information on targeted variables in an established system, which then enables one to answer relevant questions and evaluate outcomes. Data collection is a research component in all study fields, including physical and social sciences, humanities,[2] and business. While methods vary by discipline, the emphasis on ensuring accurate and honest collection remains the same. The goal for all data collection is to capture evidence that allows data analysis to lead to the formulation of credible answers to the questions that have been posed.\\n'}, {'link': '/wiki/GPU', 'title': 'Graphics processing unit', 'paragraph': 'A graphics processing unit (GPU) is a specialized electronic circuit initially designed to accelerate computer graphics and image processing (either on a video card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles). After their initial design, GPUs were found to be useful for non-graphic calculations involving embarrassingly parallel problems due to their parallel structure. Other non-graphical uses include the training of neural networks and cryptocurrency mining.\\n'}, {'link': '/wiki/Physical_neural_network', 'title': 'Physical neural network', 'paragraph': 'A physical neural network is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse or a higher-order (dendritic) neuron model.[1] \"Physical\" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches. More generally the term is applicable to other artificial neural networks in which a memristor or other electrically adjustable resistance material is used to emulate a neural synapse.[2][3]\\n'}, {'link': '/wiki/Chemical_synapse', 'title': 'Chemical synapse', 'paragraph': \"Chemical synapses are biological junctions through which neurons' signals can be sent to each other and to non-neuronal cells such as those in muscles or glands. Chemical synapses allow neurons to form circuits within the central nervous system. They are crucial to the biological computations that underlie perception and thought. They allow the nervous system to connect to and control other systems of the body.\\n\"}, {'link': '/wiki/Edge_device', 'title': 'Edge device', 'paragraph': 'In computer networking, an edge device is a device that provides an entry point into enterprise or service provider core networks. Examples include routers, routing switches, integrated access devices (IADs), multiplexers, and a variety of metropolitan area network (MAN) and wide area network (WAN) access devices.  Edge devices also provide connections into carrier and service provider networks. An edge device that connects a local area network to a high speed switch or backbone (such as an ATM switch) may be called an edge concentrator.\\n'}, {'link': '/wiki/Microcontrollers', 'title': 'Microcontroller', 'paragraph': 'A microcontroller (MC, UC, or μC) or microcontroller unit (MCU) is a small computer on a single integrated circuit. A microcontroller contains one or more CPUs (processor cores) along with memory and programmable input/output peripherals. Program memory in the form of ferroelectric RAM, NOR flash or OTP ROM is also often included on chip, as well as a small amount of RAM. Microcontrollers are designed for embedded applications, in contrast to the microprocessors used in personal computers or other general purpose applications consisting of various discrete chips.\\n'}, {'link': '/wiki/Hardware_acceleration', 'title': 'Hardware acceleration', 'paragraph': 'Hardware acceleration is the use of computer hardware designed to perform specific functions more efficiently when compared to software running on a general-purpose central processing unit (CPU). Any transformation of data that can be calculated in software running on a generic CPU can also be calculated in custom-made hardware, or in some mix of both.\\n'}, {'link': '/wiki/Approximate_computing', 'title': 'Approximate computing', 'paragraph': 'Approximate computing is an emerging paradigm for energy-efficient and/or high-performance design.[1] It includes a plethora of computation techniques that return a possibly inaccurate result rather than a guaranteed accurate result, and that can be used for applications where an approximate result is sufficient for its purpose.[2] One example of such situation is for a search engine where no exact answer may exist for a certain search query and hence, many answers may be acceptable. Similarly, occasional dropping of some frames in a video application can go undetected due to perceptual limitations of humans. Approximate computing is based on the observation that in many scenarios, although performing exact computation requires large amount of resources, allowing bounded approximation can provide disproportionate gains in performance and energy, while still achieving acceptable result accuracy.[clarification needed]  For example, in k-means clustering algorithm, allowing only 5% loss in classification accuracy can provide 50 times energy saving compared to the fully accurate classification.\\n'}, {'link': '/wiki/Software_suite', 'title': 'Software suite', 'paragraph': 'A software suite[1] (also known as an application suite) is a collection of computer programs (application software, or programming software) of related functionality, sharing a similar user interface and the ability to easily exchange data with each other.\\n'}, {'link': '/wiki/Caffe_(software)', 'title': 'Caffe (software)', 'paragraph': 'Caffe (Convolutional Architecture for Fast Feature Embedding) is a deep learning framework, originally developed at University of California, Berkeley. It is open source, under a BSD license.[4] It is written in C++, with a Python interface.[5]\\n'}, {'link': '/wiki/Deeplearning4j', 'title': 'Deeplearning4j', 'paragraph': 'Eclipse Deeplearning4j is a programming library written in Java for the Java virtual machine (JVM).[2][3] It is a framework with wide support for deep learning algorithms.[4] Deeplearning4j includes implementations of the restricted Boltzmann machine, deep belief net, deep autoencoder, stacked denoising autoencoder and recursive neural tensor network, word2vec, doc2vec, and GloVe. These algorithms all include distributed parallel versions that integrate with Apache Hadoop and Spark.[5]\\n'}, {'link': '/wiki/DeepSpeed', 'title': 'DeepSpeed', 'paragraph': 'DeepSpeed is an open source deep learning optimization library for PyTorch.[1] The library is designed to reduce computing power and memory use and to train large distributed models with better parallelism on existing computer hardware.[2][3] DeepSpeed is optimized for low latency, high throughput training. It includes the Zero Redundancy Optimizer (ZeRO) for training models with 1 trillion or more parameters.[4] Features include mixed precision training, single-GPU, multi-GPU, and multi-node training as well as custom model parallelism. The DeepSpeed source code is licensed under MIT License and available on GitHub.[5]\\n'}, {'link': '/wiki/ELKI', 'title': 'ELKI', 'paragraph': 'ELKI (Environment for Developing KDD-Applications Supported by Index-Structures) is a data mining (KDD, knowledge discovery in databases) software framework developed for use in research and teaching. It was originally at the database systems research unit of Professor Hans-Peter Kriegel at the Ludwig Maximilian University of Munich, Germany, and now continued at the Technical University of Dortmund, Germany. It aims at allowing the development and evaluation of advanced data mining algorithms and their interaction with database index structures.\\n'}, {'link': '/wiki/Google_JAX', 'title': 'Google JAX', 'paragraph': \"Google JAX is a machine learning framework for transforming numerical functions.[1][2][3] It is described as bringing together a modified version of autograd (automatic obtaining of the gradient function through differentiation of a function) and TensorFlow's XLA (Accelerated Linear Algebra). It is designed to follow the structure and workflow of NumPy as closely as possible and works with various existing frameworks such as TensorFlow and PyTorch.[4][5] The primary functions of JAX are:[1]\\n\"}, {'link': '/wiki/Infer.NET', 'title': 'Infer.NET', 'paragraph': 'Infer.NET is a free and open source .NET software library for machine learning.[2] It supports running Bayesian inference in graphical models and can also be used for probabilistic programming.[3]\\n'}, {'link': '/wiki/Keras', 'title': 'Keras', 'paragraph': 'Keras is an open-source library that provides a Python interface for artificial neural networks. Keras acts as an interface for the TensorFlow library.[citation needed]\\n'}, {'link': '/wiki/Kubeflow', 'title': 'Kubeflow', 'paragraph': 'Kubeflow is an open-source platform for machine learning and MLOps on Kubernetes introduced by Google. The different stages in a typical machine learning lifecycle are represented with different software components in Kubeflow, including model development (Kubeflow Notebooks[4]), model training (Kubeflow Pipelines,[5] Kubeflow Training Operator[6]), model serving (KServe[a][7]), and automated machine learning (Katib[8]).\\n'}, {'link': '/wiki/LightGBM', 'title': 'LightGBM', 'paragraph': 'LightGBM, short for light gradient-boosting machine, is a free and open-source distributed gradient-boosting framework for machine learning, originally developed by Microsoft.[4][5] It is based on decision tree algorithms and used for ranking, classification and other machine learning tasks. The development focus is on performance and scalability.\\n'}, {'link': '/wiki/Apache_Mahout', 'title': 'Apache Mahout', 'paragraph': 'Apache Mahout is a project of the Apache Software Foundation to produce free implementations of distributed or otherwise scalable machine learning algorithms focused primarily on linear algebra. In the past, many of the implementations use the Apache Hadoop platform, however today it is primarily focused on Apache Spark.[3][4] Mahout also provides Java/Scala libraries for common math operations (focused on linear algebra and statistics) and primitive Java collections. Mahout is a work in progress; a number of algorithms have been implemented.[5]\\n'}, {'link': '/wiki/Mallet_(software_project)', 'title': 'Mallet (software project)', 'paragraph': 'MALLET is a Java \"Machine Learning for Language Toolkit\".\\n'}, {'link': '/wiki/Microsoft_Cognitive_Toolkit', 'title': 'Microsoft Cognitive Toolkit', 'paragraph': 'Microsoft Cognitive Toolkit,[3] previously known as CNTK and sometimes styled as The Microsoft Cognitive Toolkit, is a deprecated[4] deep learning framework developed by Microsoft Research. Microsoft Cognitive Toolkit describes neural networks as a series of computational steps via a directed graph.\\n'}, {'link': '/wiki/ML.NET', 'title': 'ML.NET', 'paragraph': 'ML.NET is a free software machine learning library for the C# and F# programming languages.[4][5][6] It also supports Python models when used together with NimbusML. The preview release of ML.NET included transforms for feature engineering like n-gram creation, and learners to handle binary classification, multi-class classification, and regression tasks.[7] Additional ML tasks like anomaly detection and recommendation systems have since been added, and other approaches like deep learning will be included in future versions.[8][9]\\n'}, {'link': '/wiki/Mlpack', 'title': 'mlpack', 'paragraph': 'mlpack is a machine learning software library for C++, built on top of the Armadillo library and the ensmallen numerical optimization library.[3] mlpack has an emphasis on scalability, speed, and ease-of-use. Its aim is to make machine learning possible for novice users by means of a simple, consistent API, while simultaneously exploiting C++ language features to provide maximum performance and maximum flexibility for expert users.[4] Its intended target users are scientists and engineers.\\n'}, {'link': '/wiki/MXNet', 'title': 'Apache MXNet', 'paragraph': 'Apache MXNet is an open-source deep learning software framework that trains and deploys deep neural networks. It is scalable, allows fast model training, and supports a flexible programming model and multiple programming languages (including C++, Python, Java, Julia, MATLAB, JavaScript, Go, R, Scala, Perl, and Wolfram Language). The MXNet library is portable and can scale to multiple GPUs[2] and machines. It was co-developed by Carlos Guestrin at the University of Washington (along with GraphLab).[3]\\n'}, {'link': '/wiki/OpenNN', 'title': 'OpenNN', 'paragraph': 'OpenNN (Open Neural Networks Library) is a software library written in the C++ programming language which implements neural networks, a main area of deep learning research.[1] The library is open-source, licensed under the GNU Lesser General Public License.\\n'}, {'link': '/wiki/Orange_(software)', 'title': 'Orange (software)', 'paragraph': 'Orange is an open-source data visualization, machine learning and data mining toolkit. It features a visual programming front-end for explorative qualitative data analysis and interactive data visualization.\\n'}, {'link': '/wiki/ROOT', 'title': 'ROOT', 'paragraph': 'ROOT is an object-oriented computer program and library developed by CERN. It was originally designed for particle physics data analysis and contains several features specific to the field, but it is also used in other applications such as astronomy and data mining.  The latest minor release is 6.28, as of 2023-02-03.[3]\\n'}, {'link': '/wiki/Scikit-learn', 'title': 'scikit-learn', 'paragraph': 'scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language.[3]\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.[4]\\n'}, {'link': '/wiki/Shogun_(toolbox)', 'title': 'Shogun (toolbox)', 'paragraph': 'Shogun is a free, open-source machine learning software library  written in C++. It offers numerous algorithms and data structures for machine learning problems. It offers interfaces for Octave, Python, R, Java, Lua, Ruby and C# using SWIG.\\n'}, {'link': '/wiki/Apache_Spark#MLlib_Machine_Learning_Library', 'title': 'Apache Spark', 'paragraph': \"Apache Spark  is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.\\n\"}, {'link': '/wiki/Apache_SystemML', 'title': 'Apache SystemDS', 'paragraph': 'Apache SystemDS (Previously, Apache SystemML) is an open source ML system for the end-to-end data science lifecycle. \\n'}, {'link': '/wiki/Torch_(machine_learning)', 'title': 'Torch (machine learning)', 'paragraph': 'Torch is an open-source machine learning library, \\na scientific computing framework, and a scripting language based on Lua.[3] It provides LuaJIT interfaces to deep learning algorithms implemented in C. It was created at IDIAP at EPFL. Torch development moved in 2017 to PyTorch, a port of the library to Python.[4][5][better\\xa0source\\xa0needed]\\n'}, {'link': '/wiki/PyTorch', 'title': 'PyTorch', 'paragraph': 'PyTorch is a machine learning framework based on the Torch library,[4][5][6] used for applications such as computer vision and natural language processing,[7] originally developed by Meta AI and now part of the Linux Foundation umbrella.[8][9][10][11] It is free and open-source software released under the modified BSD license. Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface.[12]\\n'}, {'link': '/wiki/Weka_(machine_learning)', 'title': 'Weka (software)', 'paragraph': 'Waikato Environment for Knowledge Analysis (Weka) is a collection of machine learning and data analysis free software licensed under the GNU General Public License. It was developed at the University of Waikato, New Zealand and is the companion software to the book \"Data Mining: Practical Machine Learning Tools and Techniques\".[1]\\n'}, {'link': '/wiki/MOA_(Massive_Online_Analysis)', 'title': 'Massive Online Analysis', 'paragraph': 'Massive Online Analysis (MOA) is a free open-source software project specific for data stream mining with concept drift. It is written in Java and developed at the University of Waikato, New Zealand.[2]\\n'}, {'link': '/wiki/XGBoost', 'title': 'XGBoost', 'paragraph': 'XGBoost[2] (eXtreme Gradient Boosting) is an open-source software library which provides a regularizing gradient boosting framework for C++, Java, Python,[3] R,[4] Julia,[5] Perl,[6] and Scala. It works on Linux, Microsoft Windows,[7] and macOS.[8] From the project description, it aims to provide a \"Scalable, Portable and Distributed Gradient Boosting (GBM, GBRT, GBDT) Library\". It runs on a single machine, as well as the distributed processing frameworks Apache Hadoop, Apache Spark, Apache Flink, and Dask.[9][10]\\n'}, {'link': '/wiki/Yooreeka', 'title': 'Yooreeka', 'paragraph': 'Yooreeka is a library for data mining, machine learning, soft computing, and mathematical analysis. The project started with the code of the book \"Algorithms of the Intelligent Web\".[1] Although the term \"Web\" prevailed in the title, in essence, the algorithms are valuable in any software application.\\n'}, {'link': '/wiki/KNIME', 'title': 'KNIME', 'paragraph': 'KNIME (/naɪm/), the Konstanz Information Miner,[2] is a free and open-source data analytics, reporting and integration platform. KNIME integrates various components for machine learning and data mining through its modular data pipelining \"Building Blocks of Analytics\" concept. A graphical user interface and use of JDBC allows assembly of nodes blending different data sources, including preprocessing (ETL: Extraction, Transformation, Loading), for modeling, data analysis and visualization without, or with only minimal, programming.\\n'}, {'link': '/wiki/RapidMiner', 'title': 'RapidMiner', 'paragraph': \"RapidMiner is a data science platform that analyses the collective impact of an organization's data. It was acquired by Altair Engineering in September 2022.[1]\\n\"}, {'link': '/wiki/Angoss', 'title': 'Angoss', 'paragraph': \"Angoss Software Corporation, headquartered in Toronto, Ontario, Canada, with offices in the United States and UK, acquired by Datawatch and now owned by Altair, was a provider of predictive analytics systems through software licensing and services. Angoss' customers represent industries including finance, insurance, mutual funds, retail, health sciences, telecom and technology. The company was founded in 1984, and publicly traded on the TSX Venture Exchange from 2008-2013 under the ticker symbol ANC.[citation needed]\\n\"}, {'link': '/wiki/IBM_Watson_Studio', 'title': 'IBM Watson Studio', 'paragraph': 'Watson Studio, formerly Data Science Experience or DSX, is IBM’s software platform for data science. The platform consists of a workspace that includes multiple collaboration and open-source tools for use in data science.[1]\\n'}, {'link': '/wiki/SPSS_Modeler', 'title': 'SPSS Modeler', 'paragraph': 'IBM SPSS Modeler is a data mining and text analytics software application from IBM. It is used to build predictive models and conduct other analytic tasks. It has a visual interface which allows users to leverage statistical and data mining algorithms without programming.\\n'}, {'link': '/wiki/LIONsolver', 'title': 'LIONsolver', 'paragraph': 'LIONsolver is an integrated software for data mining, business intelligence, analytics, and modeling and reactive business intelligence approach.[1] A non-profit version is also available as LIONoso.\\n'}, {'link': '/wiki/Mathematica', 'title': 'Wolfram Mathematica', 'paragraph': 'Wolfram Mathematica is a software system with built-in libraries for several areas of technical computing that allow machine learning, statistics, symbolic computation, data manipulation, network analysis, time series analysis, NLP, optimization, plotting functions and various types of data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other programming languages. It was conceived by Stephen Wolfram, and is developed by Wolfram Research of Champaign, Illinois.[8][9] The Wolfram Language is the programming language used in Mathematica.[10] Mathematica 1.0 was released on June 23, 1988 in Champaign, Illinois and Santa Clara, California.[11][12][13]\\n'}, {'link': '/wiki/Neural_Designer', 'title': 'Neural Designer', 'paragraph': 'Neural Designer is a software tool for machine learning based on neural networks, a main area of artificial intelligence research, and contains a graphical user interface which simplifies data entry and interpretation of results.\\n'}, {'link': '/wiki/NeuroSolutions', 'title': 'NeuroSolutions', 'paragraph': 'NeuroSolutions is a neural network development environment developed by NeuroDimension. It combines a modular, icon-based (component-based) network design interface with an implementation of advanced learning procedures, such as conjugate gradients, the Levenberg-Marquardt algorithm, and backpropagation through time.[citation needed] The software is used to design, train and deploy neural network (supervised learning and unsupervised learning) models to perform a wide variety of tasks such as data mining, classification, function approximation, multivariate regression and time-series prediction.[citation needed]\\n'}, {'link': '/wiki/Oracle_Data_Mining', 'title': 'Oracle Data Mining', 'paragraph': 'Oracle Data Mining (ODM) is an option of Oracle Database Enterprise Edition. It contains several data mining and data analysis algorithms for classification, prediction, regression, associations, feature selection, anomaly detection, feature extraction, and specialized analytics. It provides means for the creation, management and operational deployment of data mining models inside the database environment.\\n'}, {'link': '/wiki/Oracle_Cloud#Platform_as_a_Service_(PaaS)', 'title': 'Oracle Cloud', 'paragraph': 'Oracle Cloud is a cloud computing\\xa0service offered by\\xa0Oracle Corporation\\xa0providing servers, storage, network, applications and services through a global network of Oracle Corporation managed\\xa0data centers. The company allows these services to be provisioned\\xa0on demand over the\\xa0Internet.\\n'}, {'link': '/wiki/RCASE', 'title': 'RCASE', 'paragraph': 'Root Cause Analysis Solver Engine (informally RCASE) is a proprietary algorithm developed from research originally at the Warwick Manufacturing Group (WMG) at Warwick University.[1][2] RCASE development commenced in 2003 to provide an automated version of root cause analysis, the method of problem solving that tries to identify the root causes of faults or problems.[3] RCASE is now owned by the spin-out company Warwick Analytics where it is being applied to automated predictive analytics software.\\n'}, {'link': '/wiki/SAS_(software)#Components', 'title': 'SAS (software)', 'paragraph': 'SAS (previously \"Statistical Analysis System\")[1] is a statistical software suite developed by SAS Institute for  data management, advanced analytics, multivariate analysis, business intelligence, criminal investigation,[2] and predictive analytics.\\n'}, {'link': '/wiki/SequenceL', 'title': 'SequenceL', 'paragraph': 'SequenceL is a general purpose functional programming language and auto-parallelizing (Parallel computing) compiler and tool set, whose primary design objectives are performance on multi-core processor hardware, ease of programming, platform portability/optimization, and code clarity and readability.  Its main advantage is that it can be used to write straightforward code that automatically takes full advantage of all the processing power available, without programmers needing to be concerned with identifying parallelisms, specifying vectorization, avoiding race conditions, and other challenges of manual directive-based programming approaches such as OpenMP.\\n'}, {'link': '/wiki/STATISTICA', 'title': 'Statistica', 'paragraph': 'Statistica is an advanced analytics software package originally developed by StatSoft and currently maintained by TIBCO Software Inc.[1]\\nStatistica provides data analysis, data management, statistics, data mining, machine learning, text analytics and data visualization procedures.\\n'}, {'link': '/wiki/Nature_Machine_Intelligence', 'title': 'Nature Machine Intelligence', 'paragraph': 'Nature Machine Intelligence is a monthly peer-reviewed scientific journal published by Nature Portfolio covering machine learning and artificial intelligence. The editor-in-chief is Liesbeth Venema.[1]\\n'}, {'link': '/wiki/Neural_Computation_(journal)', 'title': 'Neural Computation (journal)', 'paragraph': 'Neural Computation is a monthly peer-reviewed scientific journal covering all aspects of neural computation, including modeling the brain and the design and construction of neurally-inspired information processing systems. It was established in 1989 and is published by MIT Press. The editor-in-chief is Terrence J. Sejnowski (Salk Institute for Biological Studies).[1]\\n'}, {'link': '/wiki/IEEE_Transactions_on_Pattern_Analysis_and_Machine_Intelligence', 'title': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'paragraph': 'IEEE Transactions on Pattern Analysis and Machine Intelligence (sometimes abbreviated as IEEE PAMI or simply PAMI) is a monthly peer-reviewed scientific journal published by the IEEE Computer Society. \\n'}, {'link': '/wiki/AAAI_Conference_on_Artificial_Intelligence', 'title': 'AAAI Conference on Artificial Intelligence', 'paragraph': 'The AAAI Conference on Artificial Intelligence (AAAI) is one of the leading international academic conference in artificial intelligence held annually.[1][2][3] Along with ICML, NeurIPS and ICLR, it is one of the primary conferences of high impact in machine learning and artificial intelligence research.[4] It is supported by the Association for the Advancement of Artificial Intelligence. Precise dates vary from year to year, but paper submissions are generally due at the end of August to beginning of September, and the conference is generally held during the following February. The first AAAI was held in 1980 at Stanford University, Stanford California.[5]\\n'}, {'link': '/wiki/International_Conference_on_Intelligent_Robots_and_Systems', 'title': 'International Conference on Intelligent Robots and Systems', 'paragraph': \"IROS, the IEEE/RSJ International Conference on Intelligent Robots and Systems,[1] is an annual academic conference covering advances in robotics.[2] It is one of the premier conferences of its field (alongside ICRA, International Conference on Robotics and Automation) with an 'A' rating from the Australian Ranking of ICT Conferences obtained in 2010 and an 'A1' rating from the Brazilian ministry of education in 2012.[3][4]\\n\"}, {'link': '/wiki/Conference_on_Knowledge_Discovery_and_Data_Mining', 'title': 'Special Interest Group on Knowledge Discovery and Data Mining', 'paragraph': \"SIGKDD, representing the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining, hosts an influential annual conference.\\n\"}, {'link': '/wiki/Differentiable_programming', 'title': 'Differentiable programming', 'paragraph': 'Differentiable programming is a programming paradigm in which a numeric computer program can be differentiated throughout via automatic differentiation.[1][2][3][4][5] This allows for gradient-based optimization of parameters in the program, often via gradient descent, as well as other learning approaches that are based on higher order derivative information. Differentiable programming has found use in a wide variety of areas, particularly scientific computing and artificial intelligence.[5] One of the early proposals to adopt such a framework in a systematic fashion to improve upon learning algorithms was made by the Advanced Concepts Team at the European Space Agency in early 2016.[6]\\n'}, {'link': '/wiki/Force_control', 'title': 'Force control', 'paragraph': 'Force control is the control of the force with which a machine or the manipulator of a robot acts on an object or its environment. By controlling the contact force, damage to the machine as well as to the objects to be processed and injuries when handling people can be prevented. In manufacturing tasks, it can compensate for errors and reduce wear by maintaining a uniform contact force. Force control achieves more consistent results than position control, which is also used in machine control. Force control can be used as an alternative to the usual motion control, but is usually used in a complementary way, in the form of hybrid control concepts. The acting force for control is usually measured via force transducers or estimated via the motor current.\\n'}, {'link': '/wiki/Paraphrase', 'title': 'Paraphrase', 'paragraph': \"A paraphrase (/ˈpærəˌfreɪz/) is a restatement of the meaning of a text or passage using other words. The term itself is derived via Latin paraphrasis, from Ancient Greek  παράφρασις (paráphrasis)\\xa0'additional manner of expression'. The act of paraphrasing is also called paraphrasis.\\n\"}, {'link': '/wiki/Special:BookSources/978-94-010-6610-5', 'title': 'Book sources', 'paragraph': 'This page allows users to search multiple sources for a book given a 10- or 13-digit International Standard Book Number. Spaces and dashes in the ISBN do not matter.\\n'}, {'link': '/wiki/ISSN_(identifier)', 'title': 'ISSN', 'paragraph': 'An International Standard Serial Number (ISSN) is an eight-digit serial number used to uniquely identify a serial publication, such as a magazine.[1] The ISSN is especially helpful in distinguishing between serials with the same title. ISSNs are used in ordering, cataloging, interlibrary loans, and other practices in connection with serial literature.[2]\\n'}, {'link': '/wiki/S2CID_(identifier)', 'title': 'Semantic Scholar', 'paragraph': 'Semantic Scholar is a research tool powered by artificial intelligence for scientific literature. It was developed at the Allen Institute for AI and publicly released in November 2015.[2] It uses advances in natural language processing to provide summaries for scholarly papers.[3] The Semantic Scholar team is actively researching the use of artificial intelligence in natural language processing, machine learning, human–computer interaction, and information retrieval.[4]\\n'}, {'link': '/wiki/PMC_(identifier)', 'title': 'PubMed Central', 'paragraph': \"PubMed Central (PMC) is a free digital repository that archives open access full-text scholarly articles that have been published in biomedical and life sciences journals. As one of the major research databases developed by the National Center for Biotechnology Information (NCBI), PubMed Central is more than a document repository. Submissions to PMC are indexed and formatted for enhanced metadata, medical ontology, and unique identifiers which enrich the XML structured data for each article.[1] Content within PMC can be linked to other NCBI databases and accessed via Entrez search and retrieval systems, further enhancing the public's ability to discover, read and build upon its biomedical knowledge.[2]\\n\"}, {'link': '/wiki/Jerome_H._Friedman', 'title': 'Jerome H. Friedman', 'paragraph': 'Jerome Harold Friedman (born December 29, 1939) is an American statistician, consultant and Professor of Statistics at Stanford University, known for his contributions in the field of statistics and data mining.[2][3]\\n'}, {'link': '/wiki/CiteSeerX_(identifier)', 'title': 'CiteSeerX', 'paragraph': 'CiteSeerX (formerly called CiteSeer) is a public search engine and digital library for scientific and academic papers, primarily in the fields of computer and information science.\\n'}, {'link': '/wiki/Artificial_Intelligence:_A_Modern_Approach', 'title': 'Artificial Intelligence: A Modern Approach', 'paragraph': 'Artificial Intelligence: A Modern Approach (AIMA) is a university textbook on artificial intelligence, written by Stuart J. Russell and Peter Norvig. It was first published in 1995 and the fourth edition of the book was released on 28 April 2020.[1] It is used in over 1400 universities worldwide[2] and has been called \"the most popular artificial intelligence textbook in the world\".[3] It is considered the standard text in the field of artificial intelligence.[4][5]\\nThe book is intended for an undergraduate audience but can also be used for graduate-level studies with the suggestion of adding some of the primary sources listed in the extensive bibliography.  Programs in the book are presented in pseudo code with implementations in Java, Python, Lisp, JavaScript and Scala available online.[6][7] There are also unsupported implementations in Prolog, C++, C#, and several other languages.\\n'}, {'link': '/wiki/Naomi_Altman', 'title': 'Naomi Altman', 'paragraph': 'Naomi Altman is a statistician known for her work on kernel smoothing[KS] and kernel regression,[KR]\\nand interested in applications of statistics to gene expression and genomics. She is a professor of statistics at Pennsylvania State University,[1] and a regular columnist for the \"Points of Significance\" column in Nature Methods.[2]\\n'}, {'link': '/wiki/Nature_Methods', 'title': 'Nature Methods', 'paragraph': 'Nature Methods is a monthly peer-reviewed scientific journal covering new scientific techniques. It was established in 2004 and is published by Springer Nature under the Nature Portfolio. Like other Nature journals, there is no external editorial board and editorial decisions are made by an in-house team, although peer review by external experts forms a part of the review process.[1] The editor-in-chief is Allison Doerr.[2]\\n'}, {'link': '/wiki/Mehryar_Mohri', 'title': 'Mehryar Mohri', 'paragraph': 'Mehryar Mohri is a Professor and theoretical computer scientist[2] at the Courant Institute of Mathematical Sciences. He is also a Research Director \\nat Google Research where he heads the Learning Theory team.\\n'}, {'link': '/wiki/Bibcode_(identifier)', 'title': 'Bibcode', 'paragraph': 'The bibcode (also known as the refcode) is a compact identifier used by several astronomical data systems to uniquely specify literature references.\\n'}, {'link': '/wiki/Springer_Science%2BBusiness_Media', 'title': 'Springer Science+Business Media', 'paragraph': 'Springer Science+Business Media, commonly known as Springer, is a German multinational publishing company of books, e-books and peer-reviewed journals in science, humanities, technical and medical (STM) publishing.[1]\\n'}, {'link': '/wiki/ACM_Computing_Surveys', 'title': 'ACM Computing Surveys', 'paragraph': 'ACM Computing Surveys is peer-reviewed quarterly scientific journal and is published by the Association for Computing Machinery. It publishes survey articles and tutorials related to computer science and computing. The journal was established in 1969 with William S. Dorn as founding editor-in-chief.[1]\\n'}, {'link': '/wiki/Corinna_Cortes', 'title': 'Corinna Cortes', 'paragraph': 'Corinna Cortes (born 31 March, 1961) is a Danish computer scientist known for her contributions to machine learning. She is a Vice President at Google Research in New York City.[3] Cortes is an ACM Fellow and a recipient of the Paris Kanellakis Award for her work on theoretical foundations of support vector machines.[4][5][3][6]\\n'}, {'link': '/wiki/Chartered_Financial_Analyst_(CFA)#Curriculum', 'title': 'Chartered Financial Analyst', 'paragraph': 'The Chartered Financial Analyst (CFA) program is a postgraduate professional certification offered internationally by the America based CFA Institute (formerly the Association for Investment Management and Research, or AIMR) to investment and financial professionals. The program teaches a wide range of subjects relating to advanced investment analysis—including security analysis, statistics, probability theory, fixed income, derivatives, economics, financial analysis, corporate finance, alternative investments, portfolio management—and provides a generalist knowledge of other areas of finance.\\n'}, {'link': '/wiki/Hdl_(identifier)', 'title': 'Handle System', 'paragraph': 'The Handle System is the Corporation for National Research Initiatives\\'s proprietary registry assigning persistent identifiers, or handles, to information resources, and for resolving \"those handles into the information necessary to locate, access, and otherwise make use of the resources\".[1]\\n'}, {'link': '/wiki/Julia_Angwin', 'title': 'Julia Angwin', 'paragraph': 'Julia Angwin is a Pulitzer Prize-winning[1] American investigative journalist,[2] New York Times bestselling author, and entrepreneur. She was a co-founder and editor-in-chief of The Markup, a nonprofit newsroom that investigates the impact of technology on society. She was a senior reporter at ProPublica from 2014 to April 2018[3] and staff reporter at the New York bureau of The Wall Street Journal from 2000 to 2013. Angwin is author of non-fiction books, Stealing MySpace: The Battle to Control the Most Popular Website in America (2009) and Dragnet Nation (2014).[4] She is a winner and two-time finalist for the Pulitzer Prize in journalism.[5]\\n'}, {'link': '/wiki/IEEE_Spectrum', 'title': 'IEEE Spectrum', 'paragraph': 'IEEE Spectrum is a magazine edited by the Institute of Electrical and Electronics Engineers.\\n'}, {'link': '/wiki/New_England_Journal_of_Medicine', 'title': 'The New England Journal of Medicine', 'paragraph': 'The New England Journal of Medicine (NEJM) is a weekly medical journal published by the Massachusetts Medical Society. It is among the most prestigious peer-reviewed medical journals[1][2] as well as the oldest continuously published one.[1]\\n'}, {'link': '/wiki/Nils_Nilsson_(researcher)', 'title': 'Nils John Nilsson', 'paragraph': 'Nils John Nilsson (February 6, 1933 – April 23, 2019) was an American computer scientist. He was one of the founding researchers in the discipline of artificial intelligence.[2] He was the first Kumagai Professor of Engineering in computer science at Stanford University from 1991 until his retirement. He is particularly known for his contributions to search, planning, knowledge representation, and robotics.[2]\\n'}, {'link': '/wiki/Alan_Mackworth', 'title': 'Alan Mackworth', 'paragraph': 'Alan Mackworth is a professor emeritus in the Department of Computer Science at the University of British Columbia. He is known as \"The Founding Father\" of RoboCup. He is a former president of the Association for the Advancement of Artificial Intelligence (AAAI) and former Canada Research Chair in Artificial Intelligence from 2001–2014.\\n'}, {'link': '/wiki/Trevor_Hastie', 'title': 'Trevor Hastie', 'paragraph': 'Trevor John Hastie (born 27 June 1953) is an American statistician and computer scientist. He is currently serving as the John A. Overdeck Professor of Mathematical Sciences and Professor of Statistics at Stanford University.[1] Hastie is known for his contributions to applied statistics, especially in the field of machine learning, data mining, and bioinformatics. He has authored several popular books in statistical learning, including The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Hastie has been listed as an ISI Highly Cited Author in Mathematics by the ISI Web of Knowledge.\\n'}, {'link': '/wiki/Robert_Tibshirani', 'title': 'Robert Tibshirani', 'paragraph': 'Robert Tibshirani FRS FRSC (born July 10, 1956) is a professor in the Departments of Statistics and Biomedical Data Science at Stanford University. He was a professor at the University of Toronto from 1985 to 1998. In his work, he develops statistical tools for the analysis of complex datasets, most recently in genomics and proteomics.\\n'}, {'link': '/wiki/The_Master_Algorithm', 'title': 'The Master Algorithm', 'paragraph': 'The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.\\n'}, {'link': '/wiki/Peter_E._Hart', 'title': 'Peter E. Hart', 'paragraph': 'Peter E. Hart (born 1941[2]) is an American computer scientist and entrepreneur. He was chairman and president of Ricoh Innovations, which he founded in 1997. He made significant contributions in the field of computer science in a series of widely cited publications from the years 1967 to 1975 while associated with the Artificial Intelligence Center of SRI International, a laboratory where he also served as director.\\n'}, {'link': '/wiki/Ray_Solomonoff', 'title': 'Ray Solomonoff', 'paragraph': 'Ray Solomonoff (July 25, 1926 – December 7, 2009)[1][2] was the inventor of algorithmic probability,[3] his General Theory of Inductive Inference (also known as Universal Inductive Inference),[4] and was a founder of algorithmic information theory.[5] He was an originator of the branch of artificial intelligence based on machine learning, prediction and probability. He circulated the first report on non-semantic machine learning in 1956.[6]\\n'}, {'link': '/wiki/Dartmouth_workshop', 'title': 'Dartmouth workshop', 'paragraph': 'The Dartmouth Summer Research Project on Artificial Intelligence was a 1956 summer workshop widely considered[1][2][3] to be the founding event of artificial intelligence as a field.\\n'}, {'link': '/wiki/File:Wikiquote-logo.svg', 'title': 'File:Wikiquote-logo.svg', 'paragraph': 'Original file \\u200e(SVG file, nominally 300 × 355 pixels, file size: 1,012 bytes)\\n'}, {'link': '/wiki/Template_talk:Differentiable_computing', 'title': 'Template talk:Differentiable computing', 'paragraph': \"I don't want to remove the group because I didn't contribute anything to the template and removing stuff added by other users seems rude to me. But now using JS or GO for programming gradient descent based algorithm is not that uncommon, does it implies that wikipedia should also add JS/GO to the group. To put it simply, I don't think having a group for popular programming languages used for Differentiable computing helps anyone but rather might push the idea that Python is the language for AI ignoring many other important stuff, for example the C++ core of the python interface. I don't hate python/julia and not a Go/JS fanboy. -- 1e100 (talk) 11:16, 18 December 2021 (UTC)Reply[reply]\\n\"}, {'link': '/wiki/Differentiable_function', 'title': 'Differentiable function', 'paragraph': 'In mathematics, a differentiable function of one real variable is a function whose derivative exists at each point in its domain. In other words, the graph of a differentiable function has a non-vertical tangent line at each interior point in its domain. A differentiable function is smooth (the function is locally well approximated as a linear function at each interior point) and does not contain any break, angle, or cusp.\\n'}, {'link': '/wiki/Information_geometry', 'title': 'Information geometry', 'paragraph': 'Information geometry is an interdisciplinary field that applies the techniques of differential geometry to study probability theory and statistics. [1]  It studies statistical manifolds, which are Riemannian manifolds whose points correspond to probability distributions.\\n'}, {'link': '/wiki/Statistical_manifold', 'title': 'Statistical manifold', 'paragraph': 'In mathematics, a statistical manifold is a Riemannian manifold, each of whose points is a probability distribution.  Statistical manifolds provide a setting for the field of information geometry.  The Fisher information metric provides a metric on these manifolds. Following this definition, the log-likelihood function is a differentiable map and the score is an inclusion.[1]\\n'}, {'link': '/wiki/Automatic_differentiation', 'title': 'Automatic differentiation', 'paragraph': 'In mathematics and computer algebra, automatic differentiation (auto-differentiation, autodiff, or AD), also called algorithmic differentiation, computational differentiation,[1][2] is a set of techniques to evaluate the partial derivative of a function specified by a computer program.\\n'}, {'link': '/wiki/Tensor_calculus', 'title': 'Tensor calculus', 'paragraph': 'In mathematics, tensor calculus, tensor analysis, or Ricci calculus is an extension of vector calculus to tensor fields (tensors that may vary over a manifold, e.g. in spacetime).\\n'}, {'link': '/wiki/Inductive_bias', 'title': 'Inductive bias', 'paragraph': 'The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered.[1]\\nInductive bias is anything which makes the algorithm learn one pattern instead of another pattern (e.g. step-functions in decision trees instead of continuous function in a linear regression model).\\n'}, {'link': '/wiki/Gradient_descent', 'title': 'Gradient descent', 'paragraph': 'Gradient descent (also often called steepest descent) is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for finding a local minimum of a differentiable multivariate function\\n'}, {'link': '/wiki/Stochastic_gradient_descent', 'title': 'Stochastic gradient descent', 'paragraph': 'Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.[1]\\n'}, {'link': '/wiki/Attention_(machine_learning)', 'title': 'Attention (machine learning)', 'paragraph': 'Machine learning-based attention is a mechanism mimicking cognitive attention. It calculates \"soft\" weights for each word, more precisely for its embedding, in the context window. It can do it either in parallel (such as in transformers) or sequentially (such as recurrent neural networks). \"Soft\" weights can change during each runtime, in contrast to \"hard\" weights, which are (pre-)trained and fine-tuned and remain frozen afterwards.  \\n'}, {'link': '/wiki/Convolution', 'title': 'Convolution', 'paragraph': 'In mathematics (in particular, functional analysis), convolution is a mathematical operation on two functions (f and g) that produces a third function (\\n\\n\\n\\nf\\n∗\\ng\\n\\n\\n{\\\\displaystyle f*g}\\n\\n) that expresses how the shape of one is modified by the other. The term convolution refers to both the result function and to the process of computing it. It is defined as the integral of the product of the two functions after one is reflected about the y-axis and shifted. The choice of which function is reflected and shifted before the integral does not change the integral result (see commutativity). The integral is evaluated for all values of shift, producing the convolution function.\\n'}, {'link': '/wiki/Loss_functions_for_classification', 'title': 'Loss functions for classification', 'paragraph': 'In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to).[1]  Given \\n\\n\\n\\n\\n\\nX\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\mathcal {X}}}\\n\\n as the space of all possible inputs (usually \\n\\n\\n\\n\\n\\nX\\n\\n\\n⊂\\n\\n\\nR\\n\\n\\nd\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\mathcal {X}}\\\\subset \\\\mathbb {R} ^{d}}\\n\\n), and \\n\\n\\n\\n\\n\\nY\\n\\n\\n=\\n{\\n−\\n1\\n,\\n1\\n}\\n\\n\\n{\\\\displaystyle {\\\\mathcal {Y}}=\\\\{-1,1\\\\}}\\n\\n as the set of labels (possible outputs), a typical goal of classification algorithms is to find a function \\n\\n\\n\\nf\\n:\\n\\n\\nX\\n\\n\\n→\\n\\n\\nY\\n\\n\\n\\n\\n{\\\\displaystyle f:{\\\\mathcal {X}}\\\\to {\\\\mathcal {Y}}}\\n\\n which best predicts a label \\n\\n\\n\\ny\\n\\n\\n{\\\\displaystyle y}\\n\\n for a given input \\n\\n\\n\\n\\n\\n\\nx\\n→\\n\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\vec {x}}}\\n\\n.[2]  However, because of incomplete information, noise in the measurement, or probabilistic components in the underlying process, it is possible for the same \\n\\n\\n\\n\\n\\n\\nx\\n→\\n\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\vec {x}}}\\n\\n to generate different \\n\\n\\n\\ny\\n\\n\\n{\\\\displaystyle y}\\n\\n.[3]  As a result, the goal of the learning problem is to minimize expected loss (also known as the risk), defined as\\n'}, {'link': '/wiki/Activation_function', 'title': 'Activation function', 'paragraph': 'Activation function of a node in an artificial neural network is a function that calculates the output of the node (based on its inputs and the weights on individual inputs). Nontrivial problems can be solved only using a nonlinear activation function.[1] Modern activation functions include the smooth version of the ReLU, the GELU, which was used in the 2018 BERT model,[2] the logistic (sigmoid) function used in the 2012 speech recognition model developed by Hinton et al,[3] the ReLU used in the 2012 AlexNet computer vision model and in the 2015 ResNet model. \\n'}, {'link': '/wiki/Softmax_function', 'title': 'Softmax function', 'paragraph': \"The softmax function, also known as softargmax[1]:\\u200a184\\u200a or normalized exponential function,[2]:\\u200a198\\u200a converts a vector of K real numbers into a probability distribution of K possible outcomes. It is a generalization of the logistic function to multiple dimensions, and used in multinomial logistic regression. The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes, based on Luce's choice axiom.\\n\"}, {'link': '/wiki/Rectifier_(neural_networks)', 'title': 'Rectifier (neural networks)', 'paragraph': 'In the context of artificial neural networks, the rectifier or ReLU (rectified linear unit) activation function[1][2] is an activation function defined as the positive part of its argument:\\n'}, {'link': '/wiki/Data_augmentation', 'title': 'Data augmentation', 'paragraph': 'Data augmentation is a technique in machine learning used to reduce overfitting when training a machine learning model,[1] by training models on several slightly-modified copies of existing data.\\n'}, {'link': '/wiki/Diffusion_process', 'title': 'Diffusion process', 'paragraph': 'In probability theory and statistics, diffusion processes are a class of continuous-time Markov process with almost surely continuous sample paths. Diffusion process is stochastic in nature and hence is used to model many real-life stochastic systems. Brownian motion, reflected Brownian motion and Ornstein–Uhlenbeck processes are examples of diffusion processes. It is used heavily in statistical physics, statistical analysis, information theory, data science, neural networks, finance and marketing.\\n'}, {'link': '/wiki/Autoregressive_model', 'title': 'Autoregressive model', 'paragraph': 'In statistics, econometrics, and signal processing, an autoregressive (AR) model is a representation of a type of random process; as such, it is used to describe certain time-varying processes in nature, economics, behavior, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term); thus the model is in the form of a stochastic difference equation (or recurrence relation) which should not be confused with a differential equation. Together with the moving-average (MA) model, it is a special case and key component of the more general autoregressive–moving-average (ARMA) and autoregressive integrated moving average (ARIMA) models of time series, which have a more complicated stochastic structure; it is also a special case of the vector autoregressive model (VAR), which consists of a system of more than one interlocking stochastic difference equation in more than one evolving random variable.\\n'}, {'link': '/wiki/Prompt_engineering#In-context_learning', 'title': 'Prompt engineering', 'paragraph': 'Prompt engineering is the process of structuring text that can be interpreted and understood by a generative AI model.[1][2] A prompt is natural language text describing the task that an AI should perform.[3]\\n'}, {'link': '/wiki/Computational_science', 'title': 'Computational science', 'paragraph': '\\nComputational science, also known as scientific computing, technical computing or scientific computation (SC), is a division of science that uses advanced computing capabilities to understand and solve complex physical problems. This includes \\n'}, {'link': '/wiki/Graphcore', 'title': 'Graphcore', 'paragraph': 'Graphcore Limited is a British semiconductor company that develops accelerators for AI and machine learning. It aims to make a massively parallel Intelligence Processing Unit (IPU) that holds the complete machine learning model inside the processor.[2]\\n'}, {'link': '/wiki/Tensor_Processing_Unit', 'title': 'Tensor Processing Unit', 'paragraph': \"Tensor Processing Unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google for neural network machine learning, using Google's own TensorFlow software.[1] Google began using TPUs internally in 2015, and in 2018 made them available for third party use, both as part of its cloud infrastructure and by offering a smaller version of the chip for sale.\\n\"}, {'link': '/wiki/Vision_processing_unit', 'title': 'Vision processing unit', 'paragraph': 'A vision processing unit (VPU) is (as of 2023) an emerging class of microprocessor; it is a specific type of AI accelerator, designed to accelerate machine vision tasks.[1][2]\\n'}, {'link': '/wiki/SpiNNaker', 'title': 'SpiNNaker', 'paragraph': 'SpiNNaker (spiking neural network architecture) is a massively parallel, manycore supercomputer architecture designed by the Advanced Processor Technologies Research Group (APT) at the Department of Computer Science, University of Manchester.[2]  It is composed of 57,600 processing nodes, each with 18 ARM9 processors (specifically ARM968) and 128 MB of mobile DDR SDRAM, totalling 1,036,800 cores and over 7 TB of RAM.[3]  The computing platform is based on spiking neural networks, useful in simulating the human brain (see Human Brain Project).[4][5][6][7][8][9][10][11][12]\\n'}, {'link': '/wiki/Flux_(machine-learning_framework)', 'title': 'Flux (machine-learning framework)', 'paragraph': 'Flux is an open-source machine-learning software library and ecosystem written in Julia.[1][6] Its current stable release is v0.14.5[4]\\xa0. It has a layer-stacking-based interface for simpler models, and has a strong support on interoperability with other Julia packages instead of a monolithic design.[7] For example, GPU support is implemented transparently by CuArrays.jl[8] This is in contrast to some other machine learning frameworks which are implemented in other languages with Julia bindings,\\xa0such as TensorFlow.jl, and thus are more limited by the functionality present in the underlying implementation, which is often in C or C++.[9] Flux joined NumFOCUS as an affiliated project in December of 2021.[10]\\n'}, {'link': '/wiki/AlexNet', 'title': 'AlexNet', 'paragraph': \"AlexNet is the name of a convolutional neural network (CNN) architecture, designed by Alex Krizhevsky in collaboration with Ilya Sutskever and Geoffrey Hinton, who was Krizhevsky's Ph.D. advisor at the University of Toronto.[1][2]\\n\"}, {'link': '/wiki/WaveNet', 'title': 'WaveNet', 'paragraph': \"WaveNet is a deep neural network for generating raw audio. It was created by researchers at London-based AI firm DeepMind. The technique, outlined in a paper in September 2016,[1] is able to generate relatively realistic-sounding human-like voices by directly modelling waveforms using a neural network method trained with recordings of real speech. Tests with US English and Mandarin reportedly showed that the system outperforms Google's best existing text-to-speech (TTS) systems, although as of 2016 its text-to-speech synthesis still was less convincing than actual human speech.[2] WaveNet's ability to generate raw waveforms means that it can model any kind of audio, including music.[3]\\n\"}, {'link': '/wiki/Deep_learning_speech_synthesis', 'title': 'Deep learning speech synthesis', 'paragraph': 'Deep learning speech synthesis uses Deep Neural Networks (DNN) to produce\\nartificial speech from text (text-to-speech) or spectrum (vocoder).\\nThe deep neural networks are trained using a large amount of recorded speech and, in the case of\\na text-to-speech system, the associated labels and/or input text.\\n'}, {'link': '/wiki/AlphaFold', 'title': 'AlphaFold', 'paragraph': 'AlphaFold is an artificial intelligence (AI) program developed by DeepMind, a subsidiary of Alphabet, which performs predictions of protein structure.[1] The program is designed as a deep learning system.[2]\\n'}, {'link': '/wiki/Whisper_(speech_recognition_system)', 'title': 'Whisper (speech recognition system)', 'paragraph': 'Whisper is a machine learning model for speech recognition and transcription, created by OpenAI and first released as open-source software in September 2022.[2]\\n'}, {'link': '/wiki/Word2vec', 'title': 'Word2vec', 'paragraph': 'Word2vec is a technique for natural language processing (NLP) published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that they capture the semantic and syntactic qualities of words; as such, a simple mathematical function (cosine similarity) can indicate the level of semantic similarity between the words represented by those vectors.\\n'}, {'link': '/wiki/Seq2seq', 'title': 'Seq2seq', 'paragraph': 'Seq2seq is a family of machine learning approaches used for natural language processing.[1] Applications include language translation, image captioning, conversational models, and text summarization.[2]\\nSeq2seq uses sequence transformation: it turns one sequence into another sequence.\\n'}, {'link': '/wiki/BERT_(language_model)', 'title': 'BERT (language model)', 'paragraph': '\\nBidirectional Encoder Representations from Transformers (BERT) is a family of language models introduced in October 2018 by researchers at Google.[1][2] A 2020 literature survey concluded that \"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\"[3]\\n'}, {'link': '/wiki/Neural_machine_translation', 'title': 'Neural machine translation', 'paragraph': 'Neural machine translation (NMT) is an approach to machine translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modeling and then translating entire sentences in a single integrated model.\\n'}, {'link': '/wiki/Project_Debater', 'title': 'Project Debater', 'paragraph': 'Project Debater is an IBM artificial intelligence project, designed to participate in a full live debate with expert human debaters.[1][2][3][4] It follows on from the Watson project which played Jeopardy![5]\\n'}, {'link': '/wiki/GPT-1', 'title': 'GPT-1', 'paragraph': 'Generative Pre-trained Transformer 1 (GPT-1) was the first of OpenAI\\'s large language models following Google\\'s invention of the transformer architecture in 2017.[2] In June 2018, OpenAI released a paper entitled \"Improving Language Understanding by Generative Pre-Training\",[3] in which they introduced that initial model along with the general concept of a generative pre-trained transformer.[4]\\n'}, {'link': '/wiki/GPT-2', 'title': 'GPT-2', 'paragraph': 'Generative Pre-trained Transformer 2 (GPT-2) is a large language model by OpenAI and the second in their foundational series of GPT models. GPT-2 was pre-trained on BookCorpus,[2] a dataset of over 7,000 self-published fiction books from various genres, and trained on a dataset of 8 million web pages.[3] It was partially released in February 2019, followed by full release of the 1.5-billion-parameter model on November 5, 2019.[4][5][6][7][8]\\n'}, {'link': '/wiki/GPT-J', 'title': 'GPT-J', 'paragraph': 'GPT-J or GPT-J-6B is an open-source large language model (LLM) developed by EleutherAI in 2021.[1] As the name suggests, it is a generative pre-trained transformer model designed to produce human-like text that continues from a prompt. The optional \"6B\" in the name refers to the fact that it has 6 billion parameters.[2]\\n'}, {'link': '/wiki/Chinchilla_AI', 'title': 'Chinchilla AI', 'paragraph': 'Chinchilla is a family of large language models developed by the research team at DeepMind, presented in March of 2022.[1] It is named \"chinchilla\" because it is a further development over a previous model family named Gopher. Both model families were trained in order to investigate the scaling laws of large language models.[2]\\n'}, {'link': '/wiki/PaLM', 'title': 'PaLM', 'paragraph': 'PaLM (Pathways Language Model) is a 540 billion parameter transformer-based large language model developed by Google AI.[1] Researchers also trained smaller versions of PaLM, 8 and 62 billion parameter models, to test the effects of model scale.[2]\\n'}, {'link': '/wiki/BLOOM_(language_model)', 'title': 'BLOOM (language model)', 'paragraph': \"BigScience Large Open-science Open-access Multilingual Language Model (BLOOM[1]) is a transformer-based large language model. It was created by AI researchers to provide a free large language model for large-scale public access. Trained on around 366 billion tokens over March through July 2022, it is considered an alternative to OpenAI's GPT-3 with its 176 billion parameters. BLOOM uses a decoder-only transformer model architecture modified from Megatron-LM GPT-2.\\n\"}, {'link': '/wiki/LLaMA', 'title': 'LLaMA', 'paragraph': 'LLaMA (Large Language Model Meta AI) is a family of large language models (LLMs), released by Meta AI starting in February 2023. \\n'}, {'link': '/wiki/OpenAI_Five', 'title': 'OpenAI Five', 'paragraph': '\\nOpenAI Five is a computer program by OpenAI that plays the five-on-five video game Dota 2. Its first public appearance occurred in 2017, where it was demonstrated in a live one-on-one game against the professional player Dendi, who lost to it. The following year, the system had advanced to the point of performing as a full team of five, and began playing against and showing the capability to defeat professional teams.\\n'}, {'link': '/wiki/MuZero', 'title': 'MuZero', 'paragraph': \"MuZero is a computer program developed by artificial intelligence research company DeepMind to master games without knowing their rules.[1][2][3] Its release in 2019 included benchmarks of its performance in go, chess, shogi, and a standard suite of Atari games. The algorithm uses an approach similar to AlphaZero. It matched AlphaZero's performance in chess and shogi, improved on its performance in Go (setting a new world record), and improved on the state of the art in mastering a suite of 57 Atari games (the Arcade Learning Environment), a visually-complex domain.\\n\"}, {'link': '/wiki/Robot_control', 'title': 'Robot control', 'paragraph': 'Robotic control is the system that contributes to the movement of robots. This involves the mechanical aspects and programmable systems that makes it possible to control robots. Robotics can be controlled by various means including manual, wireless, semi-autonomous (a mix of fully automatic and wireless control), and fully autonomous (using artificial intelligence).\\n'}, {'link': '/wiki/Alex_Graves_(computer_scientist)', 'title': 'Alex Graves (computer scientist)', 'paragraph': 'Alex Graves is a computer scientist. Before working as a research scientist at DeepMind, he earned a BSc in Theoretical Physics from the University of Edinburgh and a PhD in artificial intelligence under Jürgen Schmidhuber at IDSIA.[1] He was also a postdoc under Schmidhuber at the Technical University of Munich and under Geoffrey Hinton[2] at the University of Toronto.\\n'}, {'link': '/wiki/Anthropic', 'title': 'Anthropic', 'paragraph': 'Anthropic PBC is an American artificial intelligence (AI) startup company, founded by former members of OpenAI.[3][4] Anthropic develops general AI systems and large language models.[5] It is a public-benefit corporation, and has been connected to the effective altruism movement.\\n'}, {'link': '/wiki/Hugging_Face', 'title': 'Hugging Face', 'paragraph': 'Hugging Face, Inc. is a French-American company that develops tools for building applications using machine learning, based in New York City. It is most notable for its transformers library built for natural language processing applications and its platform that allows users to share machine learning models and datasets and showcase their work.\\n'}, {'link': '/wiki/Meta_AI', 'title': 'Meta AI', 'paragraph': \"Meta AI is an artificial intelligence laboratory that belongs to Meta Platforms Inc. (formerly known as Facebook, Inc.)[1] Meta AI intends to develop various forms of artificial intelligence, improving augmented and artificial reality technologies.[2] Meta AI is an academic research laboratory focused on generating knowledge for the AI community.[3] This is in contrast to Facebook's Applied Machine Learning (AML) team, which focuses on practical applications of its products.[3]\\n\"}, {'link': '/wiki/Mila_(research_institute)', 'title': 'Mila (research institute)', 'paragraph': 'Mila - Quebec AI Institute (originally Montreal Institute for Learning Algorithms) is a research institute in Montreal, Quebec, focusing mainly on machine learning research. Approximately 1000 students and researchers and 100 faculty members, were part of Mila in 2022.[1] Mila is part of the Pan-Canadian AI Strategy. [2]\\n'}, {'link': '/wiki/MIT_Computer_Science_and_Artificial_Intelligence_Laboratory', 'title': 'MIT Computer Science and Artificial Intelligence Laboratory', 'paragraph': 'Computer Science and Artificial Intelligence Laboratory (CSAIL) is a research institute at the Massachusetts Institute of Technology (MIT) formed by the 2003 merger of the Laboratory for Computer Science (LCS) and the Artificial Intelligence Laboratory (AI Lab). Housed within the Ray and Maria Stata Center, CSAIL is the largest on-campus laboratory as measured by research scope and membership. It is part of the Schwarzman College of Computing[1] but is also overseen by the MIT Vice President of Research.[2]\\n'}, {'link': '/wiki/Neural_Turing_machine', 'title': 'Neural Turing machine', 'paragraph': 'A neural Turing machine (NTM) is a recurrent neural network model of a Turing machine. The approach was published by Alex Graves et al. in 2014.[1] NTMs combine the fuzzy pattern matching capabilities of neural networks with the algorithmic power of programmable computers. \\n'}, {'link': '/wiki/Differentiable_neural_computer', 'title': 'Differentiable neural computer', 'paragraph': 'In artificial intelligence, a differentiable neural computer (DNC) is a memory augmented neural network architecture (MANN), which is typically (but not by definition) recurrent in its implementation. The model was published in 2016 by Alex Graves et al. of DeepMind.[1]\\n'}, {'link': '/wiki/Residual_neural_network', 'title': 'Residual neural network', 'paragraph': 'A Residual Neural Network (a.k.a. Residual Network, ResNet)[1] is a deep learning model in which the weight layers learn residual functions with reference to the layer inputs. A Residual Network[1] is a network with skip connections that perform identity mappings, merged with the layer outputs by addition. It behaves like a Highway Network[2] whose gates are opened through strongly positive bias weights. This enables deep learning models with tens or hundreds of layers to train easily and approach better accuracy when going deeper. The identity skip connections, often referred to as \"residual connections\", are also used in the 1997 LSTM networks,[3] Transformer models (e.g., BERT, GPT models such as ChatGPT), the AlphaGo Zero system, the AlphaStar system, and the AlphaFold system.\\n'}, {'link': '/wiki/Graph_neural_network', 'title': 'Graph neural network', 'paragraph': 'A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs.[1][2][3][4][5]\\n'}, {'link': '/wiki/File:Symbol_portal_class.svg', 'title': 'File:Symbol portal class.svg', 'paragraph': 'Original file \\u200e(SVG file, nominally 180 × 185 pixels, file size: 12 KB)\\n'}, {'link': '/wiki/Portal:Computer_programming', 'title': 'Portal:Computer programming', 'paragraph': 'Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks. It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages. Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit. Proficient programming usually requires expertise in several different subjects, including knowledge of the application domain, details of programming languages and generic code libraries, specialized algorithms, and formal logic.\\n'}, {'link': '/wiki/Category:Artificial_neural_networks', 'title': 'Category:Artificial neural networks', 'paragraph': 'This category are for articles about artificial neural networks (ANN).\\n'}, {'link': '/wiki/Category:Machine_learning', 'title': 'Category:Machine learning', 'paragraph': 'Machine learning is a branch of statistics and computer science which studies algorithms and architectures that learn from observed facts.\\n'}, {'link': '/wiki/Template:Computer_science', 'title': 'Template:Computer science', 'paragraph': \"This template's initial visibility currently defaults to autocollapse, meaning that if there is another collapsible item on the page (a navbox, sidebar, or table with the collapsible attribute), it is hidden apart from its title bar; if not, it is fully visible.\\n\"}, {'link': '/wiki/Template_talk:Computer_science', 'title': 'Template talk:Computer science', 'paragraph': 'This part is a bit short. But does OLAP really belong into here? is it really \"major\"?\\n'}, {'link': '/wiki/ACM_Computing_Classification_System', 'title': 'ACM Computing Classification System', 'paragraph': 'The ACM Computing Classification System (CCS) is a subject classification system for computing devised by the Association for Computing Machinery (ACM). The system is comparable to the Mathematics Subject Classification (MSC) in scope, aims, and structure, being used by the various ACM journals to organize subjects by area.\\n'}, {'link': '/wiki/Peripheral', 'title': 'Peripheral', 'paragraph': 'A peripheral  device, or simply peripheral, is an auxiliary hardware device used to transfer information into and out of a computer.[1] The term peripheral device refers to all hardware components that are attached to a computer and are controlled by the computer system, but they are not the core components of the computer.\\n'}, {'link': '/wiki/Very_Large_Scale_Integration', 'title': 'Very Large Scale Integration', 'paragraph': 'Very-large-scale integration (VLSI) is the process of creating an integrated circuit (IC) by combining millions or billions of MOS transistors onto a single chip. VLSI began in the 1970s when MOS integrated circuit (Metal Oxide Semiconductor) chips were developed and then widely adopted, enabling complex semiconductor and telecommunication technologies. The microprocessor and memory chips are VLSI devices.\\n'}, {'link': '/wiki/File:Computer_Retro.svg', 'title': 'File:Computer Retro.svg', 'paragraph': 'Original file \\u200e(SVG file, nominally 512 × 512 pixels, file size: 499 KB)\\n'}, {'link': '/wiki/Computer_architecture', 'title': 'Computer architecture', 'paragraph': 'In computer science and computer engineering, computer architecture is a description of the structure of a computer system made from component parts.[1] It can sometimes be a high-level description that ignores details of the implementation.[2] At a more detailed level, the description may include the instruction set architecture design, microarchitecture design, logic design, and implementation.[3]\\n'}, {'link': '/wiki/Real-time_computing', 'title': 'Real-time computing', 'paragraph': 'Real-time computing (RTC) is the computer science term for hardware and software systems subject to a \"real-time constraint\", for example from event to system response.[1] Real-time programs must guarantee response within specified time constraints, often referred to as \"deadlines\".[2]\\n'}, {'link': '/wiki/Dependability', 'title': 'Dependability', 'paragraph': \"In systems engineering, dependability is a measure of a system's availability, reliability, maintainability, and in some cases, other characteristics such as durability, safety and security.[1]  In real-time computing, dependability is the ability to provide services that can be trusted within a time-period.[2] The service guarantees must hold even when the system is subject to attacks or natural failures. \\n\"}, {'link': '/wiki/Network_architecture', 'title': 'Network architecture', 'paragraph': \"Network architecture is the design of a computer network. It is a framework for the specification of a network's physical components and their functional organization and configuration, its operational principles and procedures, as well as communication protocols used.\\n\"}, {'link': '/wiki/Networking_hardware', 'title': 'Networking hardware', 'paragraph': '\\nNetworking hardware, also known as network equipment or computer networking devices, are electronic devices that are required for communication and interaction between devices on a computer network. Specifically, they mediate data transmission in a computer network.[1] Units which are the last receiver or generate data are called hosts, end systems or data terminal equipment.\\n'}, {'link': '/wiki/Network_scheduler', 'title': 'Network scheduler', 'paragraph': 'A network scheduler, also called packet scheduler, queueing discipline (qdisc) or queueing algorithm, is an arbiter on a node in a packet switching communication network. It manages the sequence of network packets in the transmit and receive queues of the protocol stack and network interface controller. There are several network schedulers available for the different operating systems, that implement many of the existing network scheduling algorithms.\\n'}, {'link': '/wiki/Network_performance', 'title': 'Network performance', 'paragraph': 'Network performance refers to measures of service quality of a network as seen by the customer.\\n'}, {'link': '/wiki/Network_service', 'title': 'Network service', 'paragraph': 'In computer networking, a network service is an application running at the network application layer and above, that provides data storage, manipulation, presentation, communication or other capability which is often implemented using a client–server or peer-to-peer architecture based on application layer network protocols.[1]\\n'}, {'link': '/wiki/Interpreter_(computing)', 'title': 'Interpreter (computing)', 'paragraph': 'In computer science, an interpreter is a computer program that directly executes instructions written in a programming or scripting language, without requiring them previously to have been compiled into a machine language program. An interpreter generally uses one of the following strategies for program execution:\\n'}, {'link': '/wiki/Middleware', 'title': 'Middleware', 'paragraph': 'Middleware is a type of computer software programme that provides services to software applications beyond those available from the operating system. It can be described as \"software glue\".[1]\\n'}, {'link': '/wiki/Software_quality', 'title': 'Software quality', 'paragraph': 'In the context of software engineering, software quality refers to two related but distinct notions:[citation needed]\\n'}, {'link': '/wiki/Programming_language_theory', 'title': 'Programming language theory', 'paragraph': 'Programming language theory (PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of formal languages known as programming languages. Programming language theory is closely related to other fields including mathematics, software engineering, and linguistics. There are a number of academic conferences and journals in the area.\\n'}, {'link': '/wiki/Programming_tool', 'title': 'Programming tool', 'paragraph': 'A programming tool or software development tool is a computer program that software developers use to create, debug, maintain, or otherwise support other programs and applications. The term usually refers to relatively simple programs, that can be combined to accomplish a task, much as one might use multiple hands to fix a physical object. The most basic tools are a source code editor and a compiler or interpreter, which are used ubiquitously and continuously. Other tools are used more or less depending on the language, development methodology, and individual engineer, often used for a discrete task, like a debugger or profiler. Tools may be discrete programs, executed separately – often from the command line – or may be parts of a single large program, called an integrated development environment (IDE). In many cases, particularly for simpler use, simple ad hoc techniques are used instead of a tool, such as print debugging instead of using a debugger, manual timing (of overall program or section of code) instead of a profiler, or tracking bugs in a  text file or spreadsheet instead of a bug tracking system.\\n'}, {'link': '/wiki/Programming_paradigm', 'title': 'Programming paradigm', 'paragraph': 'Programming paradigms are a way to classify programming languages based on their features. Languages can be classified into multiple paradigms.\\n'}, {'link': '/wiki/Domain-specific_language', 'title': 'Domain-specific language', 'paragraph': 'A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains. There are a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software, such as MUSH soft code. DSLs can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. Special-purpose computer languages have always existed in the computer age, but the term \"domain-specific language\" has become more popular due to the rise of domain-specific modeling. Simpler DSLs, particularly ones used by a single application, are sometimes informally called mini-languages.\\n'}, {'link': '/wiki/Modeling_language', 'title': 'Modeling language', 'paragraph': 'A modeling language is any artificial language that can be used to express data, information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure Programing language.\\n'}, {'link': '/wiki/Software_configuration_management', 'title': 'Software configuration management', 'paragraph': 'In software engineering, software configuration management (SCM or S/W CM; also expanded as source configuration management process and software change and configuration management[1]) is the task of tracking and controlling changes in the software, part of the larger cross-disciplinary field of configuration management.[2]  SCM practices include revision control and the establishment of baselines.  If something goes wrong, SCM can determine the \"what, when, why and who\" of the change.  If a configuration is working well, SCM can determine how to replicate it across many hosts.\\n'}, {'link': '/wiki/Software_repository', 'title': 'Software repository', 'paragraph': 'A software repository, or repo for short, is a storage location for software packages. Often a table of contents is also stored, along with metadata. A software repository is typically managed by source or version control, or repository managers. Package managers allow automatically installing and updating repositories, sometimes called \"packages\".\\n'}, {'link': '/wiki/Control_variable_(programming)', 'title': 'Control flow', 'paragraph': 'In computer science, control flow (or flow of control) is the order in which individual statements, instructions or function calls of an imperative program are executed or evaluated. The emphasis on explicit control flow distinguishes an imperative programming language from a declarative programming language.\\n'}, {'link': '/wiki/Software_design', 'title': 'Software design', 'paragraph': 'Software design is the process by which an agent creates a specification of a software artifact intended to accomplish goals, using a set of primitive components and subject to constraints.[1] The term is sometimes used broadly to refer to \"all the activity involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying\" the software, or more specifically \"the activity following requirements specification and before programming, as ... [in] a stylized software engineering process.\"[2]\\n'}, {'link': '/wiki/Software_construction', 'title': 'Software construction', 'paragraph': 'Software construction is a software engineering discipline. It is the detailed creation of working meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging. It is linked to all the other software engineering disciplines, most strongly to software design and software testing.[1]\\n'}, {'link': '/wiki/Software_maintenance', 'title': 'Software maintenance', 'paragraph': 'Software maintenance in software engineering is the modification of a software product after delivery to correct faults, to improve performance or other attributes.[1][2]\\n'}, {'link': '/wiki/Programming_team', 'title': 'Programming team', 'paragraph': 'A programming team is a team of people who develop or maintain computer software.[1]  They may be organised in numerous ways, but the egoless programming team and chief programmer team have been common structures.[2]\\n'}, {'link': '/wiki/Theory_of_computation', 'title': 'Theory of computation', 'paragraph': 'In theoretical computer science and mathematics, the theory of computation is the branch that deals with what problems can be solved on a model of computation, using an algorithm, how efficiently they can be solved or to what degree (e.g., approximate solutions versus precise ones).  The field is divided into three major branches: automata theory and formal languages, computability theory, and computational complexity theory, which are linked by the question: \"What are the fundamental capabilities and limitations of computers?\".[1]\\n'}, {'link': '/wiki/Model_of_computation', 'title': 'Model of computation', 'paragraph': 'In computer science, and more specifically in computability theory and computational complexity theory, a model of computation is a model which describes how an output of a mathematical function is computed given an input. A model describes how units of computations, memories, and communications are organized.[1] The computational complexity of an algorithm can be measured given a model of computation. Using a model allows studying the performance of algorithms independently of the variations that are specific to particular implementations and specific technology.\\n'}, {'link': '/wiki/Computability_theory', 'title': 'Computability theory', 'paragraph': '\\nComputability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability. In these areas, computability theory overlaps with proof theory and effective descriptive set theory.\\n'}, {'link': '/wiki/Computational_complexity_theory', 'title': 'Computational complexity theory', 'paragraph': '\\nIn theoretical computer science and mathematics, computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.\\n'}, {'link': '/wiki/Logic_in_computer_science', 'title': 'Logic in computer science', 'paragraph': 'Logic in computer science covers the overlap between the field of logic and that of computer science. The topic can essentially be divided into three main areas:\\n'}, {'link': '/wiki/Semantics_(computer_science)', 'title': 'Semantics (computer science)', 'paragraph': 'In programming language theory, semantics is the rigorous mathematical study of the meaning of programming languages.[1] Semantics assigns computational meaning to valid strings in a programming language syntax. It is closely related to, and often crosses over with, the semantics of mathematical proofs.\\n'}, {'link': '/wiki/Analysis_of_algorithms', 'title': 'Analysis of algorithms', 'paragraph': \"In computer science, the analysis of algorithms is the process of finding the computational complexity of algorithms—the amount of time, storage, or other resources needed to execute them. Usually, this involves determining a function that relates the size of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity). An algorithm is said to be efficient when this function's values are small, or grow slowly compared to a growth in the size of the input. Different inputs of the same size may cause the algorithm to have different behavior, so best, worst and average case descriptions might all be of practical interest.  When not otherwise specified, the function describing the performance of an algorithm is usually an upper bound, determined from the worst case inputs to the algorithm.\\n\"}, {'link': '/wiki/Algorithmic_efficiency', 'title': 'Algorithmic efficiency', 'paragraph': '\\nIn computer science, algorithmic efficiency is a property of an algorithm which relates to the amount of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on the usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.\\n'}, {'link': '/wiki/Randomized_algorithm', 'title': 'Randomized algorithm', 'paragraph': 'A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic or procedure. The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the \"average case\" over all possible choices of random determined by the random bits; thus either the running time, or the output (or both) are random variables.\\n'}, {'link': '/wiki/Computational_geometry', 'title': 'Computational geometry', 'paragraph': 'Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with a history stretching back to antiquity.\\n'}, {'link': '/wiki/Discrete_mathematics', 'title': 'Discrete mathematics', 'paragraph': 'Discrete mathematics is the study of mathematical structures that can be considered \"discrete\" (in a way analogous to discrete variables, having a bijection with the set of natural numbers) rather than \"continuous\" (analogously to continuous functions). Objects studied in discrete mathematics include integers, graphs, and statements in logic.[1][2][3] By contrast, discrete mathematics excludes topics in \"continuous mathematics\" such as real numbers, calculus or Euclidean geometry. Discrete objects can often be enumerated by integers; more formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets[4] (finite sets or sets with the same cardinality as the natural numbers). However, there is no exact definition of the term \"discrete mathematics\".[5]\\n'}, {'link': '/wiki/Mathematical_software', 'title': 'Mathematical software', 'paragraph': 'Mathematical software is software used to model, analyze or calculate numeric, symbolic or geometric data.[1]\\n'}, {'link': '/wiki/Information_system', 'title': 'Information system', 'paragraph': 'An information system (IS) is a formal, sociotechnical, organizational system designed to collect, process, store, and distribute information.[1] From a sociotechnical perspective, information systems are composed by four components: task, people, structure (or roles), and technology.[2] Information systems can be defined as an integration of components for collection, storage and processing of data of which the data is used to provide information, contribute to knowledge as well as digital products that facilitate decision making.[3]\\n'}, {'link': '/wiki/Database', 'title': 'Database', 'paragraph': 'In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.\\n'}, {'link': '/wiki/Enterprise_information_system', 'title': 'Enterprise information system', 'paragraph': 'An Enterprise Information System (EIS) is any kind of information system which improves the functions of enterprise business processes by integration. This means typically offering high quality of service, dealing with large volumes of data and capable of supporting some large and possibly complex organization or enterprise. An EIS must be able to be used by all parts and all levels of an enterprise.[1]\\n'}, {'link': '/wiki/Social_software', 'title': 'Social software', 'paragraph': 'Social software, also known as social apps or social platform includes communications and interactive tools that are often based on the Internet. Communication tools typically handle capturing, storing and presenting communication, usually written but increasingly including audio and video as well. Interactive tools handle mediated interactions between a pair or group of users. They focus on establishing and maintaining a connection among users, facilitating the mechanics of conversation and talk.[1] Social software generally refers to software that makes collaborative behaviour, the organisation and moulding of communities, self-expression, social interaction and feedback possible for individuals. Another element of the existing definition of social software is that it allows for the structured mediation of opinion between people, in a centralized or self-regulating manner. The most improved area for social software is that Web 2.0 applications can all promote co-operation between people and the creation of online communities more than ever before. The opportunities offered by social software are instant connections and opportunities to learn.[2]An additional defining feature of social software is that apart from interaction and collaboration, it aggregates the collective behaviour of its users, allowing not only crowds to learn from an individual but individuals to learn from the crowds as well.[3] Hence, the interactions enabled by social software can be one-to-one, one-to-many, or many-to-many.[2]\\n'}, {'link': '/wiki/Geographic_information_system', 'title': 'Geographic information system', 'paragraph': 'A geographic information system (GIS) consists of integrated computer hardware and software that store, manage, analyze, edit, output, and visualize geographic data.[1][2] Much of this often happens within a spatial database, however, this is not essential to meet the definition of a GIS.[1] In a broader sense, one may consider such a system also to include human users and support staff, procedures and workflows, the body of knowledge of relevant concepts and methods, and institutional organizations.\\n'}, {'link': '/wiki/Process_control', 'title': 'Industrial process control', 'paragraph': 'An industrial process control or simply process control in continuous production processes is a discipline that uses industrial control systems and control theory to achieve a production level of consistency, economy and safety which could not be achieved purely by human manual control. It is implemented widely in industries such as automotive, mining, dredging, oil refining, pulp and paper manufacturing, chemical processing and power generating plants.[1]\\n'}, {'link': '/wiki/Multimedia_database', 'title': 'Multimedia database', 'paragraph': 'A Multimedia database (MMDB) is a collection of related for multimedia data.[1] The multimedia data include one or more primary media data types such as text, images, graphic objects (including drawings, sketches and illustrations) animation sequences, audio and video.\\n'}, {'link': '/wiki/Digital_library', 'title': 'Digital library', 'paragraph': 'A digital library, also called an online library, an internet library, a digital repository,  a library without walls, or a digital collection, is an online database of digital objects that can include text, still images, audio, video, digital documents, or other digital media formats or a library accessible through the internet. Objects can consist of digitized content like print or photographs, as well as originally produced digital content like word processor files or social media posts. In addition to storing content, digital libraries provide means for organizing, searching, and retrieving the content contained in the collection. Digital libraries can vary immensely in size and scope, and can be maintained by individuals or organizations.[1] The digital content may be stored locally, or accessed remotely via computer networks. These information retrieval systems are able to exchange information with each other through interoperability and sustainability.[2]\\n'}, {'link': '/wiki/Computing_platform', 'title': 'Computing platform', 'paragraph': 'A computing platform, digital platform,[1] or software platform is an environment in which software is executed. It may be the hardware or the operating system (OS), a web browser and associated application programming interfaces, or other underlying software, as long as the program code is executed. Computing platforms have different abstraction levels, including a computer architecture, an OS, or runtime libraries.[2] A computing platform is the stage on which computer programs can run.\\n'}, {'link': '/wiki/Formal_methods', 'title': 'Formal methods', 'paragraph': 'In computer science, formal methods are mathematically rigorous techniques for the specification, development, analysis, and verification of software and hardware systems.[1] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.[2]\\n'}, {'link': '/wiki/Security_service_(telecommunication)', 'title': 'Security service (telecommunication)', 'paragraph': 'Security service is a service, provided by a layer of communicating open systems, which ensures adequate security of the systems or of data transfers[1] as defined by ITU-T X.800 Recommendation. \\nX.800 and ISO 7498-2 (Information processing systems – Open systems interconnection – Basic Reference Model – Part 2: Security architecture)[2]  are technically aligned. This model is widely recognized [3]\\n[4]\\n'}, {'link': '/wiki/Hardware_security', 'title': 'Hardware security', 'paragraph': 'Hardware security is a discipline originated from the cryptographic engineering and involves hardware design, access control, secure multi-party computation, secure key storage, ensuring code authenticity, measures to ensure that the supply chain that built the product is secure among other things.[1][2][3][4]\\n'}, {'link': '/wiki/Network_security', 'title': 'Network security', 'paragraph': 'Network security consists of the policies, processes and practices adopted to prevent, detect and monitor unauthorized access, misuse, modification, or denial of a computer network and network-accessible resources.[1] Network security involves the authorization of access to data in a network, which is controlled by the network administrator. Users choose or are assigned an ID and password or other authenticating information that allows them access to information and programs within their authority. Network security covers a variety of computer networks, both public and private, that are used in everyday jobs: conducting transactions and communications among businesses, government agencies and individuals. Networks can be private, such as within a company, and others which might be open to public access. Network security is involved in organizations, enterprises, and other types of institutions. It does as its title explains: it secures the network, as well as protecting and overseeing operations being done. The most common and simple way of protecting a network resource is by assigning it a unique name and a corresponding password.\\n'}, {'link': '/wiki/Information_security', 'title': 'Information security', 'paragraph': '\\nInformation security, sometimes shortened to InfoSec,[1] is the practice of protecting information by mitigating information risks. It is part of information risk management.[2][3] It typically involves preventing or reducing the probability of unauthorized or inappropriate access to data or the unlawful use, disclosure, disruption, deletion, corruption, modification, inspection, recording, or devaluation of information.[citation needed] It also involves actions intended to reduce the adverse impacts of such incidents. Protected information may take any form, e.g., electronic or physical, tangible (e.g., paperwork), or intangible (e.g., knowledge).[4][5] Information security\\'s primary focus is the balanced protection of data confidentiality, integrity, and availability (also known as the \"CIA\" triad) while maintaining a focus on efficient policy implementation, all without hampering organization productivity.[6] This is largely achieved through a structured risk management process that involves: \\n'}, {'link': '/wiki/Application_security', 'title': 'Application security', 'paragraph': 'Application security (short AppSec) includes all tasks that introduce a secure software development life cycle to development teams. Its final goal is to improve security practices and, through that, to find, fix and preferably prevent security issues within applications. It encompasses the whole application life cycle from requirements analysis, design, implementation, verification as well as maintenance.[1]\\n'}, {'link': '/wiki/Human%E2%80%93computer_interaction', 'title': 'Human–computer interaction', 'paragraph': 'Human–computer interaction (HCI) is research in the design and the use of computer technology, which focuses on the interfaces between people (users) and computers. HCI researchers observe the ways humans interact with computers and design technologies that allow humans to interact with computers in novel ways. A device that allows interaction between human being and a computer is known as a \"Human-computer Interface (HCI)\".\\n'}, {'link': '/wiki/Interaction_design', 'title': 'Interaction design', 'paragraph': '\\nInteraction design, often abbreviated as IxD, is \"the practice of designing interactive digital products, environments, systems, and services.\"[1]:\\u200axxvii,\\u200a30\\u200a While interaction design has an interest in form (similar to other design fields), its main area of focus rests on behavior.[1]:\\u200axxvii,\\u200a30\\u200a Rather than analyzing how things are, interaction design synthesizes and imagines things as they could be. This element of interaction design is what characterizes IxD as a design field, as opposed to a science or engineering field.[1]\\n'}, {'link': '/wiki/Social_computing', 'title': 'Social computing', 'paragraph': 'Social computing is an area of computer science that is concerned with the intersection of social behavior and computational systems. It is based on creating or recreating social conventions and social contexts through the use of software and technology. Thus, blogs, email, instant messaging, social network services, wikis, social bookmarking and other instances of what is often called social software illustrate ideas from social computing.   \\n'}, {'link': '/wiki/Ubiquitous_computing', 'title': 'Ubiquitous computing', 'paragraph': ' Ubiquitous computing (or \"ubicomp\") is a concept in software engineering, hardware engineering and computer science where computing is made to appear anytime and everywhere. In contrast to desktop computing, ubiquitous computing can occur using any device, in any location, and in any format. A user interacts with the computer, which can exist in many different forms, including laptop computers, tablets, smart phones and terminals in everyday objects such as a refrigerator or a pair of glasses. The underlying technologies to support ubiquitous computing include Internet, advanced middleware, operating system, mobile code, sensors, microprocessors, new I/O and user interfaces, computer networks, mobile protocols, location and positioning, and new materials.\\n'}, {'link': '/wiki/Computer_accessibility', 'title': 'Computer accessibility', 'paragraph': 'Computer accessibility (also known as accessible computing) refers to the accessibility of a computer system to all people, regardless of disability type or severity of impairment. The term accessibility is most often used in reference to specialized hardware or software, or a combination of both, designed to enable the use of a computer by a person with a disability or impairment. Computer accessibility often has direct positive effects on people with disabilities.\\n'}, {'link': '/wiki/Concurrency_(computer_science)', 'title': 'Concurrency (computer science)', 'paragraph': 'In computer science, concurrency is the ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or in partial order, without affecting the outcome.  This allows for parallel execution of the concurrent units, which can significantly improve overall speed of the execution in multi-processor and multi-core systems. In more technical terms, concurrency refers to the decomposability of a program, algorithm, or problem into order-independent or partially-ordered components or units of computation.[1]\\n'}, {'link': '/wiki/Concurrent_computing', 'title': 'Concurrent computing', 'paragraph': 'Concurrent computing is a form of computing in which several computations are executed concurrently—during overlapping time periods—instead of sequentially—with one completing before the next starts.\\n'}, {'link': '/wiki/Parallel_computing', 'title': 'Parallel computing', 'paragraph': 'Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously.[1] Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing, but has gained broader interest due to the physical constraints preventing frequency scaling.[2] As power consumption (and consequently heat generation) by computers has become a concern in recent years,[3] parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.[4]\\n'}, {'link': '/wiki/Distributed_computing', 'title': 'Distributed computing', 'paragraph': 'A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another.[1][2] Distributed computing is a field of computer science that studies distributed systems. \\n'}, {'link': '/wiki/Multithreading_(computer_architecture)', 'title': 'Multithreading (computer architecture)', 'paragraph': 'In computer architecture, multithreading is the ability of a central processing unit (CPU) (or a single core in a multi-core processor) to provide multiple threads of execution concurrently, supported by the operating system. This approach differs from multiprocessing. In a multithreaded application, the threads share the resources of a single or multiple cores, which include the computing units, the CPU caches, and the translation lookaside buffer (TLB).\\n'}, {'link': '/wiki/Multi-task_learning', 'title': 'Multi-task learning', 'paragraph': 'Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately.[1][2][3] Early versions of MTL were called \"hints\".[4][5]\\n'}, {'link': '/wiki/Computer_graphics', 'title': 'Computer graphics', 'paragraph': 'Computer graphics deals with generating images and art with the aid of computers. Today, computer graphics is a core technology in digital photography, film, video games, digital art, cell phone and computer displays, and many specialized applications. A great deal of specialized hardware and software has been developed, with the displays of most devices being driven by computer graphics hardware. It is a vast and recently developed area of computer science. The phrase was coined in 1960 by computer graphics researchers Verne Hudson and William Fetter of Boeing. It is often abbreviated as CG, or typically in the context of film as computer generated imagery (CGI). The non-artistic aspects of computer graphics are the subject of computer science research.[1]\\n'}, {'link': '/wiki/Computer_animation', 'title': 'Computer animation', 'paragraph': \"Computer animation is the process used for digitally generating animations. The more general term computer-generated imagery (CGI) encompasses both static scenes (still images) and dynamic images (moving images), while computer animation only refers to moving images. Modern computer animation usually uses 3D computer graphics. The animation's target is sometimes the computer itself, while other times it is film.\\n\"}, {'link': '/wiki/Rendering_(computer_graphics)', 'title': 'Rendering (computer graphics)', 'paragraph': 'Rendering or image synthesis is the process of generating a photorealistic or non-photorealistic image from a 2D or 3D model by means of a computer program.[citation needed]  The resulting image is referred to as the render.  Multiple models can be defined in a scene file containing objects in a strictly defined language or data structure.  The scene file contains geometry, viewpoint, texture, lighting, and shading information describing the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file. The term \"rendering\" is analogous to the concept of an artist\\'s impression of a scene.  The term \"rendering\" is also used to describe the process of calculating effects in a video editing program to produce the final video output.\\n'}, {'link': '/wiki/Mixed_reality', 'title': 'Mixed reality', 'paragraph': 'Mixed reality (MR) is a term used to describe the merging of a real-world environment and a computer-generated one. Physical and virtual objects may co-exist in mixed reality environments and interact in real time.\\n'}, {'link': '/wiki/Image_compression', 'title': 'Image compression', 'paragraph': 'Image compression is a type of data compression applied to digital images, to reduce their cost for storage or transmission. Algorithms may take advantage of visual perception and the statistical properties of image data to provide superior results compared with generic data compression methods which are used for other digital data.[1]\\n'}, {'link': '/wiki/Enterprise_software', 'title': 'Enterprise software', 'paragraph': 'Enterprise software, also known as enterprise application software (EAS), is computer software used to satisfy the needs of an organization rather than its individual users. Enterprise software is an integral part of a computer-based information system, handling a number of business operations, for example to enhance business and management reporting tasks, or support production operations and back office functions. Enterprise systems must process information at a relatively high speed.[1]\\n'}, {'link': '/wiki/Computational_mathematics', 'title': 'Computational mathematics', 'paragraph': '\\nComputational mathematics is an area of mathematics devoted to the interaction between mathematics and computer computation.[1]\\n'}, {'link': '/wiki/Computational_physics', 'title': 'Computational physics', 'paragraph': 'Computational physics is the study and implementation of numerical analysis to solve problems in physics.[1] Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science. It is sometimes regarded as a subdiscipline (or offshoot) of theoretical physics, but others consider it an intermediate branch between theoretical and experimental physics — an area of study which supplements both theory and experiment.[2]\\n'}, {'link': '/wiki/Computational_chemistry', 'title': 'Computational chemistry', 'paragraph': 'Computational chemistry is a branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into computer programs, to calculate the structures and properties of molecules, groups of molecules, and solids. It is essential because, apart from relatively recent results concerning the hydrogen molecular ion (dihydrogen cation, see references therein for more details), the quantum many-body problem cannot be solved analytically, much less in closed form.  While computational results normally complement the information obtained by chemical experiments, it can in some cases predict hitherto unobserved chemical phenomena. It is widely used in the design of new drugs and materials.[1]\\n'}, {'link': '/wiki/Computational_biology', 'title': 'Computational biology', 'paragraph': 'Computational biology refers to the use of data analysis, mathematical modeling and computational simulations to understand biological systems and relationships.[1] An intersection of computer science, biology, and big data, the field also has foundations in applied mathematics, chemistry, and genetics.[2] It differs from biological computing, a subfield of computer science and engineering which uses bioengineering to build computers.\\n'}, {'link': '/wiki/Computational_social_science', 'title': 'Computational social science', 'paragraph': 'Computational social science is the academic sub-discipline concerned with computational approaches to the social sciences. This means that computers are used to model, simulate, and analyze social phenomena.  Fields include computational economics, computational sociology, cliodynamics, culturomics, nonprofit studies,[1] and the automated analysis of contents, in social and traditional media. It focuses on investigating social and behavioral relationships and interactions through social simulation, modeling, network analysis, and media analysis.[2]\\n'}, {'link': '/wiki/Computational_engineering', 'title': 'Computational engineering', 'paragraph': 'Computational Engineering is an emerging discipline that deals with the development and application of computational models for engineering, known as Computational Engineering Models[1] or CEM. Computational engineering uses computers to solve engineering design problems important to a variety of industries.[2] At this time, various different approaches are summarized under the term Computational Engineering, including using computational geometry and virtual design for engineering tasks,[3][4] often coupled with a simulation-driven approach[5] In Computational Engineering, algorithms solve mathematical and logical models[6] that describe engineering challenges, sometimes coupled with some aspect of AI, specifically Reinforcement Learning.[7]\\n'}, {'link': '/wiki/Health_informatics', 'title': 'Health informatics', 'paragraph': '  \\n  \\n  \\n'}, {'link': '/wiki/Digital_art', 'title': 'Digital art', 'paragraph': 'Digital art refers to any artistic work or practice that uses digital technology as part of the creative or presentation process. It can also refer to computational art that uses and engages with digital media.[2]\\n'}, {'link': '/wiki/Word_processor', 'title': 'Word processor', 'paragraph': 'A word processor (WP)[1][2] is a device or computer program that provides for input, editing, formatting, and output of text, often with some additional features.\\n'}, {'link': '/wiki/Document_management_system', 'title': 'Document management system', 'paragraph': '\\nA document management system (DMS) is usually a computerized system used to store, share, track and manage files or documents. Some systems include history tracking where a log of the various versions created and modified by different users is recorded. The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems.\\n'}, {'link': '/wiki/Category:Computer_science', 'title': 'Category:Computer science', 'paragraph': 'This category has the following 25 subcategories, out of 25 total.\\n'}, {'link': '/wiki/Outline_of_computer_science', 'title': 'Outline of computer science', 'paragraph': 'Computer science (also called computing science) is the study of the theoretical foundations of information and computation and their implementation and application in computer systems. One well known subject classification system for computer science is the ACM Computing Classification System devised by the Association for Computing Machinery.\\n'}, {'link': '/wiki/Wikipedia:WikiProject_Computer_science', 'title': 'Wikipedia:WikiProject Computer science', 'paragraph': \"Welcome to the WikiProject Computer science page. The goals of the project are to build a community of interest around computer science, and to provide a focal point for coordinating efforts to improve Wikipedia's computer science articles. The scope of the project includes all articles in the area of computer science, including computer programming and software engineering. \\n\"}, {'link': '/wiki/Category:Cybernetics', 'title': 'Category:Cybernetics', 'paragraph': 'Cybernetics is a transdisciplinary approach for exploring regulatory systems with feedback, their structures, constraints, and possibilities. Cybernetics is relevant to the study of systems, such as mechanical, physical, biological, cognitive, and social.\\n'}, {'link': '/wiki/Category:Learning', 'title': 'Category:Learning', 'paragraph': 'Learning is the process of acquiring new or modifying existing knowledge, behaviors, skills, values, or preferences based on instruction.\\n'}, {'link': '/wiki/Category:Webarchive_template_wayback_links', 'title': 'Category:Webarchive template wayback links', 'paragraph': 'The following 200 pages are in this category, out of approximately 507,488 total. This list may not reflect recent changes.\\n'}, {'link': '/wiki/Category:Articles_with_short_description', 'title': 'Category:Articles with short description', 'paragraph': 'This category is for articles with short descriptions defined on Wikipedia by {{short description}} (either within the page itself or via another template).\\n'}, {'link': '/wiki/Category:Short_description_is_different_from_Wikidata', 'title': 'Category:Short description is different from Wikidata', 'paragraph': 'This category contains articles with short descriptions that do not match the description field on Wikidata. \\n'}, {'link': '/wiki/Category:All_articles_with_unsourced_statements', 'title': 'Category:All articles with unsourced statements', 'paragraph': '\\nThis is a category to help keep count of the total number of articles with the {{citation needed}} template.  They should all be in one of the dated categories, which can be found at Category:Articles with unsourced statements.\\n'}, {'link': '/wiki/Category:Articles_with_unsourced_statements_from_May_2022', 'title': 'Category:Articles with unsourced statements from May 2022', 'paragraph': 'This category combines all articles with unsourced statements from May 2022 (2022-05) to enable us to work through the backlog more systematically. It is a member of Category:Articles with unsourced statements.\\nTo add an article to this category add     {{Citation needed|date=May 2022}} to the article. If you omit the date a bot will add it for you at some point.\\n'}, {'link': '/wiki/Category:Articles_with_GND_identifiers', 'title': 'Category:Articles with GND identifiers', 'paragraph': 'This category has only the following subcategory.\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of dictionaries containing the links, titles, and paragraphs of the articles\n",
    "for link in http_links:\n",
    "    wiki_dict = {}\n",
    "    requete = requests.get(\"https://en.wikipedia.org\" + link)\n",
    "    page = BeautifulSoup(requete.text, 'html.parser')\n",
    "    h_1 = page.find('h1')\n",
    "    p_1 = page.find('p')\n",
    "    if p_1 is not None and p_1.text not in wiki_dict_sans_doublon: # We filter the duplicates\n",
    "        wiki_dict_sans_doublon.append(p_1.text)\n",
    "        wiki_dict[\"link\"] = link\n",
    "        wiki_dict[\"title\"] = h_1.text\n",
    "        wiki_dict[\"paragraph\"] = p_1.text\n",
    "        wiki_list.append(wiki_dict)\n",
    "print(wiki_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAI client\n",
    "client = OpenAI()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding of the paragraph and title for the wiki_main article\n",
    "response = client.embeddings.create(\n",
    "    input=wiki_main['title'] + wiki_main['paragraph'],\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "wiki_main[\"embeddings\"] = response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'link': 'https://en.wikipedia.org/wiki/machine_learning',\n",
       " 'title': 'Machine learning',\n",
       " 'paragraph': 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\\n',\n",
       " 'embeddings': [-0.03994451463222504,\n",
       "  -0.004929579794406891,\n",
       "  0.024746298789978027,\n",
       "  -0.0065325661562383175,\n",
       "  -0.0010125794215127826,\n",
       "  0.0012101753382012248,\n",
       "  0.0052120862528681755,\n",
       "  0.023260759189724922,\n",
       "  -0.023057609796524048,\n",
       "  -0.027272986248135567,\n",
       "  0.01749635674059391,\n",
       "  0.028898192569613457,\n",
       "  -0.016836117953062057,\n",
       "  -0.006761110387742519,\n",
       "  -0.004507407080382109,\n",
       "  0.006843640469014645,\n",
       "  0.025812841951847076,\n",
       "  0.005278745200484991,\n",
       "  0.004555020481348038,\n",
       "  -0.003945568110793829,\n",
       "  -0.021343525499105453,\n",
       "  0.028237953782081604,\n",
       "  -0.006824595388025045,\n",
       "  -0.04845145344734192,\n",
       "  -0.014956973493099213,\n",
       "  0.008906890638172626,\n",
       "  0.01682342030107975,\n",
       "  -0.043652016669511795,\n",
       "  -0.00922431331127882,\n",
       "  0.0005880261887796223,\n",
       "  0.019438985735177994,\n",
       "  -0.013014344498515129,\n",
       "  -0.0013498414773494005,\n",
       "  -0.02119116112589836,\n",
       "  -0.010932049714028835,\n",
       "  -0.005056548863649368,\n",
       "  0.03402774780988693,\n",
       "  -0.004837526939809322,\n",
       "  -0.007053139619529247,\n",
       "  0.0036217968445271254,\n",
       "  0.03072655014693737,\n",
       "  0.02646038308739662,\n",
       "  0.0028107808902859688,\n",
       "  -0.018664473667740822,\n",
       "  0.005967553239315748,\n",
       "  0.021889492869377136,\n",
       "  0.017636023461818695,\n",
       "  -0.019210441038012505,\n",
       "  -0.017001178115606308,\n",
       "  0.025558901950716972,\n",
       "  0.0215847659856081,\n",
       "  0.03641476854681969,\n",
       "  -0.0011776394676417112,\n",
       "  -0.011090761050581932,\n",
       "  0.0008379968348890543,\n",
       "  0.014309430494904518,\n",
       "  -0.021965673193335533,\n",
       "  0.0063008470460772514,\n",
       "  0.029075950384140015,\n",
       "  -0.017534447833895683,\n",
       "  -0.009700448252260685,\n",
       "  0.0004439954645931721,\n",
       "  -0.002820303663611412,\n",
       "  -0.0035075244959443808,\n",
       "  0.004834352992475033,\n",
       "  0.0055009410716593266,\n",
       "  -0.03207242488861084,\n",
       "  0.034180112183094025,\n",
       "  0.00479308795183897,\n",
       "  -0.0019950037822127342,\n",
       "  0.02376863732933998,\n",
       "  0.013433342799544334,\n",
       "  -0.003659887472167611,\n",
       "  0.011439925990998745,\n",
       "  0.03768446296453476,\n",
       "  -0.003980484791100025,\n",
       "  -0.0023648017086088657,\n",
       "  -0.001314131310209632,\n",
       "  -0.010341642424464226,\n",
       "  0.020315073430538177,\n",
       "  -0.007249942049384117,\n",
       "  -0.0029028337448835373,\n",
       "  0.009154479950666428,\n",
       "  -0.0009165590163320303,\n",
       "  0.014004704542458057,\n",
       "  -0.009967083111405373,\n",
       "  -0.0074975318275392056,\n",
       "  -0.013192101381719112,\n",
       "  0.004482013173401356,\n",
       "  -0.008176817558705807,\n",
       "  -0.016848813742399216,\n",
       "  -0.01477921660989523,\n",
       "  0.012303316965699196,\n",
       "  0.011770046316087246,\n",
       "  -0.007980015128850937,\n",
       "  0.01672184467315674,\n",
       "  -0.008303786627948284,\n",
       "  -0.000282308115856722,\n",
       "  0.000941159320063889,\n",
       "  -0.039385851472616196,\n",
       "  -0.020315073430538177,\n",
       "  0.031082063913345337,\n",
       "  -0.015236306004226208,\n",
       "  -0.007770515978336334,\n",
       "  -0.02248624712228775,\n",
       "  0.008005408570170403,\n",
       "  -0.0035202214494347572,\n",
       "  0.017064662650227547,\n",
       "  0.03293581306934357,\n",
       "  -0.0006050876691006124,\n",
       "  0.015858454629778862,\n",
       "  0.027171412482857704,\n",
       "  0.0013815837446600199,\n",
       "  -0.0364401638507843,\n",
       "  0.00300599611364305,\n",
       "  -0.0088624507188797,\n",
       "  0.014144370332360268,\n",
       "  -0.027399955317378044,\n",
       "  -0.00815777201205492,\n",
       "  -0.011528804898262024,\n",
       "  0.010690807364881039,\n",
       "  0.013496827334165573,\n",
       "  0.02793322689831257,\n",
       "  -0.00024560606107115746,\n",
       "  0.014461793005466461,\n",
       "  0.0002319172053830698,\n",
       "  -0.009573478251695633,\n",
       "  -0.025762053206562996,\n",
       "  -0.00905290525406599,\n",
       "  -0.009414766915142536,\n",
       "  -0.00064317841315642,\n",
       "  -0.0029282274190336466,\n",
       "  -0.0106273228302598,\n",
       "  0.03369762748479843,\n",
       "  -0.024695511907339096,\n",
       "  0.001993416575714946,\n",
       "  -0.0018045499455183744,\n",
       "  -0.001755349338054657,\n",
       "  -0.01188431866466999,\n",
       "  -0.011141548864543438,\n",
       "  -0.011922408826649189,\n",
       "  -0.000494386360514909,\n",
       "  -0.0032758056186139584,\n",
       "  -0.009973431937396526,\n",
       "  -0.03974136337637901,\n",
       "  0.010913004167377949,\n",
       "  -0.0017394782043993473,\n",
       "  0.007630849722772837,\n",
       "  0.0061548324301838875,\n",
       "  0.0019283449510112405,\n",
       "  0.0023473433684557676,\n",
       "  -0.03207242488861084,\n",
       "  -0.0064595588482916355,\n",
       "  0.02686668559908867,\n",
       "  0.01603621244430542,\n",
       "  0.004348695743829012,\n",
       "  0.0028234778437763453,\n",
       "  0.008545028045773506,\n",
       "  -0.0045454977080225945,\n",
       "  0.0022283096332103014,\n",
       "  0.01185892429202795,\n",
       "  -0.0018204210791736841,\n",
       "  0.0024536801502108574,\n",
       "  -0.0004086821572855115,\n",
       "  0.0004733570967800915,\n",
       "  0.016061605885624886,\n",
       "  0.002534623024985194,\n",
       "  -0.0048248302191495895,\n",
       "  -0.006710323039442301,\n",
       "  -0.013598402962088585,\n",
       "  -0.002561603905633092,\n",
       "  0.027882440015673637,\n",
       "  -0.025368448346853256,\n",
       "  0.014144370332360268,\n",
       "  -0.007605455815792084,\n",
       "  -0.011871621012687683,\n",
       "  0.019210441038012505,\n",
       "  -0.0011808136478066444,\n",
       "  -0.024492360651493073,\n",
       "  0.0013101635267958045,\n",
       "  0.013407949358224869,\n",
       "  -0.0006844433955848217,\n",
       "  -0.008075241930782795,\n",
       "  0.012462028302252293,\n",
       "  -0.010474960319697857,\n",
       "  0.0019902423955500126,\n",
       "  0.008424406871199608,\n",
       "  -0.007630849722772837,\n",
       "  0.0007788767688907683,\n",
       "  -0.011992242187261581,\n",
       "  0.019312016665935516,\n",
       "  0.007745122071355581,\n",
       "  -0.00762450136244297,\n",
       "  -0.007834000512957573,\n",
       "  -0.6452067494392395,\n",
       "  -0.021521281450986862,\n",
       "  0.011471668258309364,\n",
       "  0.015744183212518692,\n",
       "  -0.0005193834658712149,\n",
       "  0.0006050876691006124,\n",
       "  0.007897485047578812,\n",
       "  -0.017978841438889503,\n",
       "  0.008773572742938995,\n",
       "  0.026815898716449738,\n",
       "  -0.0013895193114876747,\n",
       "  0.017204327508807182,\n",
       "  0.001264137215912342,\n",
       "  3.523891791701317e-05,\n",
       "  0.012208090163767338,\n",
       "  -0.03245333209633827,\n",
       "  -0.004367740824818611,\n",
       "  -0.0587613508105278,\n",
       "  -0.007751470431685448,\n",
       "  0.0035265698097646236,\n",
       "  -0.03062497451901436,\n",
       "  0.02831413410604,\n",
       "  -0.02310839667916298,\n",
       "  0.004551846068352461,\n",
       "  -0.04720715433359146,\n",
       "  0.015261699445545673,\n",
       "  0.01933741196990013,\n",
       "  0.028695041313767433,\n",
       "  0.006875382736325264,\n",
       "  0.01620127074420452,\n",
       "  -0.015947332605719566,\n",
       "  0.009021162986755371,\n",
       "  0.007954620756208897,\n",
       "  -0.009516342543065548,\n",
       "  0.027272986248135567,\n",
       "  -0.005281919147819281,\n",
       "  -0.007230896502733231,\n",
       "  0.02899976819753647,\n",
       "  0.012887375429272652,\n",
       "  0.017674114555120468,\n",
       "  -0.016963087022304535,\n",
       "  -0.022118037566542625,\n",
       "  -0.014296733774244785,\n",
       "  -0.008265695534646511,\n",
       "  0.0011919235112145543,\n",
       "  0.0013252411736175418,\n",
       "  0.015769576653838158,\n",
       "  0.025558901950716972,\n",
       "  -0.014982366934418678,\n",
       "  -0.039842940866947174,\n",
       "  -0.007383259944617748,\n",
       "  0.009110040962696075,\n",
       "  -1.1401190931792371e-05,\n",
       "  -0.011205033399164677,\n",
       "  -0.013636493124067783,\n",
       "  0.02169903926551342,\n",
       "  0.020822951570153236,\n",
       "  -0.011846227571368217,\n",
       "  0.019489774480462074,\n",
       "  -0.025419237092137337,\n",
       "  0.019756410270929337,\n",
       "  -0.014144370332360268,\n",
       "  -0.024695511907339096,\n",
       "  -0.005900894291698933,\n",
       "  -0.01810581050813198,\n",
       "  0.01531248725950718,\n",
       "  -0.020175408571958542,\n",
       "  -0.009465554729104042,\n",
       "  -0.008557724766433239,\n",
       "  -0.026155658066272736,\n",
       "  0.01149071380496025,\n",
       "  0.003618622664362192,\n",
       "  0.002598107559606433,\n",
       "  0.00409475713968277,\n",
       "  0.0003910255036316812,\n",
       "  0.04113802686333656,\n",
       "  0.006983306724578142,\n",
       "  -0.0011863686377182603,\n",
       "  -0.02068328484892845,\n",
       "  -0.012131907977163792,\n",
       "  0.018423233181238174,\n",
       "  -0.008868799544870853,\n",
       "  0.0009205267997458577,\n",
       "  -0.022194217890501022,\n",
       "  0.02686668559908867,\n",
       "  -0.01687420904636383,\n",
       "  0.016988480463624,\n",
       "  -0.0007550700684078038,\n",
       "  -0.0007856219890527427,\n",
       "  0.002334646414965391,\n",
       "  0.03174230456352234,\n",
       "  0.03412932530045509,\n",
       "  -0.02008652873337269,\n",
       "  -0.07521656155586243,\n",
       "  -0.012690572999417782,\n",
       "  0.0022600519005209208,\n",
       "  0.007554668001830578,\n",
       "  -0.006976958364248276,\n",
       "  -0.008335528895258904,\n",
       "  -0.01962944120168686,\n",
       "  -0.01205572672188282,\n",
       "  -0.01255725510418415,\n",
       "  0.027272986248135567,\n",
       "  0.012576300650835037,\n",
       "  0.0034853050019592047,\n",
       "  -0.006231014151126146,\n",
       "  0.01884223148226738,\n",
       "  0.01733129844069481,\n",
       "  0.02859346754848957,\n",
       "  -0.027349168434739113,\n",
       "  0.004136022180318832,\n",
       "  0.020162710919976234,\n",
       "  0.0006027069757692516,\n",
       "  0.017013873904943466,\n",
       "  0.001645838376134634,\n",
       "  -0.02350200153887272,\n",
       "  -0.012176347896456718,\n",
       "  0.0027599933091551065,\n",
       "  0.010525748133659363,\n",
       "  -0.031209032982587814,\n",
       "  0.039563607424497604,\n",
       "  -0.0034281688276678324,\n",
       "  0.013433342799544334,\n",
       "  -0.015198214910924435,\n",
       "  -0.017674114555120468,\n",
       "  -0.013115920126438141,\n",
       "  -0.011281214654445648,\n",
       "  -0.010747944004833698,\n",
       "  0.000705075915902853,\n",
       "  0.0062278397381305695,\n",
       "  -0.00622466579079628,\n",
       "  -0.005513638257980347,\n",
       "  -0.001547437277622521,\n",
       "  -0.006938867270946503,\n",
       "  0.02091182954609394,\n",
       "  -0.013941220007836819,\n",
       "  0.007592759095132351,\n",
       "  0.012436634860932827,\n",
       "  -0.005050200503319502,\n",
       "  0.003685281379148364,\n",
       "  -0.015236306004226208,\n",
       "  -0.00899576861411333,\n",
       "  0.019921470433473587,\n",
       "  0.024365391582250595,\n",
       "  -0.016112392768263817,\n",
       "  -0.009014814160764217,\n",
       "  0.0019172350876033306,\n",
       "  -0.01292546559125185,\n",
       "  -0.013306373730301857,\n",
       "  -0.020289679989218712,\n",
       "  -0.013534918427467346,\n",
       "  -0.0014903011033311486,\n",
       "  -0.013611099682748318,\n",
       "  0.04479473829269409,\n",
       "  -0.006281801965087652,\n",
       "  -0.008843406103551388,\n",
       "  -0.02074676938354969,\n",
       "  -0.01315401028841734,\n",
       "  -0.023527394980192184,\n",
       "  -0.03379920497536659,\n",
       "  -0.011446274816989899,\n",
       "  0.019362805411219597,\n",
       "  -0.014512580819427967,\n",
       "  0.013738068751990795,\n",
       "  -0.003786856774240732,\n",
       "  -0.014842701144516468,\n",
       "  -0.018131203949451447,\n",
       "  0.030650367960333824,\n",
       "  -0.03605925664305687,\n",
       "  -0.04299177601933479,\n",
       "  0.03141218423843384,\n",
       "  -0.02849189192056656,\n",
       "  0.015858454629778862,\n",
       "  0.02673971652984619,\n",
       "  -0.009014814160764217,\n",
       "  0.027222199365496635,\n",
       "  -0.004770867992192507,\n",
       "  0.0012744534760713577,\n",
       "  -0.010786035098135471,\n",
       "  -0.01990877278149128,\n",
       "  -0.011497062630951405,\n",
       "  0.02378133311867714,\n",
       "  0.013027041219174862,\n",
       "  0.002109276130795479,\n",
       "  0.024543149396777153,\n",
       "  0.0034218202345073223,\n",
       "  0.04575970396399498,\n",
       "  0.009871856309473515,\n",
       "  -0.014982366934418678,\n",
       "  0.009611569344997406,\n",
       "  0.00658970233052969,\n",
       "  0.02966000884771347,\n",
       "  -0.019273927435278893,\n",
       "  0.0011332002468407154,\n",
       "  0.006132612936198711,\n",
       "  0.00959252379834652,\n",
       "  -0.0029456857591867447,\n",
       "  -0.009103692136704922,\n",
       "  0.009998825378715992,\n",
       "  0.029075950384140015,\n",
       "  0.0073769111186265945,\n",
       "  0.010360687971115112,\n",
       "  0.009014814160764217,\n",
       "  0.005262874066829681,\n",
       "  -0.006761110387742519,\n",
       "  -0.018931109458208084,\n",
       "  0.013953916728496552,\n",
       "  -0.037709854543209076,\n",
       "  0.0010451152920722961,\n",
       "  -0.009979779832065105,\n",
       "  0.016747239977121353,\n",
       "  -0.00942111574113369,\n",
       "  -0.005126381758600473,\n",
       "  -0.03781143203377724,\n",
       "  0.002131495624780655,\n",
       "  0.025774750858545303,\n",
       "  -0.007059488445520401,\n",
       "  0.017369387671351433,\n",
       "  -0.028568072244524956,\n",
       "  0.004885140340775251,\n",
       "  0.002758406102657318,\n",
       "  0.02337503246963024,\n",
       "  0.022575126960873604,\n",
       "  -0.016975784674286842,\n",
       "  -0.006469081621617079,\n",
       "  0.03811615705490112,\n",
       "  0.00240447954274714,\n",
       "  0.018550202250480652,\n",
       "  -0.012322362512350082,\n",
       "  -0.05226052924990654,\n",
       "  -0.007929227314889431,\n",
       "  0.0145252775400877,\n",
       "  0.0081260297447443,\n",
       "  0.008341877721250057,\n",
       "  0.020734071731567383,\n",
       "  0.01956595666706562,\n",
       "  0.008710088208317757,\n",
       "  -0.016924995929002762,\n",
       "  0.019375501200556755,\n",
       "  -0.015007761307060719,\n",
       "  -0.023197274655103683,\n",
       "  -0.020480133593082428,\n",
       "  0.011592289432883263,\n",
       "  -0.02282906509935856,\n",
       "  0.021394312381744385,\n",
       "  -9.944268094841391e-05,\n",
       "  0.028161771595478058,\n",
       "  -0.013573008589446545,\n",
       "  -0.016074301674962044,\n",
       "  0.004234423395246267,\n",
       "  -0.028415709733963013,\n",
       "  0.007364214397966862,\n",
       "  -0.024492360651493073,\n",
       "  -0.0020537269301712513,\n",
       "  -0.0016053669387474656,\n",
       "  -0.03958899900317192,\n",
       "  0.008646603673696518,\n",
       "  -0.030040916055440903,\n",
       "  0.024327300488948822,\n",
       "  0.01631554402410984,\n",
       "  -0.004053492099046707,\n",
       "  0.016671057790517807,\n",
       "  0.01716623827815056,\n",
       "  -0.015375971794128418,\n",
       "  0.01045591477304697,\n",
       "  -0.0032377149909734726,\n",
       "  0.023476608097553253,\n",
       "  -0.01166212186217308,\n",
       "  -0.0001307386119151488,\n",
       "  -0.0030663064680993557,\n",
       "  -0.010252763517200947,\n",
       "  -0.02102610096335411,\n",
       "  0.0081260297447443,\n",
       "  -0.020848345011472702,\n",
       "  0.03407853841781616,\n",
       "  -0.00762450136244297,\n",
       "  0.021610159426927567,\n",
       "  0.01084951963275671,\n",
       "  0.02416224218904972,\n",
       "  0.028644254431128502,\n",
       "  -0.03214860334992409,\n",
       "  -0.04050317779183388,\n",
       "  -0.004504232667386532,\n",
       "  0.017191631719470024,\n",
       "  -0.0017664592014625669,\n",
       "  -0.006173877976834774,\n",
       "  -0.028745830059051514,\n",
       "  0.0018918412970378995,\n",
       "  -0.021940279752016068,\n",
       "  -0.025977900251746178,\n",
       "  -0.010157536715269089,\n",
       "  0.027069836854934692,\n",
       "  -0.009148132055997849,\n",
       "  -0.0028155422769486904,\n",
       "  -0.0031075715087354183,\n",
       "  -0.0017251941608265042,\n",
       "  0.03781143203377724,\n",
       "  0.01721702516078949,\n",
       "  -0.0018489891663193703,\n",
       "  -0.010017870925366879,\n",
       "  -0.0022727488540112972,\n",
       "  0.01477921660989523,\n",
       "  -0.008399013429880142,\n",
       "  -0.021102283149957657,\n",
       "  0.03095509484410286,\n",
       "  -0.003393252147361636,\n",
       "  -0.01317940466105938,\n",
       "  0.009459206834435463,\n",
       "  0.01112885121256113,\n",
       "  -0.019997650757431984,\n",
       "  -5.11844627908431e-05,\n",
       "  0.007357866037636995,\n",
       "  0.03946202993392944,\n",
       "  0.013509524054825306,\n",
       "  0.015845756977796555,\n",
       "  0.01828356646001339,\n",
       "  0.005894545931369066,\n",
       "  0.02461932972073555,\n",
       "  0.007757818792015314,\n",
       "  0.015020458027720451,\n",
       "  0.01582036353647709,\n",
       "  -0.022016461938619614,\n",
       "  0.02567317523062229,\n",
       "  0.023286154493689537,\n",
       "  0.06566847860813141,\n",
       "  0.0065325661562383175,\n",
       "  -0.028948981314897537,\n",
       "  0.01315401028841734,\n",
       "  -0.03318975120782852,\n",
       "  0.004710557870566845,\n",
       "  -0.026511171832680702,\n",
       "  -0.003083764808252454,\n",
       "  0.010360687971115112,\n",
       "  0.008792618289589882,\n",
       "  -0.03458641469478607,\n",
       "  0.029228312894701958,\n",
       "  0.024073362350463867,\n",
       "  0.005739008542150259,\n",
       "  0.01225252915173769,\n",
       "  0.02793322689831257,\n",
       "  0.00088481669081375,\n",
       "  -0.010551141574978828,\n",
       "  -0.01565530337393284,\n",
       "  -0.021940279752016068,\n",
       "  0.015591819770634174,\n",
       "  -0.008900541812181473,\n",
       "  0.019273927435278893,\n",
       "  -0.009979779832065105,\n",
       "  0.014956973493099213,\n",
       "  -0.018715262413024902,\n",
       "  0.022740185260772705,\n",
       "  0.023527394980192184,\n",
       "  -0.0258509311825037,\n",
       "  -0.016328241676092148,\n",
       "  0.00512955617159605,\n",
       "  0.017077358439564705,\n",
       "  -0.00981472060084343,\n",
       "  0.005507289431989193,\n",
       "  -0.0191215630620718,\n",
       "  0.04603903740644455,\n",
       "  0.003958265297114849,\n",
       "  0.01659487560391426,\n",
       "  -0.002131495624780655,\n",
       "  -0.015426759608089924,\n",
       "  0.03405314311385155,\n",
       "  0.030040916055440903,\n",
       "  -0.0072816843166947365,\n",
       "  0.0010847932426258922,\n",
       "  -0.008532331325113773,\n",
       "  -0.027120623737573624,\n",
       "  0.017915356904268265,\n",
       "  0.027958620339632034,\n",
       "  -0.006450036074966192,\n",
       "  -0.03235175460577011,\n",
       "  0.008729133754968643,\n",
       "  -0.000491608923766762,\n",
       "  -0.030701154842972755,\n",
       "  0.008989420719444752,\n",
       "  0.01189066655933857,\n",
       "  0.027679288759827614,\n",
       "  0.011954151093959808,\n",
       "  3.863633537548594e-05,\n",
       "  0.0010681285057216883,\n",
       "  -0.0367702841758728,\n",
       "  -0.01728050969541073,\n",
       "  -0.033342115581035614,\n",
       "  0.008411710150539875,\n",
       "  0.0006979338941164315,\n",
       "  0.01716623827815056,\n",
       "  -0.007002352271229029,\n",
       "  -0.009636963717639446,\n",
       "  -0.0005487450398504734,\n",
       "  -0.03359605371952057,\n",
       "  0.010043264366686344,\n",
       "  0.004958147648721933,\n",
       "  -0.05182883143424988,\n",
       "  -0.02752692624926567,\n",
       "  -0.004710557870566845,\n",
       "  0.0004098725039511919,\n",
       "  0.011566895060241222,\n",
       "  0.015744183212518692,\n",
       "  -0.012608042918145657,\n",
       "  -0.028466498479247093,\n",
       "  0.03684646636247635,\n",
       "  -0.013865037821233273,\n",
       "  -0.014868094585835934,\n",
       "  -0.02836492285132408,\n",
       "  -0.023959090933203697,\n",
       "  0.00865930039435625,\n",
       "  -0.005412062630057335,\n",
       "  -0.023032214492559433,\n",
       "  -0.008983071893453598,\n",
       "  -0.00804984848946333,\n",
       "  0.002371150068938732,\n",
       "  0.0007673701620660722,\n",
       "  -0.015591819770634174,\n",
       "  0.013928522355854511,\n",
       "  0.029177526012063026,\n",
       "  -0.01281754206866026,\n",
       "  0.00709123071283102,\n",
       "  0.013382554985582829,\n",
       "  0.015058549121022224,\n",
       "  0.004139196127653122,\n",
       "  -0.022168824449181557,\n",
       "  0.005516812205314636,\n",
       "  -0.010951094329357147,\n",
       "  -0.027095230296254158,\n",
       "  -0.025165298953652382,\n",
       "  0.0011958912946283817,\n",
       "  0.007313426584005356,\n",
       "  0.009014814160764217,\n",
       "  0.019223138689994812,\n",
       "  -0.002534623024985194,\n",
       "  -0.006970610003918409,\n",
       "  0.02383212186396122,\n",
       "  0.006684929132461548,\n",
       "  0.0400460883975029,\n",
       "  -0.008754527196288109,\n",
       "  0.021153070032596588,\n",
       "  0.020391255617141724,\n",
       "  0.036135438829660416,\n",
       "  0.01766141690313816,\n",
       "  0.028466498479247093,\n",
       "  -0.01205572672188282,\n",
       "  -0.0024425701703876257,\n",
       "  -0.01441100612282753,\n",
       "  0.019896075129508972,\n",
       "  0.011573243886232376,\n",
       "  -0.02001034840941429,\n",
       "  -0.005427933763712645,\n",
       "  0.019502470269799232,\n",
       "  -0.00445979367941618,\n",
       "  -0.023235365748405457,\n",
       "  0.003039325587451458,\n",
       "  -0.005637432914227247,\n",
       "  0.02310839667916298,\n",
       "  -0.016404422000050545,\n",
       "  -0.007268987596035004,\n",
       "  -0.03537362441420555,\n",
       "  -0.023083003237843513,\n",
       "  0.000957823998760432,\n",
       "  0.016048908233642578,\n",
       "  -0.026561958715319633,\n",
       "  -0.022676700726151466,\n",
       "  0.008284741081297398,\n",
       "  0.0152236083522439,\n",
       "  0.015134730376303196,\n",
       "  -0.005437456537038088,\n",
       "  0.03161533549427986,\n",
       "  -0.024492360651493073,\n",
       "  0.0015720375813543797,\n",
       "  0.013395251706242561,\n",
       "  -0.004199506714940071,\n",
       "  0.04598825052380562,\n",
       "  -0.011376441456377506,\n",
       "  0.040630146861076355,\n",
       "  -0.024771694093942642,\n",
       "  -0.02130543440580368,\n",
       "  -0.002885375404730439,\n",
       "  0.010735247284173965,\n",
       "  -0.006250059697777033,\n",
       "  0.00709123071283102,\n",
       "  0.035195864737033844,\n",
       "  0.03011709824204445,\n",
       "  0.03202163428068161,\n",
       "  -0.00998612865805626,\n",
       "  0.030193278566002846,\n",
       "  0.0025092291180044413,\n",
       "  -0.022283097729086876,\n",
       "  -0.015515637584030628,\n",
       "  -0.008640254847705364,\n",
       "  0.007268987596035004,\n",
       "  0.0006281008245423436,\n",
       "  -0.0014109454350546002,\n",
       "  0.0173186007887125,\n",
       "  -0.005107336677610874,\n",
       "  0.02747613750398159,\n",
       "  -0.020772162824869156,\n",
       "  0.010551141574978828,\n",
       "  0.017242418602108955,\n",
       "  0.014601459726691246,\n",
       "  -0.02012461982667446,\n",
       "  -0.02836492285132408,\n",
       "  -0.02483517862856388,\n",
       "  -0.013280979357659817,\n",
       "  0.01895650289952755,\n",
       "  0.004901011474430561,\n",
       "  0.04410910606384277,\n",
       "  -0.023819424211978912,\n",
       "  -0.013547615148127079,\n",
       "  0.03486574441194534,\n",
       "  0.007338820490986109,\n",
       "  -0.007484835106879473,\n",
       "  0.019946863874793053,\n",
       "  0.01415706705302,\n",
       "  -0.005383494775742292,\n",
       "  -0.01956595666706562,\n",
       "  0.017597932368516922,\n",
       "  0.011420880444347858,\n",
       "  -0.03415471687912941,\n",
       "  0.010100401006639004,\n",
       "  -0.04111263155937195,\n",
       "  -0.008532331325113773,\n",
       "  0.013877734541893005,\n",
       "  -0.002693334361538291,\n",
       "  0.015579122118651867,\n",
       "  0.01569339446723461,\n",
       "  -0.009211616590619087,\n",
       "  -0.012969905510544777,\n",
       "  0.012760406360030174,\n",
       "  -0.013763463124632835,\n",
       "  0.00012538209557533264,\n",
       "  0.013903128914535046,\n",
       "  -0.031082063913345337,\n",
       "  -0.012354104779660702,\n",
       "  -0.03303739055991173,\n",
       "  0.011801788583397865,\n",
       "  0.0008665648638270795,\n",
       "  0.020772162824869156,\n",
       "  0.01844862662255764,\n",
       "  0.013039737939834595,\n",
       "  0.0009800436673685908,\n",
       "  -0.044718556106090546,\n",
       "  -0.024809783324599266,\n",
       "  0.023476608097553253,\n",
       "  -2.086808490275871e-05,\n",
       "  0.03638937696814537,\n",
       "  -0.007065836805850267,\n",
       "  0.04441383108496666,\n",
       "  0.016163181513547897,\n",
       "  0.013293677009642124,\n",
       "  -0.0008491066400893033,\n",
       "  0.02260052040219307,\n",
       "  0.013204798102378845,\n",
       "  0.00015950507076922804,\n",
       "  0.019438985735177994,\n",
       "  -0.0014823655365034938,\n",
       "  0.006970610003918409,\n",
       "  -0.020899131894111633,\n",
       "  0.008684693835675716,\n",
       "  0.002339407801628113,\n",
       "  0.003336115973070264,\n",
       "  -0.03425629436969757,\n",
       "  0.01861368678510189,\n",
       "  0.01823277957737446,\n",
       "  -0.0013982484815642238,\n",
       "  -0.023184578865766525,\n",
       "  -0.008303786627948284,\n",
       "  -0.008164120838046074,\n",
       "  0.035246655344963074,\n",
       "  -0.0025743008591234684,\n",
       "  0.00514542730525136,\n",
       "  -0.0055263349786400795,\n",
       "  -0.01582036353647709,\n",
       "  -0.012290620245039463,\n",
       "  0.014614156447350979,\n",
       "  0.0067484136670827866,\n",
       "  0.013242889195680618,\n",
       "  -0.009008465334773064,\n",
       "  -0.0035995771177113056,\n",
       "  0.001645838376134634,\n",
       "  -0.006932518910616636,\n",
       "  -0.005205737892538309,\n",
       "  -0.0004638344107661396,\n",
       "  -0.00654526287689805,\n",
       "  0.031132850795984268,\n",
       "  0.004926405381411314,\n",
       "  0.0022346582263708115,\n",
       "  -0.0025679522659629583,\n",
       "  0.008964026346802711,\n",
       "  0.0032123210839927197,\n",
       "  -0.010157536715269089,\n",
       "  -0.00981472060084343,\n",
       "  0.029583826661109924,\n",
       "  -0.013928522355854511,\n",
       "  -0.01766141690313816,\n",
       "  0.0006689690635539591,\n",
       "  0.014868094585835934,\n",
       "  0.010138492099940777,\n",
       "  -0.011744651943445206,\n",
       "  -0.008399013429880142,\n",
       "  -0.009871856309473515,\n",
       "  -0.007567365188151598,\n",
       "  -0.011820834130048752,\n",
       "  0.01088126190006733,\n",
       "  0.016671057790517807,\n",
       "  -0.01441100612282753,\n",
       "  0.023286154493689537,\n",
       "  0.002204502932727337,\n",
       "  -0.02007383294403553,\n",
       "  -0.00031563753145746887,\n",
       "  -0.007319774944335222,\n",
       "  0.02120385877788067,\n",
       "  -0.011205033399164677,\n",
       "  -0.011643077246844769,\n",
       "  0.012912768870592117,\n",
       "  -0.02159746363759041,\n",
       "  0.002923466032370925,\n",
       "  0.010779686272144318,\n",
       "  -0.010906655341386795,\n",
       "  -0.0033805551938712597,\n",
       "  0.021508583799004555,\n",
       "  -0.0032202566508203745,\n",
       "  0.006237362511456013,\n",
       "  0.0013712674845010042,\n",
       "  -0.004542323760688305,\n",
       "  -0.010201976634562016,\n",
       "  0.007154715247452259,\n",
       "  -0.014893488958477974,\n",
       "  -0.02472090534865856,\n",
       "  0.003812250681221485,\n",
       "  -0.028644254431128502,\n",
       "  -0.007916530594229698,\n",
       "  -0.018626384437084198,\n",
       "  -0.021051496267318726,\n",
       "  -0.002547319745644927,\n",
       "  -0.023514699190855026,\n",
       "  -0.010887609794735909,\n",
       "  -0.002317188074812293,\n",
       "  0.006046908907592297,\n",
       "  -0.007592759095132351,\n",
       "  0.00443122535943985,\n",
       "  0.009744887240231037,\n",
       "  0.006024688947945833,\n",
       "  -0.008106984198093414,\n",
       "  -0.009275101125240326,\n",
       "  0.012030333280563354,\n",
       "  -0.027019048109650612,\n",
       "  0.006418293807655573,\n",
       "  -0.002068011090159416,\n",
       "  0.0010038503678515553,\n",
       "  -0.012696920894086361,\n",
       "  -0.04647073149681091,\n",
       "  -0.024632027372717857,\n",
       "  -0.007922878488898277,\n",
       "  0.01834705099463463,\n",
       "  0.003942394163459539,\n",
       "  0.01799153722822666,\n",
       "  0.0026901601813733578,\n",
       "  0.02775546908378601,\n",
       "  -0.018321657553315163,\n",
       "  -0.0002279494219692424,\n",
       "  -0.01608699932694435,\n",
       "  -0.009668705984950066,\n",
       "  0.008621209301054478,\n",
       "  0.0031075715087354183,\n",
       "  0.022283097729086876,\n",
       "  0.011865273118019104,\n",
       "  -0.007675288710743189,\n",
       "  -0.0028441103640943766,\n",
       "  0.0003057180729229003,\n",
       "  -0.02215612679719925,\n",
       "  -0.039563607424497604,\n",
       "  -0.005243828520178795,\n",
       "  -0.04928945004940033,\n",
       "  0.03161533549427986,\n",
       "  0.00025493037537671626,\n",
       "  -0.0012712792959064245,\n",
       "  -0.009522691369056702,\n",
       "  -0.010557490400969982,\n",
       "  -0.027069836854934692,\n",
       "  0.006862686015665531,\n",
       "  0.01281754206866026,\n",
       "  0.003663061885163188,\n",
       "  0.003856689902022481,\n",
       "  0.03062497451901436,\n",
       "  -0.011865273118019104,\n",
       "  0.029761584475636482,\n",
       "  -0.010906655341386795,\n",
       "  0.006564308423548937,\n",
       "  -0.00013788063370157033,\n",
       "  0.016239361837506294,\n",
       "  -0.013814250007271767,\n",
       "  0.016518695279955864,\n",
       "  -0.0067738075740635395,\n",
       "  -0.013369858264923096,\n",
       "  -0.011325653642416,\n",
       "  0.001087173935957253,\n",
       "  -0.009408419020473957,\n",
       "  -0.025647781789302826,\n",
       "  -0.00838631670922041,\n",
       "  0.024644725024700165,\n",
       "  -0.01006230991333723,\n",
       "  -0.00636433158069849,\n",
       "  0.009624266065657139,\n",
       "  0.011255820281803608,\n",
       "  0.005015283823013306,\n",
       "  -0.001418881001882255,\n",
       "  0.016861511394381523,\n",
       "  0.0045201038010418415,\n",
       "  -0.007973666302859783,\n",
       "  -0.005970727186650038,\n",
       "  -0.012627088464796543,\n",
       "  -0.020886436104774475,\n",
       "  -0.0011752587743103504,\n",
       "  0.0024028923362493515,\n",
       "  -0.008430755697190762,\n",
       "  0.019807197153568268,\n",
       "  -0.02714601717889309,\n",
       "  -0.010557490400969982,\n",
       "  -0.0008114126394502819,\n",
       "  -0.015045851469039917,\n",
       "  0.022003764286637306,\n",
       "  0.003732894780114293,\n",
       "  -0.005148601718246937,\n",
       "  0.00022636230278294533,\n",
       "  -0.021495888009667397,\n",
       "  -0.0031932757701724768,\n",
       "  -0.017305903136730194,\n",
       "  -0.004999412689357996,\n",
       "  -0.006938867270946503,\n",
       "  -0.011420880444347858,\n",
       "  0.009484600275754929,\n",
       "  -0.026841292157769203,\n",
       "  -0.007573713548481464,\n",
       "  -0.006389725487679243,\n",
       "  0.017064662650227547,\n",
       "  0.0018839057302102447,\n",
       "  -0.01082412526011467,\n",
       "  -0.0003733688499778509,\n",
       "  0.019210441038012505,\n",
       "  -0.02023889310657978,\n",
       "  0.014004704542458057,\n",
       "  -0.0198579840362072,\n",
       "  -0.030269460752606392,\n",
       "  0.000806254509370774,\n",
       "  0.00427568843588233,\n",
       "  -0.0069198221899569035,\n",
       "  -0.016277452930808067,\n",
       "  -0.024124151095747948,\n",
       "  0.017064662650227547,\n",
       "  -0.026485778391361237,\n",
       "  0.0004733570967800915,\n",
       "  0.013115920126438141,\n",
       "  -0.0067738075740635395,\n",
       "  -0.023527394980192184,\n",
       "  0.012658830732107162,\n",
       "  0.00961791817098856,\n",
       "  -0.027044441550970078,\n",
       "  0.0003755511133931577,\n",
       "  0.20477594435214996,\n",
       "  -0.03572913631796837,\n",
       "  -0.0032789800316095352,\n",
       "  -0.012703269720077515,\n",
       "  -0.010862216353416443,\n",
       "  0.0034757822286337614,\n",
       "  0.038954153656959534,\n",
       "  -0.0037011525128036737,\n",
       "  0.007129321340471506,\n",
       "  0.0025377972051501274,\n",
       "  0.007224548142403364,\n",
       "  0.007941924035549164,\n",
       "  -0.03425629436969757,\n",
       "  0.012379498220980167,\n",
       "  0.029990127310156822,\n",
       "  -0.03174230456352234,\n",
       "  -0.021521281450986862,\n",
       "  -0.04047778621315956,\n",
       "  0.019159654155373573,\n",
       "  0.011122503317892551,\n",
       "  0.00747213838621974,\n",
       "  0.022524338215589523,\n",
       "  -0.04162050783634186,\n",
       "  -0.024974843487143517,\n",
       "  0.0403762087225914,\n",
       "  0.01670914888381958,\n",
       "  -0.006976958364248276,\n",
       "  0.019781803712248802,\n",
       "  0.00813237763941288,\n",
       "  0.01744556985795498,\n",
       "  -0.015337880700826645,\n",
       "  0.010563838295638561,\n",
       "  0.013306373730301857,\n",
       "  -0.001977545442059636,\n",
       "  -0.016125090420246124,\n",
       "  -0.00010613833001116291,\n",
       "  0.01364919077605009,\n",
       "  -0.0011212968965992332,\n",
       "  0.008735481649637222,\n",
       "  0.012747708708047867,\n",
       "  0.019273927435278893,\n",
       "  0.0009387786267325282,\n",
       "  0.019832590594887733,\n",
       "  -0.011541501618921757,\n",
       "  0.014601459726691246,\n",
       "  0.04228074848651886,\n",
       "  ...]}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding of the paragraph and title for each article in wiki_list\n",
    "for wiki in wiki_list:\n",
    "    response = client.embeddings.create(\n",
    "        input=wiki['title'] + wiki['paragraph'],\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    wiki[\"embeddings\"] = response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'link': '/wiki/Data_mining',\n",
       " 'title': 'Data mining',\n",
       " 'paragraph': 'Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1]\\n',\n",
       " 'embeddings': [-0.0316171795129776,\n",
       "  0.012553123757243156,\n",
       "  0.03128328546881676,\n",
       "  -0.048979658633470535,\n",
       "  -0.03927105292677879,\n",
       "  0.021561836823821068,\n",
       "  -0.018299950286746025,\n",
       "  -0.019404368475079536,\n",
       "  -0.008918818086385727,\n",
       "  -0.04733587056398392,\n",
       "  -0.004401619546115398,\n",
       "  0.043945565819740295,\n",
       "  0.004854302853345871,\n",
       "  -0.006077509839087725,\n",
       "  -0.010902917943894863,\n",
       "  0.02002078853547573,\n",
       "  0.024913616478443146,\n",
       "  0.007236506789922714,\n",
       "  0.02704540081322193,\n",
       "  -0.026428980752825737,\n",
       "  -0.016540586948394775,\n",
       "  0.009881973266601562,\n",
       "  0.0062187728472054005,\n",
       "  -0.0031639651861041784,\n",
       "  0.002308362163603306,\n",
       "  -0.002552361460402608,\n",
       "  0.02840666100382805,\n",
       "  -0.03166854754090309,\n",
       "  -0.005306985694915056,\n",
       "  -0.01921173743903637,\n",
       "  0.018646687269210815,\n",
       "  -0.0237193051725626,\n",
       "  0.0024432039353996515,\n",
       "  -0.0019664419814944267,\n",
       "  -0.02694266475737095,\n",
       "  0.009002291597425938,\n",
       "  0.01574437879025936,\n",
       "  0.003756305668503046,\n",
       "  0.0079107154160738,\n",
       "  0.02460540644824505,\n",
       "  0.010382814332842827,\n",
       "  -0.014665644615888596,\n",
       "  -0.013663963414728642,\n",
       "  -0.023873409256339073,\n",
       "  0.028355291113257408,\n",
       "  0.01898057945072651,\n",
       "  0.03269591182470322,\n",
       "  -0.028766239061951637,\n",
       "  -0.0020113892387598753,\n",
       "  0.015770062804222107,\n",
       "  -0.0048928288742899895,\n",
       "  0.03963062912225723,\n",
       "  -0.023911936208605766,\n",
       "  -0.0010843523778021336,\n",
       "  0.006671455688774586,\n",
       "  -0.010556181892752647,\n",
       "  0.020328998565673828,\n",
       "  0.017478058114647865,\n",
       "  -0.003236201824620366,\n",
       "  -0.009978288784623146,\n",
       "  0.008225345984101295,\n",
       "  0.008674819022417068,\n",
       "  0.0026968347374349833,\n",
       "  0.002398256678134203,\n",
       "  -0.004488303791731596,\n",
       "  -0.009920499287545681,\n",
       "  0.009092185646295547,\n",
       "  0.017465215176343918,\n",
       "  0.006966823246330023,\n",
       "  0.009451763704419136,\n",
       "  0.025016354396939278,\n",
       "  0.010646076872944832,\n",
       "  -0.013253017328679562,\n",
       "  0.01779910922050476,\n",
       "  0.018004583194851875,\n",
       "  -0.013355753384530544,\n",
       "  0.01740100607275963,\n",
       "  0.00881608109921217,\n",
       "  -0.0025090195704251528,\n",
       "  0.018107319250702858,\n",
       "  -0.01914752647280693,\n",
       "  -0.0025684142019599676,\n",
       "  -0.0016020482871681452,\n",
       "  0.008944502100348473,\n",
       "  0.0013460095506161451,\n",
       "  0.025042038410902023,\n",
       "  -0.014293224550783634,\n",
       "  0.012482492253184319,\n",
       "  0.021420573815703392,\n",
       "  -0.002441598568111658,\n",
       "  -0.02663445472717285,\n",
       "  0.0002106902247760445,\n",
       "  0.00904723908752203,\n",
       "  0.026133613660931587,\n",
       "  -0.025607088580727577,\n",
       "  0.02049594558775425,\n",
       "  0.0005550182540901005,\n",
       "  0.006019720807671547,\n",
       "  -0.004475461784750223,\n",
       "  -0.01898057945072651,\n",
       "  -0.024618249386548996,\n",
       "  0.0327986478805542,\n",
       "  -0.015526063740253448,\n",
       "  -0.013342911377549171,\n",
       "  -0.02951107919216156,\n",
       "  0.015076590701937675,\n",
       "  -0.0023773883003741503,\n",
       "  -0.025093406438827515,\n",
       "  0.013227332383394241,\n",
       "  0.00342562235891819,\n",
       "  -0.0006400969577953219,\n",
       "  0.03957926109433174,\n",
       "  0.008507872000336647,\n",
       "  -0.03564958646893501,\n",
       "  0.006295825354754925,\n",
       "  0.010729550383985043,\n",
       "  0.02559424750506878,\n",
       "  -0.037807054817676544,\n",
       "  -0.020778469741344452,\n",
       "  -0.020290471613407135,\n",
       "  0.011570706032216549,\n",
       "  0.0064210351556539536,\n",
       "  0.008565661497414112,\n",
       "  -0.005570248235017061,\n",
       "  0.01101207546889782,\n",
       "  0.008084083907306194,\n",
       "  -0.012931965291500092,\n",
       "  -0.02653171867132187,\n",
       "  0.026788558810949326,\n",
       "  0.0031639651861041784,\n",
       "  -0.0022666254080832005,\n",
       "  0.009689342230558395,\n",
       "  -0.0018829685868695378,\n",
       "  0.022923095151782036,\n",
       "  -0.04022136703133583,\n",
       "  -0.010658918879926205,\n",
       "  0.012842070311307907,\n",
       "  0.00582708977162838,\n",
       "  -0.012142177671194077,\n",
       "  -0.03015318140387535,\n",
       "  0.007043875753879547,\n",
       "  0.007512611337006092,\n",
       "  -0.004061304964125156,\n",
       "  -0.020688576623797417,\n",
       "  -0.00538724847137928,\n",
       "  0.020996784791350365,\n",
       "  0.005220301914960146,\n",
       "  0.011667021550238132,\n",
       "  0.008912396617233753,\n",
       "  -0.010299340821802616,\n",
       "  0.024053197354078293,\n",
       "  -0.020483102649450302,\n",
       "  -0.028329607099294662,\n",
       "  0.014331750571727753,\n",
       "  0.002626203466206789,\n",
       "  -0.03120623156428337,\n",
       "  0.014485855586826801,\n",
       "  0.011705547571182251,\n",
       "  0.02289741113781929,\n",
       "  -0.021266469731926918,\n",
       "  0.01089007593691349,\n",
       "  0.0006974849384278059,\n",
       "  0.004745144862681627,\n",
       "  -0.0019873103592544794,\n",
       "  0.029665183275938034,\n",
       "  0.021741624921560287,\n",
       "  0.01809447817504406,\n",
       "  0.01174407359212637,\n",
       "  -0.0049955653958022594,\n",
       "  -0.013638279400765896,\n",
       "  0.009580184705555439,\n",
       "  0.01679742895066738,\n",
       "  -0.027610450983047485,\n",
       "  0.00437914626672864,\n",
       "  0.006035773083567619,\n",
       "  -0.010292919352650642,\n",
       "  0.021574677899479866,\n",
       "  0.01349701639264822,\n",
       "  -0.03883442282676697,\n",
       "  -0.023693621158599854,\n",
       "  0.01873658038675785,\n",
       "  -0.022332360967993736,\n",
       "  0.0008804844692349434,\n",
       "  0.013638279400765896,\n",
       "  -0.026428980752825737,\n",
       "  -0.0009711816092021763,\n",
       "  0.009978288784623146,\n",
       "  0.0011597995180636644,\n",
       "  0.0028958870097994804,\n",
       "  -0.015834273770451546,\n",
       "  0.014177645556628704,\n",
       "  0.017555110156536102,\n",
       "  -0.01305396482348442,\n",
       "  0.003221754450351,\n",
       "  -0.6390214562416077,\n",
       "  -0.010607549920678139,\n",
       "  0.02460540644824505,\n",
       "  -0.019455736503005028,\n",
       "  -0.011365232057869434,\n",
       "  -0.00550603773444891,\n",
       "  0.015834273770451546,\n",
       "  -0.016771744936704636,\n",
       "  0.02622350864112377,\n",
       "  0.01194312609732151,\n",
       "  0.007923557423055172,\n",
       "  0.016001220792531967,\n",
       "  0.013342911377549171,\n",
       "  -0.01505090668797493,\n",
       "  -0.02395046129822731,\n",
       "  -0.03508453816175461,\n",
       "  -0.012944807298481464,\n",
       "  -0.039604946970939636,\n",
       "  0.003977831453084946,\n",
       "  0.01203302014619112,\n",
       "  -0.044125355780124664,\n",
       "  0.02840666100382805,\n",
       "  -0.003859042190015316,\n",
       "  0.012655860744416714,\n",
       "  -0.027327926829457283,\n",
       "  0.0054514589719474316,\n",
       "  0.0046199350617825985,\n",
       "  0.03724200651049614,\n",
       "  0.0067549291998147964,\n",
       "  0.009586606174707413,\n",
       "  -0.0065494561567902565,\n",
       "  -0.019699735566973686,\n",
       "  0.01709279604256153,\n",
       "  -0.0039553577080369,\n",
       "  0.03947652503848076,\n",
       "  -0.020418891683220863,\n",
       "  0.022756149992346764,\n",
       "  0.02271762304008007,\n",
       "  0.011018496938049793,\n",
       "  0.029767919331789017,\n",
       "  -0.012989754788577557,\n",
       "  -0.022383728995919228,\n",
       "  0.03652285039424896,\n",
       "  0.027071084827184677,\n",
       "  0.0010442208731546998,\n",
       "  0.0069475602358579636,\n",
       "  0.009297658689320087,\n",
       "  0.016502059996128082,\n",
       "  -0.019596999511122704,\n",
       "  -0.029819287359714508,\n",
       "  -0.021998466923832893,\n",
       "  0.010414918884634972,\n",
       "  -0.0035058853682130575,\n",
       "  -0.008539976552128792,\n",
       "  0.0016630481695756316,\n",
       "  0.021767308935523033,\n",
       "  0.010594707913696766,\n",
       "  -0.004398409277200699,\n",
       "  0.01891637034714222,\n",
       "  -0.019802473485469818,\n",
       "  0.009265554137527943,\n",
       "  0.0009800104890018702,\n",
       "  -0.02587677165865898,\n",
       "  -0.020187735557556152,\n",
       "  -0.04425377398729324,\n",
       "  0.017079953104257584,\n",
       "  -0.003981041721999645,\n",
       "  -0.01845405623316765,\n",
       "  0.0073585063219070435,\n",
       "  0.000756076886318624,\n",
       "  0.022987306118011475,\n",
       "  0.0006336758960969746,\n",
       "  0.022280992940068245,\n",
       "  -4.211296254652552e-05,\n",
       "  0.0222553089261055,\n",
       "  0.01868521235883236,\n",
       "  0.02470814436674118,\n",
       "  -0.0012801939155906439,\n",
       "  -0.0054289852268993855,\n",
       "  -0.003512306371703744,\n",
       "  -0.001342799048870802,\n",
       "  -0.008039136417210102,\n",
       "  -0.02335972711443901,\n",
       "  -0.02712245285511017,\n",
       "  0.04063231125473976,\n",
       "  0.007210822775959969,\n",
       "  -0.004314935766160488,\n",
       "  -0.016245219856500626,\n",
       "  0.001067497069016099,\n",
       "  0.014498697593808174,\n",
       "  0.014652802608907223,\n",
       "  0.03092370554804802,\n",
       "  -0.0015731536550447345,\n",
       "  -0.05645374208688736,\n",
       "  0.003276333212852478,\n",
       "  0.0010723129380494356,\n",
       "  0.002358125289902091,\n",
       "  -0.014203330501914024,\n",
       "  -0.010382814332842827,\n",
       "  -0.011089128442108631,\n",
       "  -0.019314473494887352,\n",
       "  0.004417672287672758,\n",
       "  0.0022313096560537815,\n",
       "  -0.018787948414683342,\n",
       "  0.015564589761197567,\n",
       "  -0.021125206723809242,\n",
       "  -0.002727334853261709,\n",
       "  0.0074162958189845085,\n",
       "  0.024078883230686188,\n",
       "  -0.013638279400765896,\n",
       "  0.01704142801463604,\n",
       "  0.011545021086931229,\n",
       "  -0.009978288784623146,\n",
       "  0.01950710453093052,\n",
       "  0.0038269369397312403,\n",
       "  -0.02541445754468441,\n",
       "  0.0039328839629888535,\n",
       "  -0.011249653995037079,\n",
       "  -0.02143341675400734,\n",
       "  -0.002236125525087118,\n",
       "  0.015603115782141685,\n",
       "  0.0026213875971734524,\n",
       "  0.00634077237918973,\n",
       "  -0.01358691044151783,\n",
       "  -0.0020113892387598753,\n",
       "  -0.006767771206796169,\n",
       "  0.009811341762542725,\n",
       "  -0.0016678639221936464,\n",
       "  0.012495334260165691,\n",
       "  -0.007775873877108097,\n",
       "  -0.011968810111284256,\n",
       "  -0.012437545694410801,\n",
       "  -0.018942054361104965,\n",
       "  -0.0027626503724604845,\n",
       "  0.01273291278630495,\n",
       "  -0.014511539600789547,\n",
       "  0.0030178865417838097,\n",
       "  0.006819139700382948,\n",
       "  -0.023334043100476265,\n",
       "  -0.04189083352684975,\n",
       "  -0.00796208344399929,\n",
       "  -0.02594098262488842,\n",
       "  0.015487536787986755,\n",
       "  0.007069559767842293,\n",
       "  -0.008938081562519073,\n",
       "  -0.00026346309459768236,\n",
       "  -0.016425007954239845,\n",
       "  -0.007230085786432028,\n",
       "  -0.006902612745761871,\n",
       "  -0.004385567270219326,\n",
       "  -0.022165413945913315,\n",
       "  -0.010196603834629059,\n",
       "  -0.018402686342597008,\n",
       "  0.05041797086596489,\n",
       "  0.006973244249820709,\n",
       "  -0.009169238619506359,\n",
       "  0.0022072307765483856,\n",
       "  6.180246418807656e-05,\n",
       "  -0.016900165006518364,\n",
       "  -0.025568563491106033,\n",
       "  -0.020765628665685654,\n",
       "  0.004343830514699221,\n",
       "  -0.03624032437801361,\n",
       "  0.009496711194515228,\n",
       "  0.0008174780523404479,\n",
       "  -0.037730004638433456,\n",
       "  -0.009496711194515228,\n",
       "  0.0049955653958022594,\n",
       "  -0.02377067320048809,\n",
       "  -0.03965631499886513,\n",
       "  0.03500748425722122,\n",
       "  -0.005104722920805216,\n",
       "  0.0031013600528240204,\n",
       "  0.017349638044834137,\n",
       "  -0.02727655880153179,\n",
       "  0.02587677165865898,\n",
       "  -0.026146456599235535,\n",
       "  -0.016900165006518364,\n",
       "  -0.020624365657567978,\n",
       "  -0.017195532098412514,\n",
       "  -0.007750189397484064,\n",
       "  -0.00591377355158329,\n",
       "  -0.008334503509104252,\n",
       "  -0.014627118594944477,\n",
       "  0.033106859773397446,\n",
       "  0.017824793234467506,\n",
       "  0.01674606092274189,\n",
       "  -0.005143249407410622,\n",
       "  -0.0014800486387684941,\n",
       "  -0.006443508900702,\n",
       "  0.005140038672834635,\n",
       "  0.017850477248430252,\n",
       "  -0.0014190487563610077,\n",
       "  -0.008559240028262138,\n",
       "  0.006504508666694164,\n",
       "  0.005377617198973894,\n",
       "  -0.016643323004245758,\n",
       "  0.004835039377212524,\n",
       "  0.001134115387685597,\n",
       "  0.02411740832030773,\n",
       "  0.0242971982806921,\n",
       "  0.010203025303781033,\n",
       "  -0.01314385887235403,\n",
       "  -0.0013949698768556118,\n",
       "  -0.0055220904760062695,\n",
       "  -0.03503317013382912,\n",
       "  -0.001626127166673541,\n",
       "  -0.0392196848988533,\n",
       "  0.0046873558312654495,\n",
       "  -0.01575721986591816,\n",
       "  0.026968348771333694,\n",
       "  -0.0240018293261528,\n",
       "  0.0029039131477475166,\n",
       "  -0.01815868727862835,\n",
       "  0.018903527408838272,\n",
       "  0.053679853677749634,\n",
       "  -0.02704540081322193,\n",
       "  0.01279712375253439,\n",
       "  -0.029382657259702682,\n",
       "  0.011769757606089115,\n",
       "  0.006703560706228018,\n",
       "  -0.003547622123733163,\n",
       "  0.02312856912612915,\n",
       "  -0.032233595848083496,\n",
       "  -0.013895120471715927,\n",
       "  0.0010979970684275031,\n",
       "  0.019545631483197212,\n",
       "  0.03362054005265236,\n",
       "  0.016733217984437943,\n",
       "  -0.03500748425722122,\n",
       "  0.004119093995541334,\n",
       "  0.02470814436674118,\n",
       "  0.0027979661244899035,\n",
       "  0.01019018329679966,\n",
       "  0.018004583194851875,\n",
       "  0.017490901052951813,\n",
       "  0.0032458333298563957,\n",
       "  -0.025042038410902023,\n",
       "  0.010337866842746735,\n",
       "  -0.012020178139209747,\n",
       "  -0.03189970180392265,\n",
       "  0.019635526463389397,\n",
       "  0.02330835908651352,\n",
       "  -0.017298270016908646,\n",
       "  0.002390230307355523,\n",
       "  -0.028201187029480934,\n",
       "  0.01593700982630253,\n",
       "  0.01452438160777092,\n",
       "  -0.016835954040288925,\n",
       "  0.022229624912142754,\n",
       "  -0.01914752647280693,\n",
       "  0.011024917475879192,\n",
       "  -0.02447698637843132,\n",
       "  -0.0036439376417547464,\n",
       "  -0.0044915140606462955,\n",
       "  -0.03559821844100952,\n",
       "  0.031411703675985336,\n",
       "  0.014344592578709126,\n",
       "  0.034981802105903625,\n",
       "  0.023346884176135063,\n",
       "  0.009695763699710369,\n",
       "  0.016142481938004494,\n",
       "  0.01569301076233387,\n",
       "  -0.002396651543676853,\n",
       "  0.010902917943894863,\n",
       "  -0.01285491231828928,\n",
       "  0.019224580377340317,\n",
       "  0.004000305198132992,\n",
       "  -0.008739029057323933,\n",
       "  -0.014871117658913136,\n",
       "  0.007814399898052216,\n",
       "  -0.0369081124663353,\n",
       "  0.004822197370231152,\n",
       "  -0.030872337520122528,\n",
       "  0.009650816209614277,\n",
       "  0.019070474430918694,\n",
       "  0.029125815257430077,\n",
       "  -0.0032859647180885077,\n",
       "  0.020251944661140442,\n",
       "  0.015076590701937675,\n",
       "  -0.016258060932159424,\n",
       "  -0.004334198776632547,\n",
       "  -0.005149670410901308,\n",
       "  0.028843291103839874,\n",
       "  0.005624826997518539,\n",
       "  -0.025440141558647156,\n",
       "  0.00910502765327692,\n",
       "  -0.0035058853682130575,\n",
       "  -0.00945818517357111,\n",
       "  -0.018120162189006805,\n",
       "  -0.0024592564441263676,\n",
       "  0.011557864025235176,\n",
       "  0.005361564457416534,\n",
       "  -0.019455736503005028,\n",
       "  -0.019661210477352142,\n",
       "  -0.003720989916473627,\n",
       "  0.056607846170663834,\n",
       "  0.015680167824029922,\n",
       "  -0.010299340821802616,\n",
       "  0.017670689150691032,\n",
       "  0.00042017651139758527,\n",
       "  -0.008584924042224884,\n",
       "  -0.037036530673503876,\n",
       "  -0.02517045848071575,\n",
       "  0.06518635153770447,\n",
       "  -0.013689647428691387,\n",
       "  0.00022172638273332268,\n",
       "  0.012546703219413757,\n",
       "  0.004000305198132992,\n",
       "  -0.019686894491314888,\n",
       "  0.011988072656095028,\n",
       "  -0.0031142020598053932,\n",
       "  -0.004793303087353706,\n",
       "  0.011018496938049793,\n",
       "  -0.010703865438699722,\n",
       "  -0.004841460846364498,\n",
       "  0.014383119530975819,\n",
       "  0.03151443973183632,\n",
       "  0.02663445472717285,\n",
       "  0.03033297136425972,\n",
       "  0.019417211413383484,\n",
       "  -0.02628771774470806,\n",
       "  0.007422716822475195,\n",
       "  0.02565845660865307,\n",
       "  0.04826050251722336,\n",
       "  -0.004096620716154575,\n",
       "  -0.02097110077738762,\n",
       "  0.011699126102030277,\n",
       "  -0.02013636752963066,\n",
       "  -0.002030652482062578,\n",
       "  -0.029716551303863525,\n",
       "  -0.015654483810067177,\n",
       "  -0.00855281949043274,\n",
       "  0.013985014520585537,\n",
       "  -0.020483102649450302,\n",
       "  0.017645005136728287,\n",
       "  0.018402686342597008,\n",
       "  0.02493930049240589,\n",
       "  -0.013028280809521675,\n",
       "  0.013741015456616879,\n",
       "  -0.005037302151322365,\n",
       "  -0.0015025222674012184,\n",
       "  -0.004176883492618799,\n",
       "  -0.004083778243511915,\n",
       "  -0.010960707440972328,\n",
       "  -0.005576669238507748,\n",
       "  0.017709216102957726,\n",
       "  0.010369972325861454,\n",
       "  0.02470814436674118,\n",
       "  0.001910257968120277,\n",
       "  0.04525545611977577,\n",
       "  0.03819231688976288,\n",
       "  -0.02149762585759163,\n",
       "  -0.01955847442150116,\n",
       "  -0.00038827199023216963,\n",
       "  0.03169422969222069,\n",
       "  0.008173977956175804,\n",
       "  0.009676500223577023,\n",
       "  -0.03192538768053055,\n",
       "  0.04777250438928604,\n",
       "  -0.005380827467888594,\n",
       "  0.012167861685156822,\n",
       "  0.0011469573946669698,\n",
       "  -0.016078272834420204,\n",
       "  0.019571315497159958,\n",
       "  0.014190488494932652,\n",
       "  0.0053262487053871155,\n",
       "  -0.016232376918196678,\n",
       "  -0.0005457879742607474,\n",
       "  -0.028894659131765366,\n",
       "  0.0079107154160738,\n",
       "  0.026428980752825737,\n",
       "  -0.009323343634605408,\n",
       "  -0.010267235338687897,\n",
       "  -0.006241246126592159,\n",
       "  -0.001669469173066318,\n",
       "  -0.022178256884217262,\n",
       "  -0.0060582468286156654,\n",
       "  0.021805835887789726,\n",
       "  0.009997552260756493,\n",
       "  0.024926459416747093,\n",
       "  0.001389351557008922,\n",
       "  -0.015243537724018097,\n",
       "  -0.03839779272675514,\n",
       "  -0.010883654467761517,\n",
       "  -0.01182754710316658,\n",
       "  0.008867450058460236,\n",
       "  -0.002086836379021406,\n",
       "  0.001704784925095737,\n",
       "  -0.004539671819657087,\n",
       "  -0.021972782909870148,\n",
       "  0.003720989916473627,\n",
       "  -0.015474694781005383,\n",
       "  0.029254237189888954,\n",
       "  0.005939457565546036,\n",
       "  -0.03642011061310768,\n",
       "  -0.011018496938049793,\n",
       "  0.0014431276358664036,\n",
       "  -0.0010426156222820282,\n",
       "  0.01914752647280693,\n",
       "  0.022216781973838806,\n",
       "  -0.003929673694074154,\n",
       "  -0.003276333212852478,\n",
       "  0.006761350203305483,\n",
       "  -0.022114045917987823,\n",
       "  0.0007203599088825285,\n",
       "  -0.029613815248012543,\n",
       "  -0.025375932455062866,\n",
       "  0.011121232993900776,\n",
       "  -0.014819749630987644,\n",
       "  -0.025863930583000183,\n",
       "  -0.015307747758924961,\n",
       "  0.001712811179459095,\n",
       "  0.017067112028598785,\n",
       "  -0.0036760426592081785,\n",
       "  -0.01276501826941967,\n",
       "  -0.0033646225929260254,\n",
       "  0.015115116722881794,\n",
       "  0.007223664782941341,\n",
       "  0.010735970921814442,\n",
       "  0.016091113910079002,\n",
       "  0.0015876010293141007,\n",
       "  -0.004475461784750223,\n",
       "  -0.011955968104302883,\n",
       "  0.006247667130082846,\n",
       "  0.0012809965992346406,\n",
       "  -0.024810880422592163,\n",
       "  -0.01379238348454237,\n",
       "  0.009182080626487732,\n",
       "  -0.002891071140766144,\n",
       "  -0.009002291597425938,\n",
       "  0.005705089773982763,\n",
       "  0.03652285039424896,\n",
       "  -0.007750189397484064,\n",
       "  0.014562907628715038,\n",
       "  -0.018081635236740112,\n",
       "  0.038911473006010056,\n",
       "  -0.010967127978801727,\n",
       "  0.02804708294570446,\n",
       "  -0.006909034214913845,\n",
       "  0.020945416763424873,\n",
       "  0.03649716451764107,\n",
       "  0.024875091388821602,\n",
       "  -0.01525637973099947,\n",
       "  0.007249348796904087,\n",
       "  -0.012431124225258827,\n",
       "  0.016887322068214417,\n",
       "  0.027148136869072914,\n",
       "  -0.0158085897564888,\n",
       "  0.0027851241175085306,\n",
       "  0.009214186109602451,\n",
       "  -0.027302242815494537,\n",
       "  -0.05532364174723625,\n",
       "  -0.021715940907597542,\n",
       "  -0.010954285971820354,\n",
       "  0.007756610866636038,\n",
       "  -0.04263567551970482,\n",
       "  -0.026377612724900246,\n",
       "  -0.025607088580727577,\n",
       "  0.008218925446271896,\n",
       "  0.017182691022753716,\n",
       "  0.027250874787569046,\n",
       "  -0.02758476696908474,\n",
       "  -0.02201130986213684,\n",
       "  0.0022858886513859034,\n",
       "  0.005005197133868933,\n",
       "  0.009625132195651531,\n",
       "  -0.0016437850426882505,\n",
       "  0.03151443973183632,\n",
       "  -0.026377612724900246,\n",
       "  -0.015102274715900421,\n",
       "  -0.017966056242585182,\n",
       "  -0.007140191271901131,\n",
       "  0.024220144376158714,\n",
       "  -0.010735970921814442,\n",
       "  0.007859347388148308,\n",
       "  -0.00852071400731802,\n",
       "  -0.018942054361104965,\n",
       "  0.017991740256547928,\n",
       "  0.007210822775959969,\n",
       "  -0.000254032202064991,\n",
       "  0.017786268144845963,\n",
       "  0.04189083352684975,\n",
       "  0.05280659347772598,\n",
       "  0.020097840577363968,\n",
       "  0.018865002319216728,\n",
       "  0.031540125608444214,\n",
       "  -0.0020499154925346375,\n",
       "  -0.02694266475737095,\n",
       "  -0.018338477239012718,\n",
       "  0.0031447019428014755,\n",
       "  -0.005579879507422447,\n",
       "  0.003547622123733163,\n",
       "  -0.004539671819657087,\n",
       "  0.004902460612356663,\n",
       "  -0.02038036659359932,\n",
       "  0.013137438334524632,\n",
       "  -0.0039039894472807646,\n",
       "  0.03546980023384094,\n",
       "  0.009207764640450478,\n",
       "  0.013124596327543259,\n",
       "  -0.018492581322789192,\n",
       "  -0.02230667695403099,\n",
       "  -0.012553123757243156,\n",
       "  -0.006048615090548992,\n",
       "  0.028072766959667206,\n",
       "  0.026608770713210106,\n",
       "  0.049108076840639114,\n",
       "  -0.03490474820137024,\n",
       "  0.013638279400765896,\n",
       "  0.04543524608016014,\n",
       "  0.00024299605865962803,\n",
       "  0.0012553123524412513,\n",
       "  0.0042635672725737095,\n",
       "  0.017542269080877304,\n",
       "  -0.004963460378348827,\n",
       "  -0.010658918879926205,\n",
       "  0.008462924510240555,\n",
       "  0.01973826251924038,\n",
       "  -0.011602810584008694,\n",
       "  0.0038397791795432568,\n",
       "  -0.03652285039424896,\n",
       "  -0.005762879271060228,\n",
       "  0.01985384151339531,\n",
       "  -0.003162359818816185,\n",
       "  0.01903194934129715,\n",
       "  0.03993884101510048,\n",
       "  -0.020739944651722908,\n",
       "  0.007114507257938385,\n",
       "  0.010800180956721306,\n",
       "  -0.002772282110527158,\n",
       "  0.011275338008999825,\n",
       "  0.02348814718425274,\n",
       "  -0.030435707420110703,\n",
       "  -0.005669774021953344,\n",
       "  -0.016425007954239845,\n",
       "  -0.018775107339024544,\n",
       "  -0.0029312025289982557,\n",
       "  0.023744989186525345,\n",
       "  0.03683105856180191,\n",
       "  0.01019018329679966,\n",
       "  0.02084268070757389,\n",
       "  -0.021985623985528946,\n",
       "  -0.024489829316735268,\n",
       "  0.007435558829456568,\n",
       "  -0.005396880209445953,\n",
       "  0.030435707420110703,\n",
       "  -0.02020057663321495,\n",
       "  0.0492108128964901,\n",
       "  0.006883349735289812,\n",
       "  0.01557743176817894,\n",
       "  0.0023838093038648367,\n",
       "  0.01232196670025587,\n",
       "  -0.005672984756529331,\n",
       "  -0.0018380213296040893,\n",
       "  0.028124134987592697,\n",
       "  0.005332669708877802,\n",
       "  -0.008970186114311218,\n",
       "  -0.023693621158599854,\n",
       "  -0.0070502967573702335,\n",
       "  -0.014794065617024899,\n",
       "  -3.5616681998362765e-05,\n",
       "  -0.02699403278529644,\n",
       "  0.020457418635487556,\n",
       "  -0.007024612743407488,\n",
       "  0.00787218939512968,\n",
       "  -0.022473623976111412,\n",
       "  0.005245985928922892,\n",
       "  -0.014819749630987644,\n",
       "  0.037396110594272614,\n",
       "  0.0008419582736678421,\n",
       "  0.002083625877276063,\n",
       "  0.0008379450882785022,\n",
       "  -0.00872618705034256,\n",
       "  0.00023878224601503462,\n",
       "  0.02809845097362995,\n",
       "  -0.020007945597171783,\n",
       "  0.02758476696908474,\n",
       "  -0.022280992940068245,\n",
       "  -0.005666563753038645,\n",
       "  -0.0065141404047608376,\n",
       "  0.002945649903267622,\n",
       "  -0.019468579441308975,\n",
       "  -0.0011485626455396414,\n",
       "  0.005804616026580334,\n",
       "  0.03207949176430702,\n",
       "  -0.0084886085242033,\n",
       "  0.00415762048214674,\n",
       "  -0.004953828640282154,\n",
       "  0.0020178102422505617,\n",
       "  -0.010703865438699722,\n",
       "  0.011564284563064575,\n",
       "  -0.00611924659460783,\n",
       "  0.030949389562010765,\n",
       "  -0.0017352848080918193,\n",
       "  -0.021227942779660225,\n",
       "  -0.011448705568909645,\n",
       "  0.01121112797409296,\n",
       "  0.010594707913696766,\n",
       "  -0.0029199658893048763,\n",
       "  -0.009843447245657444,\n",
       "  -0.020881207659840584,\n",
       "  -0.009939762763679028,\n",
       "  -0.016617638990283012,\n",
       "  0.005714721512049437,\n",
       "  0.007127349264919758,\n",
       "  -0.0044915140606462955,\n",
       "  0.007384190801531076,\n",
       "  -0.008424398489296436,\n",
       "  -0.028560765087604523,\n",
       "  0.010864391922950745,\n",
       "  -0.007429137825965881,\n",
       "  0.03963062912225723,\n",
       "  -0.024618249386548996,\n",
       "  -0.031540125608444214,\n",
       "  0.007242927793413401,\n",
       "  -0.011249653995037079,\n",
       "  0.02026478759944439,\n",
       "  0.02056015469133854,\n",
       "  0.002661519218236208,\n",
       "  -0.006404982879757881,\n",
       "  0.016823112964630127,\n",
       "  0.0005104722804389894,\n",
       "  0.00033148593502119184,\n",
       "  -0.003265096340328455,\n",
       "  0.007711663376539946,\n",
       "  -0.008212503977119923,\n",
       "  0.0008965370361693203,\n",
       "  -0.006870507728308439,\n",
       "  -0.010125972330570221,\n",
       "  0.01194312609732151,\n",
       "  -0.03069254942238331,\n",
       "  -0.014986696653068066,\n",
       "  5.134319872013293e-05,\n",
       "  0.003907199949026108,\n",
       "  0.02201130986213684,\n",
       "  -0.02505487948656082,\n",
       "  -0.0055895112454891205,\n",
       "  0.005069407168775797,\n",
       "  -0.004067725967615843,\n",
       "  0.005201038438826799,\n",
       "  0.007814399898052216,\n",
       "  -0.010440603829920292,\n",
       "  0.01557743176817894,\n",
       "  -0.02735361084342003,\n",
       "  -0.01919889636337757,\n",
       "  -0.012110072188079357,\n",
       "  -0.011500074528157711,\n",
       "  0.028714869171380997,\n",
       "  -0.02090689167380333,\n",
       "  -0.0018283898243680596,\n",
       "  -0.012437545694410801,\n",
       "  -0.055426377803087234,\n",
       "  -0.02681424282491207,\n",
       "  -0.029356973245739937,\n",
       "  0.009400395676493645,\n",
       "  -0.008456503041088581,\n",
       "  0.03564958646893501,\n",
       "  0.006244456861168146,\n",
       "  0.02131783775985241,\n",
       "  -0.020226260647177696,\n",
       "  0.01651490293443203,\n",
       "  -0.009284816682338715,\n",
       "  0.0008194846450351179,\n",
       "  0.024091724306344986,\n",
       "  0.006042194087058306,\n",
       "  0.03256748989224434,\n",
       "  0.008475766517221928,\n",
       "  -0.022704780101776123,\n",
       "  0.0012858123518526554,\n",
       "  -0.02997339330613613,\n",
       "  -0.008751871064305305,\n",
       "  -0.028021398931741714,\n",
       "  0.011134075000882149,\n",
       "  -0.0404011532664299,\n",
       "  0.03713926672935486,\n",
       "  0.018839318305253983,\n",
       "  -0.019237421452999115,\n",
       "  -0.030255917459726334,\n",
       "  -0.01069102343171835,\n",
       "  -0.006716402713209391,\n",
       "  -0.025928139686584473,\n",
       "  0.01722121611237526,\n",
       "  0.014421645551919937,\n",
       "  0.012424703687429428,\n",
       "  0.03138602152466774,\n",
       "  -0.009875552728772163,\n",
       "  0.033466435968875885,\n",
       "  -0.007634610868990421,\n",
       "  0.005997247062623501,\n",
       "  -0.026082245633006096,\n",
       "  0.023154253140091896,\n",
       "  -0.016604797914624214,\n",
       "  -0.010536919347941875,\n",
       "  0.02143341675400734,\n",
       "  -0.015115116722881794,\n",
       "  -0.01809447817504406,\n",
       "  -0.003598990384489298,\n",
       "  -0.012039441615343094,\n",
       "  -0.0254658255726099,\n",
       "  0.0009792079217731953,\n",
       "  0.010620392858982086,\n",
       "  -0.010755234397947788,\n",
       "  0.006042194087058306,\n",
       "  0.0060229310765862465,\n",
       "  -0.007133770268410444,\n",
       "  -0.008071240969002247,\n",
       "  0.011789021082222462,\n",
       "  0.009162817150354385,\n",
       "  -0.024078883230686188,\n",
       "  -0.012610913254320621,\n",
       "  0.0012569177197292447,\n",
       "  0.009156396612524986,\n",
       "  -0.029382657259702682,\n",
       "  0.003409569850191474,\n",
       "  0.011352390050888062,\n",
       "  -0.004006726201623678,\n",
       "  -0.004863934125751257,\n",
       "  -0.0009503132314421237,\n",
       "  -0.0025298879481852055,\n",
       "  -0.008084083907306194,\n",
       "  -0.017478058114647865,\n",
       "  -0.0013789173681288958,\n",
       "  0.021921414881944656,\n",
       "  -0.002858965890482068,\n",
       "  0.017670689150691032,\n",
       "  0.009490289725363255,\n",
       "  0.0036792531609535217,\n",
       "  -0.023500990122556686,\n",
       "  -0.00019644355052150786,\n",
       "  -0.022460781037807465,\n",
       "  -0.01949426345527172,\n",
       "  -0.009008712135255337,\n",
       "  -0.02628771774470806,\n",
       "  0.006986086256802082,\n",
       "  -0.028303923085331917,\n",
       "  0.0029745446518063545,\n",
       "  0.0006569521501660347,\n",
       "  0.012277019210159779,\n",
       "  -0.021561836823821068,\n",
       "  -0.0020804153755307198,\n",
       "  -0.025324562564492226,\n",
       "  -0.0059555103071033955,\n",
       "  0.010941443964838982,\n",
       "  -0.01738816313445568,\n",
       "  -0.011545021086931229,\n",
       "  0.004940986633300781,\n",
       "  -0.0193273164331913,\n",
       "  -0.020418891683220863,\n",
       "  -0.01358691044151783,\n",
       "  -0.0006079917657189071,\n",
       "  -0.03677969053387642,\n",
       "  -0.0006786231533624232,\n",
       "  0.014896801672875881,\n",
       "  -0.0020723892375826836,\n",
       "  -0.012110072188079357,\n",
       "  -0.0022024151403456926,\n",
       "  0.013407121412456036,\n",
       "  -0.017131322994828224,\n",
       "  0.003666411153972149,\n",
       "  0.19078180193901062,\n",
       "  -0.02648034878075123,\n",
       "  0.01496101263910532,\n",
       "  0.008456503041088581,\n",
       "  0.007037454750388861,\n",
       "  -0.016181008890271187,\n",
       "  0.014704170636832714,\n",
       "  0.007544716354459524,\n",
       "  0.019596999511122704,\n",
       "  0.004225041251629591,\n",
       "  0.02319278009235859,\n",
       "  -0.008186819963157177,\n",
       "  -0.00840513501316309,\n",
       "  0.00831524096429348,\n",
       "  0.0333893820643425,\n",
       "  -0.013407121412456036,\n",
       "  -0.01593700982630253,\n",
       "  -0.02799571305513382,\n",
       "  0.0010402076877653599,\n",
       "  0.004642408341169357,\n",
       "  0.0074162958189845085,\n",
       "  -0.0005337485345080495,\n",
       "  -0.032002441585063934,\n",
       "  -0.005332669708877802,\n",
       "  0.017850477248430252,\n",
       "  -0.0023099675308912992,\n",
       "  0.019802473485469818,\n",
       "  0.02294878102838993,\n",
       "  0.015076590701937675,\n",
       "  -0.001232036156579852,\n",
       "  -0.008835344575345516,\n",
       "  -0.007525453343987465,\n",
       "  0.024489829316735268,\n",
       "  -0.0028718081302940845,\n",
       "  -0.019301632419228554,\n",
       "  0.0036278849001973867,\n",
       "  0.011339548043906689,\n",
       "  -0.02681424282491207,\n",
       "  0.018133003264665604,\n",
       "  -0.015282063744962215,\n",
       "  0.019340157508850098,\n",
       "  -0.013201648369431496,\n",
       "  -0.017426690086722374,\n",
       "  -0.03010181337594986,\n",
       "  0.011223969981074333,\n",
       "  0.006735666189342737,\n",
       "  ...]}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_list[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance between the main article and each article in wiki_list using the dot product of their embeddings\n",
    "for wiki in wiki_list:\n",
    "    wiki['distance_embedding'] = np.dot(wiki_main['embeddings'], wiki['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the wiki_list by distance to the main article\n",
    "wiki_list.sort(key=lambda x: x['distance_embedding'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Machine_learning\n",
      "Machine learning is a branch of statistics and computer science which studies algorithms and architectures that learn from observed facts.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Automated_machine_learning\n",
      "Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Deep_learning\n",
      "Deep learning is the subset of machine learning methods which are based on artificial neural networks with representation learning. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Rule-based_machine_learning\n",
      "Rule-based machine learning (RBML) is a term in computer science intended to encompass any machine learning method that identifies, learns, or evolves 'rules' to store, manipulate or apply.[1][2][3] The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[clarification needed][citation needed]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Outline_of_machine_learning\n",
      "The following outline is provided as an overview of and topical guide to machine learning:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Online_machine_learning\n",
      "In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Robot_learning\n",
      "Robot learning is a research field at the intersection of machine learning and robotics. It studies techniques allowing a robot to acquire novel skills or adapt to its environment through learning algorithms. The embodiment of the robot, situated in a physical embedding, provides at the same time specific difficulties (e.g. high-dimensionality, real time constraints for collecting data and learning) and opportunities for guiding the learning process (e.g. sensorimotor synergies, motor primitives).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Machine_learning_control\n",
      "Machine learning control (MLC) is a subfield of machine learning, intelligent control and control theory\n",
      "which solves optimal control problems with methods of machine learning.\n",
      "Key applications are complex nonlinear systems\n",
      "for which linear control theory methods are not applicable.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Meta-learning_(computer_science)\n",
      "Meta learning[1][2]\n",
      "is a subfield of machine learning where automatic learning algorithms are applied to metadata about machine learning experiments. As of 2017, the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to learn.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Artificial_intelligence_in_healthcare\n",
      "Artificial intelligence in healthcare is a term used to describe the use of machine-learning algorithms and software, or artificial intelligence (AI), to copy human cognition in the analysis, presentation, and understanding of complex medical and health care data, or to exceed human capabilities by providing new ways to diagnose, treat, or prevent disease.[1][2] Specifically, AI is the ability of computer algorithms to approximate conclusions based solely on input data.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Adversarial_machine_learning\n",
      "Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks.[1] A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_learning_theory\n",
      "In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Progress_in_artificial_intelligence\n",
      "Progress in artificial intelligence (AI) refers to the advances, milestones, and breakthroughs that have been achieved in the field of artificial intelligence over time. AI is a multidisciplinary branch of computer science that aims to create machines and systems capable of performing tasks that typically require human intelligence. Artificial intelligence applications have been used in a wide range of fields including medical diagnosis, economic-financial applications, robot control, law, scientific discovery, video games, and toys. However, many AI applications are not perceived as AI:  \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[1][2] \"Many thousands of AI applications are deeply embedded in the infrastructure of every industry.\"[3] In the late 1990s and early 21st century, AI technology became widely used as elements of larger systems,[3][4] but the field was rarely credited for these successes at the time.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Machine_Learning_(journal)\n",
      "Machine Learning  is a peer-reviewed scientific journal, published since 1986.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Quantum_machine_learning\n",
      "Quantum machine learning is the integration of quantum algorithms within machine learning programs.[1][2][3][4][5][6][7][8]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Natural_language_processing\n",
      "Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Statistical_learning_theory\n",
      "Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis.[1][2][3] Statistical learning theory deals with the statistical inference problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ensemble_learning\n",
      "In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.[1][2][3]\n",
      "Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Supervised_learning\n",
      "Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data on expected output values.[1]  An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Multi-task_learning\n",
      "Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately.[1][2][3] Early versions of MTL were called \"hints\".[4][5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Active_learning_(machine_learning)\n",
      "Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs.[1][2][3] In statistics literature, it is sometimes also called optimal experimental design.[4] The information source is also called teacher or oracle.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Feature_learning\n",
      "In machine learning, feature learning or representation learning[2] is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform  a specific task.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Machine_learning_in_physics\n",
      "Applying classical methods of machine learning to the study of quantum systems is the focus of an emergent area of physics research. A basic example of this is quantum state tomography, where a quantum state is learned from measurement.[1] Other examples include learning Hamiltonians,[2][3] learning quantum phase transitions,[4][5] and automatically generating new quantum experiments.[6][7][8][9] Classical machine learning is effective at processing large amounts of experimental or calculated data in order to characterize an unknown quantum system, making its application useful in contexts including quantum information theory, quantum technologies development, and computational materials design. In this context, it can be used for example as a tool to interpolate pre-calculated interatomic potentials[10] or directly solving the Schrödinger equation with a variational method.[11]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Learning_to_rank\n",
      "Learning to rank[1] or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems.[2] Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. \"relevant\" or \"not relevant\") for each item. The goal of constructing the ranking model is to rank new, unseen lists in a similar way to rankings in the training data.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Similarity_learning\n",
      "Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Artificial_intelligence_in_industry\n",
      "Industrial artificial intelligence, or industrial AI, usually refers to the application of artificial intelligence to industry. Unlike general artificial intelligence which is a frontier research discipline to build computerized systems that perform tasks requiring human intelligence, industrial AI is more concerned with the application of such technologies to address industrial pain-points for customer value creation, productivity improvement, cost reduction, site optimization, predictive analysis[1]  and insight discovery.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Reinforcement_learning\n",
      "Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Unsupervised_learning\n",
      "Unsupervised learning is a paradigm in machine learning where, in contrast to supervised learning and semi-supervised learning, algorithms learn patterns exclusively from unlabeled data.\n",
      "\n",
      "https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning\n",
      "The International Conference on Machine Learning (ICML) is the leading international academic conference in machine learning. Along with NeurIPS and ICLR, it is one of the three primary conferences of high impact in machine learning and artificial intelligence research.[1] It is supported by the (IMLS). Precise dates vary year to year, but paper submissions are generally due at the end of January, and the conference is generally held the following July. The first ICML was held 1980 in Pittsburgh.[2][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Machine_perception\n",
      "Machine perception is the capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them.[1][2][3] The basic method that the computers take in and respond to their environment is through the attached hardware.  Until recently input was limited to a keyboard, or a mouse, but advances in technology, both in hardware and software, have allowed computers to take in sensory input in a way similar to humans.[1][2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Boosting_(machine_learning)\n",
      "In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance[1] in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones.[2] Boosting is based on the question posed by Kearns and Valiant (1988, 1989):[3][4] \"Can a set of weak learners create a single strong learner?\" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Grammar_induction\n",
      "Grammar induction (or grammatical inference)[1] is the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      "Decision tree learning is a supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Multimodal_learning\n",
      "Multimodal learning, in context of machine learning, is deep learning from a combination of various modalities of data, often arising in real-world applications. An example of multi-modal data is data that combines text (typically represented as feature vector) with imaging data consisting of pixel intensities and annotation tags. As these modalities have fundamentally different statistical properties, combining them is non-trivial, which is why specialized modelling strategies and algorithms are required.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Self-supervised_learning\n",
      "Self-supervised learning (SSL) is a paradigm in machine learning where a model is trained on a task using the data itself to generate supervisory signals, rather than relying on external labels provided by humans. In the context of neural networks, self-supervised learning aims to leverage inherent structures or relationships within the input data to create meaningful training signals. SSL tasks are designed so that solving it requires capturing essential features or relationships in the data. The input data is typically augmented or transformed in a way that creates pairs of related samples. One sample serves as the input, and the other is used to formulate the supervisory signal. This augmentation can involve introducing noise, cropping, rotation, or other transformations. Self-supervised learning more closely imitates the way humans learn to classify objects.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Machine_ethics\n",
      "Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence, otherwise known as artificial intelligent agents.[1] Machine ethics differs from other ethical fields related to engineering and technology.  Machine ethics should not be confused with computer ethics, which focuses on human use of computers. It should also be distinguished from the philosophy of technology, which concerns itself with the grander social effects of technology.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Predictive_analytics\n",
      "Predictive analytics is a form of business analytics applying machine learning to generate a predictive model for certain business applications. As such, it encompasses a variety of statistical techniques from predictive modeling and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events.[1] It represents a major subset of machine learning applications; in some contexts, it is synonymous with machine learning.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Explainable_artificial_intelligence\n",
      "Explainable AI (XAI), often overlapping with Interpretable AI, or Explainable Machine Learning (XML), either refers to an AI system over which it is possible for humans to retain intellectual oversight, or to the methods to achieve this.[1] The main focus is usually on the reasoning behind the decisions or predictions made by the AI[2] which are made more understandable and transparent.[3] XAI counters the \"black box\" tendency of machine learning, where even the AI's designers cannot explain why it arrived at a specific decision.[4][5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Neural_machine_translation\n",
      "Neural machine translation (NMT) is an approach to machine translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modeling and then translating entire sentences in a single integrated model.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Learning_classifier_system\n",
      "Learning classifier systems, or LCS, are a paradigm of rule-based machine learning methods that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning).[2]  Learning classifier systems seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions (e.g. behavior modeling,[3] classification,[4][5] data mining,[5][6][7] regression,[8] function approximation,[9] or game strategy).  This approach allows complex solution spaces to be broken up into smaller, simpler parts.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Fairness_(machine_learning)\n",
      "Fairness in machine learning refers to the various attempts at correcting algorithmic bias in automated decision processes based on machine learning models. Decisions made by computers after a machine-learning process may be considered unfair if they were based on variables considered sensitive. Examples of these kinds of variable include gender, ethnicity, sexual orientation, disability and more. As it is the case with many ethical concepts, definitions of fairness and bias are always controversial. In general, fairness and bias are considered relevant when the decision process impacts people's lives. In machine learning, the problem of algorithmic bias is well known and well studied. Outcomes may be skewed by a range of factors and thus might be considered unfair with respect to certain groups or individuals. An example would be the way social media sites deliver personalized news to consumers.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Statistical_learning_in_language_acquisition\n",
      "Statistical learning is the ability for humans and other animals to extract statistical regularities from the world around them to learn about the environment. Although statistical learning is now thought to be a generalized learning mechanism, the phenomenon was first identified in human infant language acquisition.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Kernel_machines\n",
      "In machine learning, kernel machines are a class of algorithms for pattern analysis, whose best known member is the support-vector machine (SVM). These methods involve using linear classifiers to solve nonlinear problems.[1] The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over all pairs of data points computed using inner products. The feature map in kernel machines is infinite dimensional but only requires a finite dimensional matrix from user-input according to the Representer theorem. Kernel machines are slow to compute for datasets larger than a couple of thousand examples without parallel processing. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Data_mining\n",
      "Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Generalization_(learning)\n",
      "Generalization is the concept that humans, other animals, and artificial neural networks use past learning in present situations of learning if the conditions in the situations are regarded as similar.[1] The learner uses generalized patterns, principles, and other similarities between past experiences and novel experiences to more efficiently navigate the world.[2] For example, if a person has learned in the past that every time they eat an apple, their throat becomes itchy and swollen, they might assume they are allergic to all fruit. When this person is offered a banana to eat, they reject it upon assuming they are also allergic to it through generalizing that all fruits cause the same reaction. Although this generalization about being allergic to all fruit based on experiences with one fruit could be correct in some cases, it may not be correct in all. Both positive and negative effects have been shown in education through learned generalization and its contrasting notion of discrimination learning.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ensemble_Averaging\n",
      "In machine learning, particularly in the creation of artificial neural networks, ensemble averaging is the process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models \"average out.\"\n",
      "\n",
      "https://en.wikipedia.org/wiki/Support_vector_machine\n",
      "In machine learning, support vector machines (SVMs, also support vector networks[1]) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995,[1] Vapnik et al., 1997[citation needed]) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Federated_learning\n",
      "Federated learning (also known as collaborative learning) is a machine learning technique that trains an algorithm via multiple independent sessions, each using its own dataset. This approach stands in contrast to traditional centralized machine learning techniques where local datasets are merged into one training session, as well as to approaches that assume that local data samples are identically distributed.\n",
      "\n",
      "https://en.wikipedia.org/wiki/ML.NET\n",
      "ML.NET is a free software machine learning library for the C# and F# programming languages.[4][5][6] It also supports Python models when used together with NimbusML. The preview release of ML.NET included transforms for feature engineering like n-gram creation, and learners to handle binary classification, multi-class classification, and regression tasks.[7] Additional ML tasks like anomaly detection and recommendation systems have since been added, and other approaches like deep learning will be included in future versions.[8][9]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Apprenticeship_learning\n",
      "In artificial intelligence, apprenticeship learning (or learning from demonstration or imitation learning) is the process of learning by observing an expert.[1][2] It can be viewed as a form of supervised learning, where the training dataset consists of task executions by a demonstration teacher.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Linear_classifier\n",
      "In the field of machine learning, the goal of statistical classification is to use an object's characteristics to identify which class (or group) it belongs to.  A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics.  An object's characteristics are also known as feature values and are typically presented to the machine in a vector called a feature vector. Such classifiers work well for practical problems such as document classification, and more generally for problems with many variables (features), reaching accuracy levels comparable to non-linear classifiers while taking less time to train and use.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_linguistics\n",
      "Computational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon linguistics, computer science, artificial intelligence, mathematics, logic, philosophy, cognitive science, cognitive psychology, psycholinguistics, anthropology and neuroscience, among others.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:WhatLinksHere/Machine_learning\n",
      "The following pages link to Machine learning \n",
      "\n",
      "https://en.wikipedia.org/wiki/Automated_decision-making\n",
      "Automated decision-making (ADM) involves the use of data, machines and algorithms to make decisions in a range of contexts, including public administration, business, health, education, law, employment, transport, media and entertainment, with varying degrees of human oversight or intervention. ADM involves large-scale data from a range of sources, such as databases, text, social media, sensors, images or speech, that is processed using various technologies including computer software, algorithms, machine learning, natural language processing, artificial intelligence, augmented intelligence and robotics. The increasing use of automated decision-making systems (ADMS) across a range of contexts presents many benefits and challenges to human society requiring consideration of the technical, legal, ethical, societal, educational, economic and health consequences.[1][2][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Large_language_model\n",
      "A large language model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by using massive amounts of data to learn billions of parameters during training and consuming large computational resources during their training and operation.[1] LLMs are artificial neural networks (mainly transformers[2]) and are (pre-)trained using self-supervised learning and semi-supervised learning.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Nature_Machine_Intelligence\n",
      "Nature Machine Intelligence is a monthly peer-reviewed scientific journal published by Nature Portfolio covering machine learning and artificial intelligence. The editor-in-chief is Liesbeth Venema.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Automated_planning_and_scheduling\n",
      "Automated planning and scheduling, sometimes denoted as simply AI planning,[1] is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Artificial_immune_system\n",
      "In artificial intelligence, artificial immune systems (AIS) are a class of computationally intelligent, rule-based machine learning systems inspired by the principles and processes of the vertebrate immune system. The algorithms are typically modeled after the immune system's characteristics of learning and memory for use in problem-solving.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Generative_model\n",
      "In statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent,[a] but three major types can be distinguished, following Jebara (2004):\n",
      "\n",
      "https://en.wikipedia.org/wiki/Scikit-learn\n",
      "scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language.[3]\n",
      "It features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Outline_of_artificial_intelligence\n",
      "The following outline is provided as an overview of and topical guide to artificial intelligence:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Natural-language_understanding\n",
      "Natural-language understanding (NLU) or natural-language interpretation (NLI)[1] is a subtopic  of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research\n",
      "The Journal of Machine Learning Research is a peer-reviewed open access scientific journal covering machine learning. It was established in 2000 and the first editor-in-chief was Leslie Kaelbling.[1] The current editors-in-chief are Francis Bach (Inria) and David Blei (Columbia University).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Feature_vector\n",
      "In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon.[1] Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of \"feature\" is related to that of explanatory variable used in statistical techniques such as linear regression.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mila_(research_institute)\n",
      "Mila - Quebec AI Institute (originally Montreal Institute for Learning Algorithms) is a research institute in Montreal, Quebec, focusing mainly on machine learning research. Approximately 1000 students and researchers and 100 faculty members, were part of Mila in 2022.[1] Mila is part of the Pan-Canadian AI Strategy. [2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Template_talk:Machine_learning\n",
      "This section title and contents seem pretty much random to me. How are contents chosen? One regression, one random clustering algorithm, 4 standard classificators; but no decision tree; which is probably the grandfather of all classificators. --Chire (talk) 12:41, 22 October 2013 (UTC)Reply[reply]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Multilayer_perceptron\n",
      "A multilayer perceptron (MLP) is a misnomer for a modern feedforward artificial neural network, consisting of fully connected neurons with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not linearly separable.[1] It is a misnomer because the original perceptron used a Heaviside step function, instead of a nonlinear kind of activation function (used by modern networks).\n",
      "\n",
      "https://en.wikipedia.org/wiki/General_game_playing\n",
      "General game playing (GGP) is the design of artificial intelligence programs to be able to play more than one game successfully.[1][2][3] For many games like chess, computers are programmed to play these games using a specially designed algorithm, which cannot be transferred to another context. For instance, a chess-playing computer program cannot play checkers. General game playing is considered as a necessary milestone on the way to artificial general intelligence.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Semantic_analysis_(machine_learning)\n",
      "In machine learning, semantic analysis of a corpus is the task of building structures that approximate concepts from a large set of documents. It generally does not involve prior semantic understanding of the documents. A metalanguage based on predicate logic can analyze the speech of humans.[1]: 93-  Another strategy to understand the semantics of a text is symbol grounding. If language is grounded, it is equal to recognizing a machine readable meaning. For the restricted domain of spatial analysis, a computer based language understanding system was demonstrated.[2]: 123 \n",
      "\n",
      "https://en.wikipedia.org/wiki/The_Master_Algorithm\n",
      "The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Neural_Turing_machine\n",
      "A neural Turing machine (NTM) is a recurrent neural network model of a Turing machine. The approach was published by Alex Graves et al. in 2014.[1] NTMs combine the fuzzy pattern matching capabilities of neural networks with the algorithmic power of programmable computers. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Structured_prediction\n",
      "Structured prediction or structured (output) learning is an umbrella term for supervised machine learning techniques that involves predicting structured objects, rather than scalar discrete or real values.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Attention_(machine_learning)\n",
      "Machine learning-based attention is a mechanism mimicking cognitive attention. It calculates \"soft\" weights for each word, more precisely for its embedding, in the context window. It can do it either in parallel (such as in transformers) or sequentially (such as recurrent neural networks). \"Soft\" weights can change during each runtime, in contrast to \"hard\" weights, which are (pre-)trained and fine-tuned and remain frozen afterwards.  \n",
      "\n",
      "https://en.wikipedia.org/wiki/LightGBM\n",
      "LightGBM, short for light gradient-boosting machine, is a free and open-source distributed gradient-boosting framework for machine learning, originally developed by Microsoft.[4][5] It is based on decision tree algorithms and used for ranking, classification and other machine learning tasks. The development focus is on performance and scalability.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence\n",
      "In artificial intelligence, symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search.[1] Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems),  symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to seminal ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Sentiment_analysis\n",
      "\n",
      "Sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. With the rise of deep language models, such as RoBERTa, also more difficult data domains can be analyzed, e.g., news texts where authors typically express their opinion/sentiment less explicitly.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ontology_learning\n",
      "Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computer_vision\n",
      "Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions.[1][2][3][4] Understanding in this context means the transformation of visual images (the input to the retina in the human analog) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Expert_system\n",
      "In artificial intelligence, an expert system is a computer system emulating the decision-making ability of a human expert.[1]\n",
      "Expert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural code.[2] The first expert systems were created in the 1970s and then proliferated in the 1980s.[3] Expert systems were among the first truly successful forms of artificial intelligence (AI) software.[4][5][6][7][8] \n",
      "An expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence\n",
      "\n",
      "\n",
      "\"Computing Machinery and Intelligence\" is a seminal paper written by Alan Turing on the topic of artificial intelligence. The paper, published in 1950 in Mind, was the first to introduce his concept of what is now known as the Turing test to the general public.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Affective_computing\n",
      "Affective computing is the study and development of systems and devices that can recognize, interpret, process, and simulate human affects. It is an interdisciplinary field spanning computer science, psychology, and cognitive science.[1] While some core ideas in the field may be traced as far back as to early philosophical inquiries into emotion,[2] the more modern branch of computer science originated with Rosalind Picard's 1995 paper[3] on affective computing and her book Affective Computing[4] published by MIT Press.[5][6] One of the motivations for the research is the ability to give machines emotional intelligence, including to simulate empathy. The machine should interpret the emotional state of humans and adapt its behavior to them, giving an appropriate response to those emotions.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Backpropagation\n",
      "As a machine-learning algorithm, backpropagation is a crucial step in a common method used to iteratively train a neural network model. It is used to calculate the necessary parameter adjustments, to gradually minimize error.\n",
      "\n",
      "https://en.wikipedia.org/wiki/AI_safety\n",
      "AI safety is an interdisciplinary field concerned with preventing accidents, misuse, or other harmful consequences that could result from artificial intelligence (AI) systems. It encompasses machine ethics and AI alignment, which aim to make AI systems moral and beneficial, and AI safety encompasses technical problems including monitoring systems for risks and making them highly reliable. Beyond AI research, it involves developing norms and policies that promote safety.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Neural_Designer\n",
      "Neural Designer is a software tool for machine learning based on neural networks, a main area of artificial intelligence research, and contains a graphical user interface which simplifies data entry and interpretation of results.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Learning_curve_(machine_learning)\n",
      "In machine learning, a learning curve (or training curve) plots the optimal value of a model's loss function for a training set against this loss function evaluated on a validation data set with same parameters as produced the optimal function.[1] Synonyms include error curve, experience curve, improvement curve and generalization curve.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Probabilistic_classification\n",
      "In machine learning, a probabilistic classifier is a classifier that is able to predict, given an observation of an input, a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to. Probabilistic classifiers provide classification that can be useful in its own right[1] or when combining classifiers into ensembles.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Artificial_intelligence_in_government\n",
      "Artificial intelligence (AI) has a range of uses in government. It can be used to further public policy objectives (in areas such as emergency services, health and welfare), as well as assist the public to interact with the government  (through the use of virtual assistants, for example). According to the Harvard Business Review, \"Applications of artificial intelligence to the public sector are broad and growing, with early experiments taking place around the world.\"[1] Hila Mehr from the Ash Center for Democratic Governance and Innovation at Harvard University notes that AI in government is not new, with postal services using machine methods in the late 1990s to recognise handwriting on envelopes to automatically route letters.[2] The use of AI in government comes with significant benefits, including efficiencies resulting in cost savings (for instance by reducing the number of front office staff), and reducing the opportunities for corruption.[3] However, it also carries risks.[citation needed][further explanation needed]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:EditPage/Template:Machine_learning\n",
      "Copy and paste: – — ° ′ ″ ≈ ≠ ≤ ≥ ± − × ÷ ← → · §   Sign your posts on talk pages: ~~~~   Cite your sources: <ref></ref> \n",
      "\n",
      "https://en.wikipedia.org/wiki/Semi-supervised_learning\n",
      "Weak supervision is a paradigm in machine learning, the relevance and notability of which increased with the advent of large language models due to large amount of data required to train them.  It is characterized by using a combination of a small amount of human-labeled data (exclusively used in more expensive and time-consuming supervised learning paradigm), followed by a large amount of unlabeled data (used exclusively in unsupervised learning paradigm). In other words, the desired output values are provided only for a subset of the training data. The remaining data is unlabeled or imprecisely labeled. Intuitively, it can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam. Technically, it could be viewed as performing clustering and then labeling the clusters with the labeled data, pushing the decision boundary away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Template:Artificial_intelligence\n",
      "This template shows topics in the area of artificial intelligence.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Perceptron\n",
      "In machine learning, the perceptron (or McCulloch-Pitts neuron) is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.[1]  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence\n",
      "The philosophy of artificial intelligence is a branch of the philosophy of mind and the philosophy of computer science[1] that explores artificial intelligence and its implications for knowledge and understanding of intelligence, ethics, consciousness, epistemology, and free will.[2][3] Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; see artificial life) so the discipline is of considerable interest to philosophers.[4] These factors contributed to the emergence of the philosophy of artificial intelligence. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Manifold_regularization\n",
      "In machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is smooth: data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Modeling_language\n",
      "A modeling language is any artificial language that can be used to express data, information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure Programing language.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Outline_of_computer_science\n",
      "Computer science (also called computing science) is the study of the theoretical foundations of information and computation and their implementation and application in computer systems. One well known subject classification system for computer science is the ACM Computing Classification System devised by the Association for Computing Machinery.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Regularization_(mathematics)\n",
      "In mathematics, statistics, finance,[1] computer science, particularly in machine learning and inverse problems, regularization is a process that changes the result answer to be \"simpler\". It is often used to obtain results for ill-posed problems or to prevent overfitting.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Meta_AI\n",
      "Meta AI is an artificial intelligence laboratory that belongs to Meta Platforms Inc. (formerly known as Facebook, Inc.)[1] Meta AI intends to develop various forms of artificial intelligence, improving augmented and artificial reality technologies.[2] Meta AI is an academic research laboratory focused on generating knowledge for the AI community.[3] This is in contrast to Facebook's Applied Machine Learning (AML) team, which focuses on practical applications of its products.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Graph_neural_network\n",
      "A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs.[1][2][3][4][5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Association_rule_learning\n",
      "Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.[1] In any given transaction with a variety of items, association rules are meant to discover the rules that determine how or why certain items are connected.\n",
      "\n",
      "https://en.wikipedia.org/wiki/AI_takeover\n",
      "An AI takeover is a hypothetical scenario in which artificial intelligence (AI) becomes the dominant form of intelligence on Earth, as computer programs or robots effectively take control of the planet away from the human species. Possible scenarios include replacement of the entire human workforce, takeover by a superintelligent AI, and the popular notion of a robot uprising. Stories of AI takeovers are very popular throughout science fiction. Some public figures, such as Stephen Hawking and Elon Musk, have advocated research into precautionary measures to ensure future superintelligent machines remain under human control.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Word2vec\n",
      "Word2vec is a technique for natural language processing (NLP) published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that they capture the semantic and syntactic qualities of words; as such, a simple mathematical function (cosine similarity) can indicate the level of semantic similarity between the words represented by those vectors.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Inductive_bias\n",
      "The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered.[1]\n",
      "Inductive bias is anything which makes the algorithm learn one pattern instead of another pattern (e.g. step-functions in decision trees instead of continuous function in a linear regression model).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Hyperparameter_optimization\n",
      "In machine learning, hyperparameter optimization[1] or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Arthur_Samuel_(computer_scientist)\n",
      "Arthur Lee Samuel (December 5, 1901 – July 29, 1990)[3] was an American pioneer in the field of computer gaming and artificial intelligence.[2] He popularized the term \"machine learning\" in 1959.[4] The Samuel Checkers-playing Program was among the world's first successful self-learning programs, and as such a very early demonstration of the fundamental concept of artificial intelligence (AI).[5] He was also a senior member in the TeX community who devoted much time giving personal attention to the needs of users and wrote an early TeX manual in 1983.[6]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning\n",
      "Knowledge representation and reasoning (KRR, KR&R, KR²) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can use to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. Knowledge representation incorporates findings from psychology[1] about how humans solve problems and represent knowledge in order to design formalisms that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from logic to automate various kinds of reasoning, such as the application of rules or the relations of sets and subsets.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Relevance_vector_machine\n",
      "In mathematics, a Relevance Vector Machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification.[1]\n",
      "The RVM has an identical functional form to the support vector machine, but provides probabilistic classification.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mallet_(software_project)\n",
      "MALLET is a Java \"Machine Learning for Language Toolkit\".\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence\n",
      "The ethics of artificial intelligence is the branch of the ethics of technology specific to artificially intelligent systems.[1] It is sometimes divided into a concern with the moral behavior of humans as they design, make, use and treat artificially intelligent systems, and a concern with the behavior of machines, in machine ethics. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Differentiable_neural_computer\n",
      "In artificial intelligence, a differentiable neural computer (DNC) is a memory augmented neural network architecture (MANN), which is typically (but not by definition) recurrent in its implementation. The model was published in 2016 by Alex Graves et al. of DeepMind.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach\n",
      "Artificial Intelligence: A Modern Approach (AIMA) is a university textbook on artificial intelligence, written by Stuart J. Russell and Peter Norvig. It was first published in 1995 and the fourth edition of the book was released on 28 April 2020.[1] It is used in over 1400 universities worldwide[2] and has been called \"the most popular artificial intelligence textbook in the world\".[3] It is considered the standard text in the field of artificial intelligence.[4][5]\n",
      "The book is intended for an undergraduate audience but can also be used for graduate-level studies with the suggestion of adding some of the primary sources listed in the extensive bibliography.  Programs in the book are presented in pseudo code with implementations in Java, Python, Lisp, JavaScript and Scala available online.[6][7] There are also unsupported implementations in Prolog, C++, C#, and several other languages.\n",
      "\n",
      "https://en.wikipedia.org/wiki/S2CID_(identifier)\n",
      "Semantic Scholar is a research tool powered by artificial intelligence for scientific literature. It was developed at the Allen Institute for AI and publicly released in November 2015.[2] It uses advances in natural language processing to provide summaries for scholarly papers.[3] The Semantic Scholar team is actively researching the use of artificial intelligence in natural language processing, machine learning, human–computer interaction, and information retrieval.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Pattern_recognition\n",
      "Pattern recognition is the automated recognition of patterns and regularities in data. While similar, pattern recognition (PR) is not to be confused with pattern machines (PM) which may possess (PR) capabilities but their primary function is to distinguish and create emergent  pattern.   PR has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Feature_engineering\n",
      "Feature engineering or feature extraction  or feature discovery is the process of extracting features (characteristics, properties, attributes) from raw data.[1] This can be done with deep learning networks such as convolutional neural networks that are able to learn features by themselves.[citation needed]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Differentiable_programming\n",
      "Differentiable programming is a programming paradigm in which a numeric computer program can be differentiated throughout via automatic differentiation.[1][2][3][4][5] This allows for gradient-based optimization of parameters in the program, often via gradient descent, as well as other learning approaches that are based on higher order derivative information. Differentiable programming has found use in a wide variety of areas, particularly scientific computing and artificial intelligence.[5] One of the early proposals to adopt such a framework in a systematic fashion to improve upon learning algorithms was made by the Advanced Concepts Team at the European Space Agency in early 2016.[6]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_mathematics\n",
      "\n",
      "Computational mathematics is an area of mathematics devoted to the interaction between mathematics and computer computation.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Music_and_artificial_intelligence\n",
      "Artificial intelligence and music (AIM) is a common subject in the International Computer Music Conference, the Computing Society Conference[1] and the International Joint Conference on Artificial Intelligence. The first International Computer Music Conference (ICMC) was held in 1974 at Michigan State University.[2] Current research includes the application of AI in music composition, performance, theory and digital sound processing.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Deep_learning_speech_synthesis\n",
      "Deep learning speech synthesis uses Deep Neural Networks (DNN) to produce\n",
      "artificial speech from text (text-to-speech) or spectrum (vocoder).\n",
      "The deep neural networks are trained using a large amount of recorded speech and, in the case of\n",
      "a text-to-speech system, the associated labels and/or input text.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Speech_recognition\n",
      "\n",
      "Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Hybrid_intelligent_system\n",
      "\n",
      "Hybrid intelligent system denotes a software system which employs, in parallel, a combination of methods and techniques from artificial intelligence subfields, such as:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\n",
      "A transformer is a deep learning architecture, initially proposed in 2017, that relies on the parallel multi-head attention mechanism.[1] It is notable for requiring less training time than previous recurrent neural architectures, such as long short-term memory (LSTM),[2] and its later variation has been prevalently adopted for training large language models on large (language) datasets, such as the Wikipedia corpus and Common Crawl, by virtue of the parallelized processing of input sequence.[3]\n",
      "Input text is split into n-grams encoded as tokens and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished. Though the transformer paper was published in 2017, the softmax-based attention mechanism was proposed in 2014 for machine translation,[4][5] and the Fast Weight Controller, similar to a transformer, was proposed in 1992.[6][7][8]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_engineering\n",
      "Computational Engineering is an emerging discipline that deals with the development and application of computational models for engineering, known as Computational Engineering Models[1] or CEM. Computational engineering uses computers to solve engineering design problems important to a variety of industries.[2] At this time, various different approaches are summarized under the term Computational Engineering, including using computational geometry and virtual design for engineering tasks,[3][4] often coupled with a simulation-driven approach[5] In Computational Engineering, algorithms solve mathematical and logical models[6] that describe engineering challenges, sometimes coupled with some aspect of AI, specifically Reinforcement Learning.[7]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Inductive_programming\n",
      "Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Data_augmentation\n",
      "Data augmentation is a technique in machine learning used to reduce overfitting when training a machine learning model,[1] by training models on several slightly-modified copies of existing data.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Recurrent_neural_network\n",
      "A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs[1][2][3] makes them applicable to tasks such as unsegmented, connected handwriting recognition[4] or speech recognition.[5][6] The term \"recurrent neural network\" is used to refer to the class of networks with an infinite impulse response, whereas \"convolutional neural network\" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior.[7] A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mlpack\n",
      "mlpack is a machine learning software library for C++, built on top of the Armadillo library and the ensmallen numerical optimization library.[3] mlpack has an emphasis on scalability, speed, and ease-of-use. Its aim is to make machine learning possible for novice users by means of a simple, consistent API, while simultaneously exploiting C++ language features to provide maximum performance and maximum flexibility for expert users.[4] Its intended target users are scientists and engineers.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Situated_approach_(artificial_intelligence)\n",
      "In artificial intelligence research, the situated approach builds agents that are designed to behave effectively successfully in their environment. This requires designing AI \"from the bottom-up\" by focussing on the basic perceptual and motor skills required to survive. The situated approach gives a much lower priority to abstract reasoning or problem-solving skills.\n",
      "\n",
      "https://en.wikipedia.org/wiki/MIT_Computer_Science_and_Artificial_Intelligence_Laboratory\n",
      "Computer Science and Artificial Intelligence Laboratory (CSAIL) is a research institute at the Massachusetts Institute of Technology (MIT) formed by the 2003 merger of the Laboratory for Computer Science (LCS) and the Artificial Intelligence Laboratory (AI Lab). Housed within the Ray and Maria Stata Center, CSAIL is the largest on-campus laboratory as measured by research scope and membership. It is part of the Schwarzman College of Computing[1] but is also overseen by the MIT Vice President of Research.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Seq2seq\n",
      "Seq2seq is a family of machine learning approaches used for natural language processing.[1] Applications include language translation, image captioning, conversational models, and text summarization.[2]\n",
      "Seq2seq uses sequence transformation: it turns one sequence into another sequence.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Torch_(machine_learning)\n",
      "Torch is an open-source machine learning library, \n",
      "a scientific computing framework, and a scripting language based on Lua.[3] It provides LuaJIT interfaces to deep learning algorithms implemented in C. It was created at IDIAP at EPFL. Torch development moved in 2017 to PyTorch, a port of the library to Python.[4][5][better source needed]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mathematical_optimization\n",
      "Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criterion, from some set of available alternatives.[1] It is generally divided into two subfields: discrete optimization and continuous optimization. Optimization problems arise in all quantitative disciplines from computer science and engineering[2] to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Convolutional_neural_network\n",
      "Convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters (or kernel) optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections.[1][2] For example, for each neuron in the fully-connected layer 10,000 weights would be required for processing an image sized 100 × 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels,[3][4]  only 25 neurons are required to process 5x5-sized tiles.[5][6] Higher-layer features are extracted  from wider context windows, compared to lower-layer features.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mathematica\n",
      "Wolfram Mathematica is a software system with built-in libraries for several areas of technical computing that allow machine learning, statistics, symbolic computation, data manipulation, network analysis, time series analysis, NLP, optimization, plotting functions and various types of data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other programming languages. It was conceived by Stephen Wolfram, and is developed by Wolfram Research of Champaign, Illinois.[8][9] The Wolfram Language is the programming language used in Mathematica.[10] Mathematica 1.0 was released on June 23, 1988 in Champaign, Illinois and Santa Clara, California.[11][12][13]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Programming_language_theory\n",
      "Programming language theory (PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of formal languages known as programming languages. Programming language theory is closely related to other fields including mathematics, software engineering, and linguistics. There are a number of academic conferences and journals in the area.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Approximate_computing\n",
      "Approximate computing is an emerging paradigm for energy-efficient and/or high-performance design.[1] It includes a plethora of computation techniques that return a possibly inaccurate result rather than a guaranteed accurate result, and that can be used for applications where an approximate result is sufficient for its purpose.[2] One example of such situation is for a search engine where no exact answer may exist for a certain search query and hence, many answers may be acceptable. Similarly, occasional dropping of some frames in a video application can go undetected due to perceptual limitations of humans. Approximate computing is based on the observation that in many scenarios, although performing exact computation requires large amount of resources, allowing bounded approximation can provide disproportionate gains in performance and energy, while still achieving acceptable result accuracy.[clarification needed]  For example, in k-means clustering algorithm, allowing only 5% loss in classification accuracy can provide 50 times energy saving compared to the fully accurate classification.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Theoretical_computer_science\n",
      "Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, formal language theory, the lambda calculus and type theory.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Yooreeka\n",
      "Yooreeka is a library for data mining, machine learning, soft computing, and mathematical analysis. The project started with the code of the book \"Algorithms of the Intelligent Web\".[1] Although the term \"Web\" prevailed in the title, in essence, the algorithms are valuable in any software application.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Watson_(computer)\n",
      "IBM Watson is a computer system capable of answering questions posed in natural language.[1] It was developed in IBM's DeepQA project by a research team led by principal investigator David Ferrucci.[2] Watson was named after IBM's founder and first CEO, industrialist Thomas J. Watson.[3][4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Probably_approximately_correct_learning\n",
      "In computational learning theory, probably approximately correct (PAC) learning is a framework for mathematical analysis of machine learning. It was proposed in 1984 by Leslie Valiant.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Search_algorithm\n",
      "In computer science, a search algorithm is an algorithm designed to solve a search problem. Search algorithms work to retrieve information stored within particular data structure, or calculated in the search space of a problem domain, with either discrete or continuous values.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Multi-agent_reinforcement_learning\n",
      "Multi-agent reinforcement learning (MARL) is a sub-field of reinforcement learning. It focuses on studying the behavior of multiple learning agents that coexist in a shared environment.[1] Each agent is motivated by its own rewards, and does actions to advance its own interests; in some environments these interests are opposed to the interests of other agents, resulting in complex group dynamics.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Learning\n",
      "Learning is the process of acquiring new or modifying existing knowledge, behaviors, skills, values, or preferences based on instruction.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Artificial_neural_networks\n",
      "This category are for articles about artificial neural networks (ANN).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Regression_analysis\n",
      "In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome' or 'response' variable, or a 'label' in machine learning parlance) and one or more independent variables (often called 'predictors', 'covariates', 'explanatory variables' or 'features'). The most common form of regression analysis is linear regression, in which one finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared differences between the true data and that line (or hyperplane). For specific mathematical reasons (see linear regression), this allows the researcher to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values. Less common forms of regression use slightly different procedures to estimate alternative location parameters (e.g., quantile regression or Necessary Condition Analysis[1]) or estimate the conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_statistics\n",
      "Computational statistics, or statistical computing, is the bond between statistics and computer science, and refers to the statistical methods that are enabled by using computational methods. It is the area of computational science (or scientific computing) specific to the mathematical science of statistics. This area is also developing rapidly, leading to calls that a broader concept of computing should be taught as part of general statistical education.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Portal:Computer_programming\n",
      "Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks. It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages. Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit. Proficient programming usually requires expertise in several different subjects, including knowledge of the application domain, details of programming languages and generic code libraries, specialized algorithms, and formal logic.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Feedforward_neural_network\n",
      "A feedforward neural network (FNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers.[2] Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops,[2] in contrast to recurrent neural networks,[3] which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method[4][5][6][7][8] and are colloquially referred to as the \"vanilla\" neural networks.[9]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Knowledge_graph_embedding\n",
      "In representation learning, knowledge graph embedding (KGE), also referred to as knowledge representation learning (KRL), or multi-relation learning,[1] is a machine learning task of learning a low-dimensional representation of a knowledge graph's entities and relations while preserving their semantic meaning.[1][2][3]  Leveraging their embedded representation, knowledge graphs (KGs) can be used for various applications such as link prediction, triple classification, entity recognition, clustering, and relation extraction.[1][4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Distributed_artificial_intelligence\n",
      "Distributed Artificial Intelligence (DAI) also called Decentralized Artificial Intelligence[1] is a subfield of artificial intelligence research dedicated to the development of distributed solutions for problems. DAI is closely related to and a predecessor of the field of multi-agent systems. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Predictive_modeling\n",
      "Predictive modelling uses statistics to predict outcomes.[1] Most often the event one wants to predict is in the future, but predictive modelling can be applied to any type of unknown event, regardless of when it occurred. For example, predictive models are often used to detect crimes and identify suspects, after the crime has taken place.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Occam_learning\n",
      "In computational learning theory, Occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to probably approximately correct (PAC) learning, where the learner is evaluated on its predictive power of a test set.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Weka_(machine_learning)\n",
      "Waikato Environment for Knowledge Analysis (Weka) is a collection of machine learning and data analysis free software licensed under the GNU General Public License. It was developed at the University of Waikato, New Zealand and is the companion software to the book \"Data Mining: Practical Machine Learning Tools and Techniques\".[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Theory_of_computation\n",
      "In theoretical computer science and mathematics, the theory of computation is the branch that deals with what problems can be solved on a model of computation, using an algorithm, how efficiently they can be solved or to what degree (e.g., approximate solutions versus precise ones).  The field is divided into three major branches: automata theory and formal languages, computability theory, and computational complexity theory, which are linked by the question: \"What are the fundamental capabilities and limitations of computers?\".[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Evolutionary_algorithm\n",
      "In computational intelligence (CI), an evolutionary algorithm (EA) is a subset of evolutionary computation,[1] a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_biology\n",
      "Computational biology refers to the use of data analysis, mathematical modeling and computational simulations to understand biological systems and relationships.[1] An intersection of computer science, biology, and big data, the field also has foundations in applied mathematics, chemistry, and genetics.[2] It differs from biological computing, a subfield of computer science and engineering which uses bioengineering to build computers.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Knowledge_discovery\n",
      "Knowledge extraction is the creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criterion is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Long_short-term_memory\n",
      "Long short-term memory (LSTM)[1] network is a recurrent neural network (RNN), aimed to deal with the vanishing gradient problem[2] present in traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus \"long short-term memory\".[1] It is applicable to classification, processing and predicting data based on time series, such as in handwriting,[3] speech recognition,[4][5] machine translation,[6][7] speech activity detection,[8] robot control,[9][10] video games,[11][12] and healthcare.[13]\n",
      "\n",
      "https://en.wikipedia.org/wiki/NeuroSolutions\n",
      "NeuroSolutions is a neural network development environment developed by NeuroDimension. It combines a modular, icon-based (component-based) network design interface with an implementation of advanced learning procedures, such as conjugate gradients, the Levenberg-Marquardt algorithm, and backpropagation through time.[citation needed] The software is used to design, train and deploy neural network (supervised learning and unsupervised learning) models to perform a wide variety of tasks such as data mining, classification, function approximation, multivariate regression and time-series prediction.[citation needed]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Kubeflow\n",
      "Kubeflow is an open-source platform for machine learning and MLOps on Kubernetes introduced by Google. The different stages in a typical machine learning lifecycle are represented with different software components in Kubeflow, including model development (Kubeflow Notebooks[4]), model training (Kubeflow Pipelines,[5] Kubeflow Training Operator[6]), model serving (KServe[a][7]), and automated machine learning (Katib[8]).\n",
      "\n",
      "https://en.wikipedia.org/wiki/AAAI_Conference_on_Artificial_Intelligence\n",
      "The AAAI Conference on Artificial Intelligence (AAAI) is one of the leading international academic conference in artificial intelligence held annually.[1][2][3] Along with ICML, NeurIPS and ICLR, it is one of the primary conferences of high impact in machine learning and artificial intelligence research.[4] It is supported by the Association for the Advancement of Artificial Intelligence. Precise dates vary from year to year, but paper submissions are generally due at the end of August to beginning of September, and the conference is generally held during the following February. The first AAAI was held in 1980 at Stanford University, Stanford California.[5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Generalized_linear_model\n",
      "In statistics, a generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Connectionism\n",
      "Connectionism (coined by Edward Thorndike in the 1930s) is the name of an approach to the study of human mental processes and cognition that utilizes mathematical models known as connectionist networks or artificial neural networks.[1] Connectionism has had many 'waves' since its beginnings.\n",
      "\n",
      "https://en.wikipedia.org/wiki/ECML_PKDD\n",
      "ECML PKDD, the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, is one of the leading[1][2] academic conferences on machine learning and knowledge discovery, held in Europe every year.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Google_JAX\n",
      "Google JAX is a machine learning framework for transforming numerical functions.[1][2][3] It is described as bringing together a modified version of autograd (automatic obtaining of the gradient function through differentiation of a function) and TensorFlow's XLA (Accelerated Linear Algebra). It is designed to follow the structure and workflow of NumPy as closely as possible and works with various existing frameworks such as TensorFlow and PyTorch.[4][5] The primary functions of JAX are:[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Robot_control\n",
      "Robotic control is the system that contributes to the movement of robots. This involves the mechanical aspects and programmable systems that makes it possible to control robots. Robotics can be controlled by various means including manual, wireless, semi-autonomous (a mix of fully automatic and wireless control), and fully autonomous (using artificial intelligence).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_science\n",
      "\n",
      "Computational science, also known as scientific computing, technical computing or scientific computation (SC), is a division of science that uses advanced computing capabilities to understand and solve complex physical problems. This includes \n",
      "\n",
      "https://en.wikipedia.org/wiki/Mathematical_software\n",
      "Mathematical software is software used to model, analyze or calculate numeric, symbolic or geometric data.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Astroinformatics\n",
      "Astroinformatics is an interdisciplinary field of study involving the combination of astronomy, data science, machine learning, informatics, and information/communications technologies.[2][3] The field is closely related to astrostatistics.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Diffusion_model\n",
      "In machine learning, diffusion models, also known as diffusion probabilistic models or score-based generative models, are a class of generative models. The goal of diffusion models is to learn a diffusion process that generates the probability distribution of a given dataset. It mainly consists of three major components: the forward process, the reverse process, and the sampling procedure.[1] Three examples of generic diffusion modeling frameworks used in computer vision are denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
      "In statistics, naive Bayes classifiers are a family of linear \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features (see Bayes classifier). They are among the simplest Bayesian network models,[1] but coupled with kernel density estimation, they can achieve high accuracy levels.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Model_of_computation\n",
      "In computer science, and more specifically in computability theory and computational complexity theory, a model of computation is a model which describes how an output of a mathematical function is computed given an input. A model describes how units of computations, memories, and communications are organized.[1] The computational complexity of an algorithm can be measured given a model of computation. Using a model allows studying the performance of algorithms independently of the variations that are specific to particular implementations and specific technology.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Topic_modeling\n",
      "In statistics and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear approximately equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.\n",
      "\n",
      "https://en.wikipedia.org/wiki/OpenNN\n",
      "OpenNN (Open Neural Networks Library) is a software library written in the C++ programming language which implements neural networks, a main area of deep learning research.[1] The library is open-source, licensed under the GNU Lesser General Public License.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_complexity_theory\n",
      "\n",
      "In theoretical computer science and mathematics, computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.\n",
      "\n",
      "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
      "In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951,[1] and later expanded by Thomas Cover.[2] It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Microsoft_Cognitive_Toolkit\n",
      "Microsoft Cognitive Toolkit,[3] previously known as CNTK and sometimes styled as The Microsoft Cognitive Toolkit, is a deprecated[4] deep learning framework developed by Microsoft Research. Microsoft Cognitive Toolkit describes neural networks as a series of computational steps via a directed graph.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computer_graphics\n",
      "Computer graphics deals with generating images and art with the aid of computers. Today, computer graphics is a core technology in digital photography, film, video games, digital art, cell phone and computer displays, and many specialized applications. A great deal of specialized hardware and software has been developed, with the displays of most devices being driven by computer graphics hardware. It is a vast and recently developed area of computer science. The phrase was coined in 1960 by computer graphics researchers Verne Hudson and William Fetter of Boeing. It is often abbreviated as CG, or typically in the context of film as computer generated imagery (CGI). The non-artistic aspects of computer graphics are the subject of computer science research.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/LLaMA\n",
      "LLaMA (Large Language Model Meta AI) is a family of large language models (LLMs), released by Meta AI starting in February 2023. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Human-in-the-loop\n",
      "Human-in-the-loop  or HITL is used in multiple contexts. It can be defined as a model requiring human interaction.[1][2] HITL is associated with modeling and simulation (M&S) in the live, virtual, and constructive taxonomy. HITL along with the related human-on-the-loop are also used in relation to lethal autonomous weapons.[3] Further, HITL is used in the context of machine learning.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/MuZero\n",
      "MuZero is a computer program developed by artificial intelligence research company DeepMind to master games without knowing their rules.[1][2][3] Its release in 2019 included benchmarks of its performance in go, chess, shogi, and a standard suite of Atari games. The algorithm uses an approach similar to AlphaZero. It matched AlphaZero's performance in chess and shogi, improved on its performance in Go (setting a new world record), and improved on the state of the art in mastering a suite of 57 Atari games (the Arcade Learning Environment), a visually-complex domain.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Cluster_analysis\n",
      "Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.\n",
      "\n",
      "https://en.wikipedia.org/wiki/AI_winter\n",
      "\n",
      "In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1]  The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Reservoir_computing\n",
      "Reservoir computing is a framework for computation derived from recurrent neural network theory that maps input signals into higher dimensional computational spaces through the dynamics of a fixed, non-linear system called a reservoir.[1] After the input signal is fed into the reservoir, which is treated as a \"black box,\" a simple readout mechanism is trained to read the state of the reservoir and map it to the desired output.[1] The first key benefit of this framework is that training is performed only at the readout stage, as the reservoir dynamics are fixed.[1] The second is that the computational power of naturally available systems, both classical and quantum mechanical, can be used to reduce the effective computational cost.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_geometry\n",
      "Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with a history stretching back to antiquity.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Apache_Mahout\n",
      "Apache Mahout is a project of the Apache Software Foundation to produce free implementations of distributed or otherwise scalable machine learning algorithms focused primarily on linear algebra. In the past, many of the implementations use the Apache Hadoop platform, however today it is primarily focused on Apache Spark.[3][4] Mahout also provides Java/Scala libraries for common math operations (focused on linear algebra and statistics) and primitive Java collections. Mahout is a work in progress; a number of algorithms have been implemented.[5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/GPU\n",
      "A graphics processing unit (GPU) is a specialized electronic circuit initially designed to accelerate computer graphics and image processing (either on a video card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles). After their initial design, GPUs were found to be useful for non-graphic calculations involving embarrassingly parallel problems due to their parallel structure. Other non-graphical uses include the training of neural networks and cryptocurrency mining.\n",
      "\n",
      "https://en.wikipedia.org/wiki/List_of_datasets_in_computer_vision_and_image_processing\n",
      "This is a list of datasets for machine learning research. It is part of the list of datasets for machine-learning research. These datasets consist primarily of images or videos for tasks such as object detection, facial recognition, and multi-label classification.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_social_science\n",
      "Computational social science is the academic sub-discipline concerned with computational approaches to the social sciences. This means that computers are used to model, simulate, and analyze social phenomena.  Fields include computational economics, computational sociology, cliodynamics, culturomics, nonprofit studies,[1] and the automated analysis of contents, in social and traditional media. It focuses on investigating social and behavioral relationships and interactions through social simulation, modeling, network analysis, and media analysis.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_anatomy\n",
      "Computational anatomy is an interdisciplinary field of biology focused on quantitative investigation and modelling of anatomical shapes variability.[1][2] It involves the development and application of mathematical, statistical and data-analytical methods for modelling and simulation of biological structures.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Multilinear_subspace_learning\n",
      "Multilinear subspace learning is an approach for disentangling the causal factor of data formation and performing  dimensionality reduction.[1][2][3][4][5]   \n",
      "The Dimensionality reduction can be performed on a data tensor that contains a collection of observations have been vectorized,[1] or observations that are treated as matrices and concatenated into a data tensor.[6][7]  Here are some examples of data tensors whose observations are vectorized  or whose observations are matrices concatenated into data tensor images (2D/3D), video sequences (3D/4D), and hyperspectral cubes (3D/4D).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Deeplearning4j\n",
      "Eclipse Deeplearning4j is a programming library written in Java for the Java virtual machine (JVM).[2][3] It is a framework with wide support for deep learning algorithms.[4] Deeplearning4j includes implementations of the restricted Boltzmann machine, deep belief net, deep autoencoder, stacked denoising autoencoder and recursive neural tensor network, word2vec, doc2vec, and GloVe. These algorithms all include distributed parallel versions that integrate with Apache Hadoop and Spark.[5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Platt_scaling\n",
      "In machine learning, Platt scaling or Platt calibration is a way of transforming the outputs of a classification model into a probability distribution over classes. The method was invented by John Platt in the context of support vector machines,[1]\n",
      "replacing an earlier method by Vapnik,\n",
      "but can be applied to other classification models.[2]\n",
      "Platt scaling works by fitting a logistic regression model to a classifier's scores.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Loss_functions_for_classification\n",
      "In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to).[1]  Given \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "X\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\displaystyle {\\mathcal {X}}}\n",
      "\n",
      " as the space of all possible inputs (usually \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "X\n",
      "\n",
      "\n",
      "⊂\n",
      "\n",
      "\n",
      "R\n",
      "\n",
      "\n",
      "d\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\displaystyle {\\mathcal {X}}\\subset \\mathbb {R} ^{d}}\n",
      "\n",
      "), and \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Y\n",
      "\n",
      "\n",
      "=\n",
      "{\n",
      "−\n",
      "1\n",
      ",\n",
      "1\n",
      "}\n",
      "\n",
      "\n",
      "{\\displaystyle {\\mathcal {Y}}=\\{-1,1\\}}\n",
      "\n",
      " as the set of labels (possible outputs), a typical goal of classification algorithms is to find a function \n",
      "\n",
      "\n",
      "\n",
      "f\n",
      ":\n",
      "\n",
      "\n",
      "X\n",
      "\n",
      "\n",
      "→\n",
      "\n",
      "\n",
      "Y\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\displaystyle f:{\\mathcal {X}}\\to {\\mathcal {Y}}}\n",
      "\n",
      " which best predicts a label \n",
      "\n",
      "\n",
      "\n",
      "y\n",
      "\n",
      "\n",
      "{\\displaystyle y}\n",
      "\n",
      " for a given input \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x\n",
      "→\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\displaystyle {\\vec {x}}}\n",
      "\n",
      ".[2]  However, because of incomplete information, noise in the measurement, or probabilistic components in the underlying process, it is possible for the same \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x\n",
      "→\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\displaystyle {\\vec {x}}}\n",
      "\n",
      " to generate different \n",
      "\n",
      "\n",
      "\n",
      "y\n",
      "\n",
      "\n",
      "{\\displaystyle y}\n",
      "\n",
      ".[3]  As a result, the goal of the learning problem is to minimize expected loss (also known as the risk), defined as\n",
      "\n",
      "https://en.wikipedia.org/wiki/Whisper_(speech_recognition_system)\n",
      "Whisper is a machine learning model for speech recognition and transcription, created by OpenAI and first released as open-source software in September 2022.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
      "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Algorithmic_efficiency\n",
      "\n",
      "In computer science, algorithmic efficiency is a property of an algorithm which relates to the amount of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on the usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Statistical_model\n",
      "A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process.[1] When referring specifically to probabilities, the corresponding term is probabilistic model.\n",
      "\n",
      "https://en.wikipedia.org/wiki/MXNet\n",
      "Apache MXNet is an open-source deep learning software framework that trains and deploys deep neural networks. It is scalable, allows fast model training, and supports a flexible programming model and multiple programming languages (including C++, Python, Java, Julia, MATLAB, JavaScript, Go, R, Scala, Perl, and Wolfram Language). The MXNet library is portable and can scale to multiple GPUs[2] and machines. It was co-developed by Carlos Guestrin at the University of Washington (along with GraphLab).[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Email_filtering\n",
      "Email filtering is the processing of email to organize it according to specified criteria. The term can apply to the intervention of human intelligence, but most often refers to the automatic processing of messages at an SMTP server, possibly applying anti-spam techniques. Filtering can be applied to incoming emails as well as to outgoing ones.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Neural_Computation_(journal)\n",
      "Neural Computation is a monthly peer-reviewed scientific journal covering all aspects of neural computation, including modeling the brain and the design and construction of neurally-inspired information processing systems. It was established in 1989 and is published by MIT Press. The editor-in-chief is Terrence J. Sejnowski (Salk Institute for Biological Studies).[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Linear_regression\n",
      "In statistics, linear regression is a linear approach for modelling a predictive relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables), which are measured without error. The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression.[1] This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.[2]If the explanatory variables are measured with error then errors-in-variables models are required, also known as measurement error models.\n",
      "\n",
      "https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations\n",
      "The International Conference on Learning Representations (ICLR) is a machine learning conference typically held in late April or early May each year. The conference includes invited talks as well as oral and poster presentations of refereed papers. Since its inception in 2013, ICLR has employed an open peer review process to referee paper submissions (based on models proposed by Yann LeCun[1]). In 2019, there were 1591 paper submissions, of which 500 accepted with poster presentations (31%) and 24 with oral presentations (1.5%).[2]. In 2021, there were 2997 paper submissions, of which 860 were accepted (29%).[3].\n",
      "\n",
      "https://en.wikipedia.org/wiki/Social_computing\n",
      "Social computing is an area of computer science that is concerned with the intersection of social behavior and computational systems. It is based on creating or recreating social conventions and social contexts through the use of software and technology. Thus, blogs, email, instant messaging, social network services, wikis, social bookmarking and other instances of what is often called social software illustrate ideas from social computing.   \n",
      "\n",
      "https://en.wikipedia.org/wiki/Automatic_differentiation\n",
      "In mathematics and computer algebra, automatic differentiation (auto-differentiation, autodiff, or AD), also called algorithmic differentiation, computational differentiation,[1][2] is a set of techniques to evaluate the partial derivative of a function specified by a computer program.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Prompt_engineering#In-context_learning\n",
      "Prompt engineering is the process of structuring text that can be interpreted and understood by a generative AI model.[1][2] A prompt is natural language text describing the task that an AI should perform.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Inductive_logic_programming\n",
      "Inductive logic programming (ILP) is a subfield of symbolic artificial intelligence  which uses logic programming as a uniform representation for examples, background knowledge and hypotheses.  The term \"inductive\" here refers to philosophical (i.e. suggesting a theory to explain observed facts) rather than mathematical (i.e. proving a property for all members of a well-ordered set) induction. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesised logic program which entails all the positive and none of the negative examples.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Manifold_hypothesis\n",
      "The manifold hypothesis posits that many high-dimensional data sets that occur in the real world actually lie along low-dimensional latent manifolds inside that high-dimensional space.[1][2][3][4] As a consequence of the manifold hypothesis, many data sets that appear to initially require many variables to describe, can actually be described by a comparatively small number of variables, likened to the local coordinate system of the underlying manifold. It is suggested that this principle underpins the effectiveness of machine learning algorithms in describing high-dimensional data sets by considering a few common features.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Semantics_(computer_science)\n",
      "In programming language theory, semantics is the rigorous mathematical study of the meaning of programming languages.[1] Semantics assigns computational meaning to valid strings in a programming language syntax. It is closely related to, and often crosses over with, the semantics of mathematical proofs.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Manifold_learning\n",
      "Nonlinear dimensionality reduction, also known as manifold learning, refers to various related techniques that aim to project high-dimensional data onto lower-dimensional latent manifolds, with the goal of either visualizing the data in the low-dimensional space, or learning the mapping (either from the high-dimensional space to the low-dimensional embedding or vice versa) itself.[1][2] The techniques described below can be understood as generalizations of linear decomposition methods used for dimensionality reduction, such as singular value decomposition and principal component analysis.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Parallel_computing\n",
      "Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously.[1] Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing, but has gained broader interest due to the physical constraints preventing frequency scaling.[2] As power consumption (and consequently heat generation) by computers has become a concern in recent years,[3] parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Infer.NET\n",
      "Infer.NET is a free and open source .NET software library for machine learning.[2] It supports running Bayesian inference in graphical models and can also be used for probabilistic programming.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Toronto_Declaration\n",
      "The Toronto Declaration: Protecting the Rights to Equality and Non-Discrimination in Machine Learning Systems is a declaration that advocates responsible practices for machine learning practitioners and governing bodies. It is a joint statement issued by groups including Amnesty International and Access Now, with other notable signatories including Human Rights Watch and The Wikimedia Foundation.[1] It was published at RightsCon on May 16, 2018.[2][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Logic_programming\n",
      "Logic programming is a programming, database and knowledge-representation and reasoning paradigm which is based on formal logic. A program, database or knowledge base in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain.  Major logic programming language families include Prolog, Answer Set Programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Q-learning\n",
      "Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. It does not require a model of the environment (hence \"model-free\"), and it can handle problems with stochastic transitions and rewards without requiring adaptations [1].\n",
      "\n",
      "https://en.wikipedia.org/wiki/Self-play_(reinforcement_learning_technique)\n",
      "Self-play is a technique for improving the performance of reinforcement learning agents. Intuitively, agents learn to improve their performance by playing \"against themselves\".\n",
      "\n",
      "https://en.wikipedia.org/wiki/Binary_classifier\n",
      "Binary classification is the task of classifying the elements of a set into two groups (each called class) on the basis of a classification rule. Typical binary classification problems include:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Logic_in_computer_science\n",
      "Logic in computer science covers the overlap between the field of logic and that of computer science. The topic can essentially be divided into three main areas:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Random_forest\n",
      "\n",
      "Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned.[1][2] Random decision forests correct for decision trees' habit of overfitting to their training set.[3]: 587–588 \n",
      "\n",
      "https://en.wikipedia.org/wiki/PyTorch\n",
      "PyTorch is a machine learning framework based on the Torch library,[4][5][6] used for applications such as computer vision and natural language processing,[7] originally developed by Meta AI and now part of the Linux Foundation umbrella.[8][9][10][11] It is free and open-source software released under the modified BSD license. Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface.[12]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Swarm_intelligence\n",
      "Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_economics\n",
      "Computational economics is an interdisciplinary research discipline that involves computer science, economics, and management science.[1]  This subject encompasses computational modeling of economic systems. Some of these areas are unique, while others established areas of economics by allowing robust data analytics and solutions of problems that would be arduous to research without computers and associated numerical methods.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/AlexNet\n",
      "AlexNet is the name of a convolutional neural network (CNN) architecture, designed by Alex Krizhevsky in collaboration with Ilya Sutskever and Geoffrey Hinton, who was Krizhevsky's Ph.D. advisor at the University of Toronto.[1][2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Domain-specific_language\n",
      "A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains. There are a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software, such as MUSH soft code. DSLs can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. Special-purpose computer languages have always existed in the computer age, but the term \"domain-specific language\" has become more popular due to the rise of domain-specific modeling. Simpler DSLs, particularly ones used by a single application, are sometimes informally called mini-languages.\n",
      "\n",
      "https://en.wikipedia.org/wiki/DeepDream\n",
      "DeepDream is a computer vision program created by Google engineer Alexander Mordvintsev that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dream-like appearance reminiscent of a psychedelic experience in the deliberately overprocessed images.[1][2][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Alex_Graves_(computer_scientist)\n",
      "Alex Graves is a computer scientist. Before working as a research scientist at DeepMind, he earned a BSc in Theoretical Physics from the University of Edinburgh and a PhD in artificial intelligence under Jürgen Schmidhuber at IDSIA.[1] He was also a postdoc under Schmidhuber at the Technical University of Munich and under Geoffrey Hinton[2] at the University of Toronto.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Automated_theorem_proving\n",
      "Automated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs. Automated reasoning over mathematical proof was a major impetus for the development of computer science.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Corinna_Cortes\n",
      "Corinna Cortes (born 31 March, 1961) is a Danish computer scientist known for her contributions to machine learning. She is a Vice President at Google Research in New York City.[3] Cortes is an ACM Fellow and a recipient of the Paris Kanellakis Award for her work on theoretical foundations of support vector machines.[4][5][3][6]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Formal_methods\n",
      "In computer science, formal methods are mathematically rigorous techniques for the specification, development, analysis, and verification of software and hardware systems.[1] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/GPT-1\n",
      "Generative Pre-trained Transformer 1 (GPT-1) was the first of OpenAI's large language models following Google's invention of the transformer architecture in 2017.[2] In June 2018, OpenAI released a paper entitled \"Improving Language Understanding by Generative Pre-Training\",[3] in which they introduced that initial model along with the general concept of a generative pre-trained transformer.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Heuristic_(computer_science)\n",
      "In mathematical optimization and computer science, heuristic (from Greek εὑρίσκω \"I find, discover\") is a technique designed for problem solving more quickly when classic methods are too slow for finding an exact or approximate solution, or when classic methods fail to find any exact solution in a search space.  This is achieved by trading optimality, completeness, accuracy, or precision for speed.  In a way, it can be considered a shortcut.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\n",
      "In statistics and machine learning, the bias–variance tradeoff describes the relationship between a model's complexity, the accuracy of its predictions, and how well it can make predictions on previously unseen data that were not used to train the model. In general, as we increase the number of tunable parameters in a model, it becomes more flexible, and can better fit a training data set. It is said to have lower error, or bias. However, for more flexible models, there will tend to be greater variance to the model fit each time we take a set of samples to create a new training data set. It is said that there is greater variance in the model's estimated parameters.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Robot_locomotion\n",
      "Robot locomotion is the collective name for the various methods that robots use to transport themselves from place to place.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Cheminformatics\n",
      "Cheminformatics (also known as chemoinformatics) refers to the use of physical chemistry theory with computer and information science techniques—so called \"in silico\" techniques—in application to a range of descriptive and prescriptive problems in the field of chemistry, including in its applications to biology and related molecular fields. Such in silico techniques are used, for example, by pharmaceutical companies and in academic settings to aid and inform the process of drug discovery, for instance in the design of well-defined combinatorial libraries of synthetic compounds, or to assist in structure-based drug design. The methods can also be used in chemical and allied industries, and such fields as environmental science and pharmacology, where chemical processes are involved or studied.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Graphical_model\n",
      "A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statistics—particularly Bayesian statistics—and machine learning.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Statistical_inference\n",
      "Statistical inference is the process of using data analysis to infer properties of an underlying distribution of probability.[1] Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computer_animation\n",
      "Computer animation is the process used for digitally generating animations. The more general term computer-generated imagery (CGI) encompasses both static scenes (still images) and dynamic images (moving images), while computer animation only refers to moving images. Modern computer animation usually uses 3D computer graphics. The animation's target is sometimes the computer itself, while other times it is film.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Dartmouth_workshop\n",
      "The Dartmouth Summer Research Project on Artificial Intelligence was a 1956 summer workshop widely considered[1][2][3] to be the founding event of artificial intelligence as a field.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction\n",
      "Human–computer interaction (HCI) is research in the design and the use of computer technology, which focuses on the interfaces between people (users) and computers. HCI researchers observe the ways humans interact with computers and design technologies that allow humans to interact with computers in novel ways. A device that allows interaction between human being and a computer is known as a \"Human-computer Interface (HCI)\".\n",
      "\n",
      "https://en.wikipedia.org/wiki/IEEE_Transactions_on_Pattern_Analysis_and_Machine_Intelligence\n",
      "IEEE Transactions on Pattern Analysis and Machine Intelligence (sometimes abbreviated as IEEE PAMI or simply PAMI) is a monthly peer-reviewed scientific journal published by the IEEE Computer Society. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Artificial_neuron\n",
      "An artificial neuron is a mathematical function conceived as a model of biological neurons in a neural network. Artificial neurons are the elementary units of artificial neural networks.[1] The artificial neuron receives one or more inputs (representing excitatory postsynaptic potentials and inhibitory postsynaptic potentials at neural dendrites) and sums them to produce an output (or activation, representing a neuron's action potential which is transmitted along its axon). Usually, each input is separately weighted (representing the synaptic weight), and the sum is often added to a term known as a bias (loosely corresponding to the threshold potential), before being passed through a non-linear function known as an activation function or transfer function[clarification needed]. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often monotonically increasing, continuous, differentiable and bounded. Non-monotonic, unbounded and oscillating activation functions with multiple zeros that outperform sigmoidal and ReLU-like activation functions on many tasks have also been recently explored. The thresholding function has inspired building logic gates referred to as threshold logic; applicable to building logic circuits resembling brain processing. For example, new devices such as memristors have been extensively used to develop such logic in recent times.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Statistical_classification\n",
      "In statistics, classification is the problem of identifying which of a set of categories (sub-populations) an observation (or observations) belongs to.  Examples are assigning a given email to the \"spam\" or \"non-spam\" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).\n",
      "\n",
      "https://en.wikipedia.org/wiki/BERT_(language_model)\n",
      "\n",
      "Bidirectional Encoder Representations from Transformers (BERT) is a family of language models introduced in October 2018 by researchers at Google.[1][2] A 2020 literature survey concluded that \"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\"[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Apache_SystemML\n",
      "Apache SystemDS (Previously, Apache SystemML) is an open source ML system for the end-to-end data science lifecycle. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory\n",
      "Vapnik–Chervonenkis theory (also known as VC theory) was developed during 1960–1990 by Vladimir Vapnik and Alexey Chervonenkis. The theory is a form of computational learning theory, which attempts to explain the learning process from a statistical point of view.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Programming_paradigm\n",
      "Programming paradigms are a way to classify programming languages based on their features. Languages can be classified into multiple paradigms.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_physics\n",
      "Computational physics is the study and implementation of numerical analysis to solve problems in physics.[1] Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science. It is sometimes regarded as a subdiscipline (or offshoot) of theoretical physics, but others consider it an intermediate branch between theoretical and experimental physics — an area of study which supplements both theory and experiment.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/ADALINE\n",
      "ADALINE (Adaptive Linear Neuron or later Adaptive Linear Element) is an early single-layer artificial neural network and the name of the physical device that implemented this network.[1][2][3][4][5] The network uses memistors. It was developed by professor Bernard Widrow and his doctoral student Ted Hoff at Stanford University in 1960. It is based on the perceptron. It consists of a weight, a bias and a summation function.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems\n",
      "The Conference and Workshop on Neural Information Processing Systems (abbreviated as NeurIPS and formerly NIPS)  is a machine learning and computational neuroscience conference held every December. The conference is currently a double-track meeting (single-track until 2015) that includes invited talks as well as oral and poster presentations of refereed papers, followed by parallel-track workshops that up to 2013 were held at ski resorts.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Dynamic_programming\n",
      "Dynamic programming is both a mathematical optimization method and an algorithmic paradigm. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Interpreter_(computing)\n",
      "In computer science, an interpreter is a computer program that directly executes instructions written in a programming or scripting language, without requiring them previously to have been compiled into a machine language program. An interpreter generally uses one of the following strategies for program execution:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Empirical_risk_minimization\n",
      "Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an algorithm will work in practice (the true \"risk\") because we don't know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the \"empirical\" risk).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Analysis_of_algorithms\n",
      "In computer science, the analysis of algorithms is the process of finding the computational complexity of algorithms—the amount of time, storage, or other resources needed to execute them. Usually, this involves determining a function that relates the size of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity). An algorithm is said to be efficient when this function's values are small, or grow slowly compared to a growth in the size of the input. Different inputs of the same size may cause the algorithm to have different behavior, so best, worst and average case descriptions might all be of practical interest.  When not otherwise specified, the function describing the performance of an algorithm is usually an upper bound, determined from the worst case inputs to the algorithm.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Overfitting\n",
      "In mathematical modeling, overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably\".[1] An overfitted model is a mathematical model that contains more parameters than can be justified by the data.[2] In a mathematical sense, these parameters represent the degree of a polynomial. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.[3]: 45 \n",
      "\n",
      "https://en.wikipedia.org/wiki/Self-organizing_map\n",
      "A self-organizing map (SOM) or self-organizing feature map (SOFM) is an unsupervised machine learning technique used to produce a low-dimensional (typically two-dimensional) representation of a higher dimensional data set while preserving the topological structure of the data. For example, a data set with \n",
      "\n",
      "\n",
      "\n",
      "p\n",
      "\n",
      "\n",
      "{\\displaystyle p}\n",
      "\n",
      " variables measured in \n",
      "\n",
      "\n",
      "\n",
      "n\n",
      "\n",
      "\n",
      "{\\displaystyle n}\n",
      "\n",
      " observations could be represented as clusters of observations with similar values for the variables. These clusters then could be visualized as a two-dimensional \"map\" such that observations in proximal clusters have more similar values than observations in distal clusters. This can make high-dimensional data easier to visualize and analyze.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Sequence_mining\n",
      "Sequential pattern mining is a topic of data mining concerned with finding statistically relevant patterns between data examples where the values are delivered in a sequence.[1][2] It is usually presumed that the values are discrete, and thus time series mining is closely related, but usually considered a different activity.  Sequential pattern mining is a special case of structured data mining.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Rendering_(computer_graphics)\n",
      "Rendering or image synthesis is the process of generating a photorealistic or non-photorealistic image from a 2D or 3D model by means of a computer program.[citation needed]  The resulting image is referred to as the render.  Multiple models can be defined in a scene file containing objects in a strictly defined language or data structure.  The scene file contains geometry, viewpoint, texture, lighting, and shading information describing the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file. The term \"rendering\" is analogous to the concept of an artist's impression of a scene.  The term \"rendering\" is also used to describe the process of calculating effects in a video editing program to produce the final video output.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Shogun_(toolbox)\n",
      "Shogun is a free, open-source machine learning software library  written in C++. It offers numerous algorithms and data structures for machine learning problems. It offers interfaces for Octave, Python, R, Java, Lua, Ruby and C# using SWIG.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Probabilistic_reasoning\n",
      "Probabilistic logic (also probability logic and probabilistic reasoning) involves the use of probability and logic to deal with uncertain situations. Probabilistic logic extends traditional logic truth tables with probabilistic expressions. A difficulty of probabilistic logics is their tendency to multiply the computational complexities of their probabilistic and logical components.  Other difficulties include the possibility of counter-intuitive results, such as in case of belief fusion in Dempster–Shafer theory. Source trust and epistemic uncertainty about the probabilities they provide, such as defined in subjective logic, are additional elements to consider. The need to deal with a broad variety of contexts and issues has led to many different proposals.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Template_talk:Artificial_intelligence\n",
      "The current pic, which I have added to the template, is at top, and the previous one is at bottom. I do not think the old one was very good; it is an illustration of the contours of a human brain with a random circuit board overlaid on it. What circuit board? We don't know. It looks like there is supposed to be a pad for a CPU in the middle... and there is part of a ball grid array or something there... but there is also a gigantic randomly-shaped splotch of copper there, what is that for? I am confident that this is not an actual PCB, nor is it a plausible design for one, and I object to illustrating articles about artificial intelligence with a ridiculously fake image.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Kernel_regression\n",
      "In statistics, kernel regression is a non-parametric technique to estimate the conditional expectation of a random variable. The objective is to find a non-linear relation between a pair of random variables X and Y.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Chinchilla_AI\n",
      "Chinchilla is a family of large language models developed by the research team at DeepMind, presented in March of 2022.[1] It is named \"chinchilla\" because it is a further development over a previous model family named Gopher. Both model families were trained in order to investigate the scaling laws of large language models.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Sparse_coding\n",
      "Neural coding (or neural representation) is a neuroscience field concerned with characterising the hypothetical relationship between the stimulus and the individual or ensemble neuronal responses and the relationship among the electrical activity of the neurons in the ensemble.[1][2] Based on the theory that\n",
      "sensory and other information is represented in the brain by networks of neurons, it is thought that neurons can encode both digital and analog information.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/International_Joint_Conference_on_Artificial_Intelligence\n",
      "The International Joint Conference on Artificial Intelligence (IJCAI) is the leading conference in the field of artificial intelligence. The conference series has been organized by the nonprofit IJCAI Organization since 1969, making it the oldest premier AI conference series in the world.[1] It was held biennially in odd-numbered years from 1969 to 2015 and annually starting from 2016. More recently, IJCAI was held jointly every four years with ECAI since 2018 and PRICAI since 2020 to promote collaboration of AI researchers and practitioners. IJCAI covers a broad range of research areas in the field of AI. It is a large and highly selective conference, with only about 20% or less of the submitted papers accepted after peer review in the 5 years leading up to 2022.[2] A lower acceptance rate usually means better quality papers and a higher reputation conference.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Programming_team\n",
      "A programming team is a team of people who develop or maintain computer software.[1]  They may be organised in numerous ways, but the egoless programming team and chief programmer team have been common structures.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Statistics\n",
      "Statistics (from German: Statistik, orig. \"description of a state, a country\")[1][2] is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data.[3][4][5] In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.[6]\n",
      "\n",
      "https://en.wikipedia.org/wiki/PaLM\n",
      "PaLM (Pathways Language Model) is a 540 billion parameter transformer-based large language model developed by Google AI.[1] Researchers also trained smaller versions of PaLM, 8 and 62 billion parameter models, to test the effects of model scale.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/AlphaFold\n",
      "AlphaFold is an artificial intelligence (AI) program developed by DeepMind, a subsidiary of Alphabet, which performs predictions of protein structure.[1] The program is designed as a deep learning system.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Temporal_difference_learning\n",
      "Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Markov_decision_process\n",
      "In mathematics, a Markov decision process (MDP) is a discrete-time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming. MDPs were known at least as early as the 1950s;[1] a core body of research on Markov decision processes resulted from Ronald Howard's 1960 book, Dynamic Programming and Markov Processes.[2] They are used in many disciplines, including robotics, automatic control, economics and manufacturing. The name of MDPs comes from the Russian mathematician Andrey Markov as they are an extension of Markov chains.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Spiking_neural_network\n",
      "Spiking neural networks (SNNs) are artificial neural networks that more closely mimic natural neural networks.[1] In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather transmit information only when a membrane potential—an intrinsic quality of the neuron related to its membrane electrical charge—reaches a specific value, called the threshold. When the membrane potential reaches the threshold, the neuron fires, and generates a signal that travels to other neurons which, in turn, increase or decrease their potentials in response to this signal. A neuron model  that fires at the moment of threshold crossing is also called a spiking neuron model.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/MOA_(Massive_Online_Analysis)\n",
      "Massive Online Analysis (MOA) is a free open-source software project specific for data stream mining with concept drift. It is written in Java and developed at the University of Waikato, New Zealand.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Behaviorism\n",
      "\n",
      "Behaviorism (also spelled behaviourism)[1] is a systematic approach to understanding the behavior of humans and other animals.[2] It assumes that behavior is either a reflex evoked by the pairing of certain antecedent stimuli in the environment, or a consequence of that individual's history, including especially reinforcement and punishment contingencies, together with the individual's current motivational state and controlling stimuli. Although behaviorists generally accept the important role of heredity in determining behavior, they focus primarily on environmental events.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Hugging_Face\n",
      "Hugging Face, Inc. is a French-American company that develops tools for building applications using machine learning, based in New York City. It is most notable for its transformers library built for natural language processing applications and its platform that allows users to share machine learning models and datasets and showcase their work.\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Artificial_intelligence_prompt_completion_by_dalle_mini.jpg\n",
      "Original file ‎(1,024 × 1,024 pixels, file size: 211 KB, MIME type: image/jpeg)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Action_selection\n",
      "Action selection is a way of characterizing the most basic problem of intelligent systems: what to do next. In artificial intelligence and computational cognitive science, \"the action selection problem\" is typically associated with intelligent agents and animats—artificial systems that exhibit complex behaviour in an agent environment. The term is also sometimes used in ethology or animal behavior.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Gradient_descent\n",
      "Gradient descent (also often called steepest descent) is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for finding a local minimum of a differentiable multivariate function\n",
      "\n",
      "https://en.wikipedia.org/wiki/Map_(mathematics)\n",
      "In mathematics, a map or mapping is a function in its general sense.[1]  These terms may have originated as from the process of making a geographical map: mapping the Earth surface to a sheet of paper.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Keras\n",
      "Keras is an open-source library that provides a Python interface for artificial neural networks. Keras acts as an interface for the TensorFlow library.[citation needed]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Nils_Nilsson_(researcher)\n",
      "Nils John Nilsson (February 6, 1933 – April 23, 2019) was an American computer scientist. He was one of the founding researchers in the discipline of artificial intelligence.[2] He was the first Kumagai Professor of Engineering in computer science at Stanford University from 1991 until his retirement. He is particularly known for his contributions to search, planning, knowledge representation, and robotics.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Software_engineering\n",
      "Software engineering is an engineering-based approach to software development.[1][2][3]\n",
      "A software engineer is a person who applies the engineering design process to design, develop, test, maintain, and evaluate computer software. The term programmer is sometimes used as a synonym, but may emphasize software implementation over design and can also lack connotations of engineering education or skills.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
      "Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Conditional_random_field\n",
      "Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighbouring\" samples, a CRF can take context into account. To do so, the predictions are modelled as a graphical model, which represents the presence of dependencies between the predictions. What kind of graph is used depends on the application. For example, in natural language processing, \"linear chain\" CRFs are popular, for which each prediction is dependent only on its immediate neighbours. In image processing, the graph typically connects locations to nearby and/or similar locations to enforce that they receive similar predictions.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Tensor_Processing_Unit\n",
      "Tensor Processing Unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google for neural network machine learning, using Google's own TensorFlow software.[1] Google began using TPUs internally in 2015, and in 2018 made them available for third party use, both as part of its cloud infrastructure and by offering a smaller version of the chip for sale.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
      "Cross-validation,[2][3][4] sometimes called rotation estimation[5][6][7] or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.\n",
      "Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.  In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set).[8][9] The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias[10] and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Programming_tool\n",
      "A programming tool or software development tool is a computer program that software developers use to create, debug, maintain, or otherwise support other programs and applications. The term usually refers to relatively simple programs, that can be combined to accomplish a task, much as one might use multiple hands to fix a physical object. The most basic tools are a source code editor and a compiler or interpreter, which are used ubiquitously and continuously. Other tools are used more or less depending on the language, development methodology, and individual engineer, often used for a discrete task, like a debugger or profiler. Tools may be discrete programs, executed separately – often from the command line – or may be parts of a single large program, called an integrated development environment (IDE). In many cases, particularly for simpler use, simple ad hoc techniques are used instead of a tool, such as print debugging instead of using a debugger, manual timing (of overall program or section of code) instead of a profiler, or tracking bugs in a  text file or spreadsheet instead of a bug tracking system.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine\n",
      "A restricted Boltzmann machine (RBM) (also called a restricted Sherrington–Kirkpatrick model with external field or restricted stochastic Ising–Lenz–Little model) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Functional_programming\n",
      "In computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions. It is a declarative programming paradigm in which function definitions are trees of expressions that map values to other values, rather than a sequence of imperative statements which update the running state of the program.\n",
      "\n",
      "https://en.wikipedia.org/wiki/WaveNet\n",
      "WaveNet is a deep neural network for generating raw audio. It was created by researchers at London-based AI firm DeepMind. The technique, outlined in a paper in September 2016,[1] is able to generate relatively realistic-sounding human-like voices by directly modelling waveforms using a neural network method trained with recordings of real speech. Tests with US English and Mandarin reportedly showed that the system outperforms Google's best existing text-to-speech (TTS) systems, although as of 2016 its text-to-speech synthesis still was less convincing than actual human speech.[2] WaveNet's ability to generate raw waveforms means that it can model any kind of audio, including music.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computability_theory\n",
      "\n",
      "Computability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability. In these areas, computability theory overlaps with proof theory and effective descriptive set theory.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Distributed_computing\n",
      "A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another.[1][2] Distributed computing is a field of computer science that studies distributed systems. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Software_design\n",
      "Software design is the process by which an agent creates a specification of a software artifact intended to accomplish goals, using a set of primitive components and subject to constraints.[1] The term is sometimes used broadly to refer to \"all the activity involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying\" the software, or more specifically \"the activity following requirements specification and before programming, as ... [in] a stylized software engineering process.\"[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Rakesh_Agrawal_(computer_scientist)\n",
      "Rakesh Agrawal (हिन्दी - राकेश अग्रवाल) is a computer scientist who until recently was a Technical Fellow at the Microsoft Search Labs.[1] Rakesh is well known for developing fundamental data mining concepts and technologies and pioneering key concepts in data privacy, including Hippocratic Database, Sovereign Information Sharing, and Privacy-Preserving Data Mining. IBM's commercial data mining product, Intelligent Miner, grew out of his work. His research has been incorporated into other IBM products, including DB2 Mining Extender, DB2 OLAP Server and WebSphere Commerce Server, and has influenced several other commercial and academic products, prototypes and applications. His other technical contributions include Polyglot object-oriented type system, Alert active database system, Ode (Object database and environment), Alpha (extension of relational databases with generalized transitive closure), Nest distributed system, transaction management, and database machines.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Multithreading_(computer_architecture)\n",
      "In computer architecture, multithreading is the ability of a central processing unit (CPU) (or a single core in a multi-core processor) to provide multiple threads of execution concurrently, supported by the operating system. This approach differs from multiprocessing. In a multithreaded application, the threads share the resources of a single or multiple cores, which include the computing units, the CPU caches, and the translation lookaside buffer (TLB).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Flux_(machine-learning_framework)\n",
      "Flux is an open-source machine-learning software library and ecosystem written in Julia.[1][6] Its current stable release is v0.14.5[4] . It has a layer-stacking-based interface for simpler models, and has a strong support on interoperability with other Julia packages instead of a monolithic design.[7] For example, GPU support is implemented transparently by CuArrays.jl[8] This is in contrast to some other machine learning frameworks which are implemented in other languages with Julia bindings, such as TensorFlow.jl, and thus are more limited by the functionality present in the underlying implementation, which is often in C or C++.[9] Flux joined NumFOCUS as an affiliated project in December of 2021.[10]\n",
      "\n",
      "https://en.wikipedia.org/wiki/SPSS_Modeler\n",
      "IBM SPSS Modeler is a data mining and text analytics software application from IBM. It is used to build predictive models and conduct other analytic tasks. It has a visual interface which allows users to leverage statistical and data mining algorithms without programming.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Sparse_dictionary_learning\n",
      "Sparse dictionary learning (also known as sparse coding or SDL) is a representation learning method which aims at finding a sparse representation of the input data in the form of a linear combination of basic elements as well as those basic elements themselves. These elements are called atoms and they compose a dictionary. Atoms in the dictionary are not required to be orthogonal, and they may be an over-complete spanning set. This problem setup also allows the dimensionality of the signals being represented to be higher than the one of the signals being observed. The above two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal but also provide an improvement in sparsity and flexibility of the representation.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Autoregressive_model\n",
      "In statistics, econometrics, and signal processing, an autoregressive (AR) model is a representation of a type of random process; as such, it is used to describe certain time-varying processes in nature, economics, behavior, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term); thus the model is in the form of a stochastic difference equation (or recurrence relation) which should not be confused with a differential equation. Together with the moving-average (MA) model, it is a special case and key component of the more general autoregressive–moving-average (ARMA) and autoregressive integrated moving average (ARIMA) models of time series, which have a more complicated stochastic structure; it is also a special case of the vector autoregressive model (VAR), which consists of a system of more than one interlocking stochastic difference equation in more than one evolving random variable.\n",
      "\n",
      "https://en.wikipedia.org/wiki/KNIME\n",
      "KNIME (/naɪm/), the Konstanz Information Miner,[2] is a free and open-source data analytics, reporting and integration platform. KNIME integrates various components for machine learning and data mining through its modular data pipelining \"Building Blocks of Analytics\" concept. A graphical user interface and use of JDBC allows assembly of nodes blending different data sources, including preprocessing (ETL: Extraction, Transformation, Loading), for modeling, data analysis and visualization without, or with only minimal, programming.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Market_basket_analysis\n",
      "Affinity analysis falls under the umbrella term of data mining which uncovers meaningful correlations between different entities according to their co-occurrence in a data set. In almost all systems and processes, the application of affinity analysis can extract significant knowledge about the unexpected trends[citation needed]. In fact, affinity analysis takes advantages of studying attributes that go together which helps uncover the hidden pattens in a big data through generating association rules. Association rules mining procedure is two-fold: first, it finds all frequent attributes in a data set and, then generates association rules satisfying some predefined criteria, support and confidence, to identify the most important relationships in the frequent itemset. The first step in the process is to count the co-occurrence of attributes in the data set. Next, a subset is created called the frequent itemset. The association rules mining takes the form of if a condition or feature (A) is present then another condition or feature (B) exists. The first condition or feature (A) is called antecedent and the latter (B) is known as consequent. This process is repeated until no additional frequent itemsets are found.  There are two important metrics for performing the association rules mining technique: support and confidence. Also, a priori algorithm is used to reduce the search space for the problem.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Graphcore\n",
      "Graphcore Limited is a British semiconductor company that develops accelerators for AI and machine learning. It aims to make a massively parallel Intelligence Processing Unit (IPU) that holds the complete machine learning model inside the processor.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Decision_making\n",
      "In psychology, decision-making (also spelled decision making and decisionmaking) is regarded as the cognitive process resulting in the selection of a belief or a course of action among several possible alternative options. It could be either rational or irrational. The decision-making process is a reasoning process based on assumptions of values, preferences and beliefs of the decision-maker.[1] Every decision-making process produces a final choice, which may or may not prompt action.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Fuzzy_logic\n",
      "\n",
      "Fuzzy logic is a form of many-valued logic in which the truth value of variables may be any real number between 0 and 1. It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false.[1] By contrast, in Boolean logic, the truth values of variables may only be the integer values 0 or 1.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Data_Cleaning\n",
      "Data cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data.[1] Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting or a data quality firewall.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Digital_art\n",
      "Digital art refers to any artistic work or practice that uses digital technology as part of the creative or presentation process. It can also refer to computational art that uses and engages with digital media.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Randomized_algorithm\n",
      "A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic or procedure. The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the \"average case\" over all possible choices of random determined by the random bits; thus either the running time, or the output (or both) are random variables.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Heuristic\n",
      "A heuristic (/hjʊˈrɪstɪk/; from Ancient Greek  εὑρίσκω (heurískō) 'to find, discover'), or heuristic technique, is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision.[1][2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_chemistry\n",
      "Computational chemistry is a branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into computer programs, to calculate the structures and properties of molecules, groups of molecules, and solids. It is essential because, apart from relatively recent results concerning the hydrogen molecular ion (dihydrogen cation, see references therein for more details), the quantum many-body problem cannot be solved analytically, much less in closed form.  While computational results normally complement the information obtained by chemical experiments, it can in some cases predict hitherto unobserved chemical phenomena. It is widely used in the design of new drugs and materials.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computer_architecture\n",
      "In computer science and computer engineering, computer architecture is a description of the structure of a computer system made from component parts.[1] It can sometimes be a high-level description that ignores details of the implementation.[2] At a more detailed level, the description may include the instruction set architecture design, microarchitecture design, logic design, and implementation.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Information_retrieval\n",
      "Information retrieval (IR) in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources.  Searches can be based on full-text or other content-based indexing. Information retrieval is the science[1] of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Caffe_(software)\n",
      "Caffe (Convolutional Architecture for Fast Feature Embedding) is a deep learning framework, originally developed at University of California, Berkeley. It is open source, under a BSD license.[4] It is written in C++, with a Python interface.[5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Biological_neural_network\n",
      "A neural circuit (also known as a biological neural network BNNs) is a population of neurons interconnected by synapses to carry out a specific function when activated.[1] Multiple neural circuits interconnect with one another to form large scale brain networks.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Bayesian_network\n",
      "A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). While it is one of several forms of causal notation, causal networks are special cases of Bayesian networks. Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Linear_discriminant_analysis\n",
      "Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mean_shift\n",
      "Mean shift is a non-parametric feature-space mathematical analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm.[1] Application domains include cluster analysis in computer vision and image processing.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mehryar_Mohri\n",
      "Mehryar Mohri is a Professor and theoretical computer scientist[2] at the Courant Institute of Mathematical Sciences. He is also a Research Director \n",
      "at Google Research where he heads the Learning Theory team.\n",
      "\n",
      "https://en.wikipedia.org/wiki/GPT-2\n",
      "Generative Pre-trained Transformer 2 (GPT-2) is a large language model by OpenAI and the second in their foundational series of GPT models. GPT-2 was pre-trained on BookCorpus,[2] a dataset of over 7,000 self-published fiction books from various genres, and trained on a dataset of 8 million web pages.[3] It was partially released in February 2019, followed by full release of the 1.5-billion-parameter model on November 5, 2019.[4][5][6][7][8]\n",
      "\n",
      "https://en.wikipedia.org/wiki/David_Rumelhart\n",
      "David Everett Rumelhart (June 12, 1942 – March 13, 2011)[1] was an American psychologist who made many contributions to the formal analysis of human cognition, working primarily within the frameworks of mathematical psychology, symbolic artificial intelligence, and parallel distributed processing. He also admired formal linguistic approaches to cognition, and explored the possibility of formulating a formal grammar to capture the structure of stories.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computing_platform\n",
      "A computing platform, digital platform,[1] or software platform is an environment in which software is executed. It may be the hardware or the operating system (OS), a web browser and associated application programming interfaces, or other underlying software, as long as the program code is executed. Computing platforms have different abstraction levels, including a computer architecture, an OS, or runtime libraries.[2] A computing platform is the stage on which computer programs can run.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Concurrency_(computer_science)\n",
      "In computer science, concurrency is the ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or in partial order, without affecting the outcome.  This allows for parallel execution of the concurrent units, which can significantly improve overall speed of the execution in multi-processor and multi-core systems. In more technical terms, concurrency refers to the decomposability of a program, algorithm, or problem into order-independent or partially-ordered components or units of computation.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Activation_function\n",
      "Activation function of a node in an artificial neural network is a function that calculates the output of the node (based on its inputs and the weights on individual inputs). Nontrivial problems can be solved only using a nonlinear activation function.[1] Modern activation functions include the smooth version of the ReLU, the GELU, which was used in the 2018 BERT model,[2] the logistic (sigmoid) function used in the 2012 speech recognition model developed by Hinton et al,[3] the ReLU used in the 2012 AlexNet computer vision model and in the 2015 ResNet model. \n",
      "\n",
      "https://en.wikipedia.org/wiki/RapidMiner\n",
      "RapidMiner is a data science platform that analyses the collective impact of an organization's data. It was acquired by Altair Engineering in September 2022.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Uncertainty_quantification\n",
      "Uncertainty quantification (UQ) is the science of quantitative characterization and estimation of uncertainties in both computational and real world applications. It tries to determine how likely certain outcomes are if some aspects of the system are not exactly known. An example would be to predict the acceleration of a human body in a head-on crash with another car: even if the speed was exactly known, small differences in the manufacturing of individual cars, how tightly every bolt has been tightened, etc., will lead to different results that can only be predicted in a statistical sense.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Algorithmic_transparency\n",
      "Algorithmic transparency is the principle that the factors that influence the decisions made by algorithms should be visible, or transparent, to the people who use, regulate, and are affected by systems that employ those algorithms. Although the phrase was coined in 2016 by Nicholas Diakopoulos and Michael Koliska about the role of algorithms in deciding the content of digital journalism services,[1] the underlying principle dates back to the 1970s and the rise of automated systems for scoring consumer credit.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Cybernetics\n",
      "Cybernetics is a transdisciplinary approach for exploring regulatory systems with feedback, their structures, constraints, and possibilities. Cybernetics is relevant to the study of systems, such as mechanical, physical, biological, cognitive, and social.\n",
      "\n",
      "https://en.wikipedia.org/wiki/ACM_Computing_Classification_System\n",
      "The ACM Computing Classification System (CCS) is a subject classification system for computing devised by the Association for Computing Machinery (ACM). The system is comparable to the Mathematics Subject Classification (MSC) in scope, aims, and structure, being used by the various ACM journals to organize subjects by area.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mixed_reality\n",
      "Mixed reality (MR) is a term used to describe the merging of a real-world environment and a computer-generated one. Physical and virtual objects may co-exist in mixed reality environments and interact in real time.\n",
      "\n",
      "https://en.wikipedia.org/wiki/SpiNNaker\n",
      "SpiNNaker (spiking neural network architecture) is a massively parallel, manycore supercomputer architecture designed by the Advanced Processor Technologies Research Group (APT) at the Department of Computer Science, University of Manchester.[2]  It is composed of 57,600 processing nodes, each with 18 ARM9 processors (specifically ARM968) and 128 MB of mobile DDR SDRAM, totalling 1,036,800 cores and over 7 TB of RAM.[3]  The computing platform is based on spiking neural networks, useful in simulating the human brain (see Human Brain Project).[4][5][6][7][8][9][10][11][12]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Middleware\n",
      "Middleware is a type of computer software programme that provides services to software applications beyond those available from the operating system. It can be described as \"software glue\".[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Logistic_regression\n",
      "In statistics, the logistic model (or logit model) is a statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables. In regression analysis, logistic regression[1] (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination). Formally, in binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled \"0\" and \"1\", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \"1\" can vary between 0 (certainly the value \"0\") and 1 (certainly the value \"1\"), hence the labeling;[2] the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See § Background and § Definition for formal mathematics, and § Example for a worked example.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Physical_neural_network\n",
      "A physical neural network is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse or a higher-order (dendritic) neuron model.[1] \"Physical\" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches. More generally the term is applicable to other artificial neural networks in which a memristor or other electrically adjustable resistance material is used to emulate a neural synapse.[2][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Fuzzy_clustering\n",
      "Fuzzy clustering (also referred to as soft clustering or soft k-means) is a form of clustering in which each data point can belong to more than one cluster.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Orange_(software)\n",
      "Orange is an open-source data visualization, machine learning and data mining toolkit. It features a visual programming front-end for explorative qualitative data analysis and interactive data visualization.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Discrete_mathematics\n",
      "Discrete mathematics is the study of mathematical structures that can be considered \"discrete\" (in a way analogous to discrete variables, having a bijection with the set of natural numbers) rather than \"continuous\" (analogously to continuous functions). Objects studied in discrete mathematics include integers, graphs, and statements in logic.[1][2][3] By contrast, discrete mathematics excludes topics in \"continuous mathematics\" such as real numbers, calculus or Euclidean geometry. Discrete objects can often be enumerated by integers; more formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets[4] (finite sets or sets with the same cardinality as the natural numbers). However, there is no exact definition of the term \"discrete mathematics\".[5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Oracle_Data_Mining\n",
      "Oracle Data Mining (ODM) is an option of Oracle Database Enterprise Edition. It contains several data mining and data analysis algorithms for classification, prediction, regression, associations, feature selection, anomaly detection, feature extraction, and specialized analytics. It provides means for the creation, management and operational deployment of data mining models inside the database environment.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Computer_science\n",
      "Welcome to the WikiProject Computer science page. The goals of the project are to build a community of interest around computer science, and to provide a focal point for coordinating efforts to improve Wikipedia's computer science articles. The scope of the project includes all articles in the area of computer science, including computer programming and software engineering. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Information_theory\n",
      "Information theory is the mathematical study of the quantification, storage, and communication of information.[1] The field was originally established by the works of Harry Nyquist and Ralph Hartley, in the 1920s, and Claude Shannon in the 1940s.[2]: vii  The field, in  applied mathematics, is at the intersection of probability theory, statistics, computer science, statistical mechanics, information engineering, and electrical engineering.\n",
      "\n",
      "https://en.wikipedia.org/wiki/BLOOM_(language_model)\n",
      "BigScience Large Open-science Open-access Multilingual Language Model (BLOOM[1]) is a transformer-based large language model. It was created by AI researchers to provide a free large language model for large-scale public access. Trained on around 366 billion tokens over March through July 2022, it is considered an alternative to OpenAI's GPT-3 with its 176 billion parameters. BLOOM uses a decoder-only transformer model architecture modified from Megatron-LM GPT-2.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Inference\n",
      "Inferences are steps in reasoning, moving from premises to logical consequences; etymologically, the word infer means to \"carry forward\". Inference is theoretically traditionally divided into deduction and induction, a distinction that in Europe dates at least to Aristotle (300s BCE). Deduction is inference deriving logical conclusions from premises known or assumed to be true, with the laws of valid inference being studied in logic. Induction is inference from particular evidence to a universal conclusion. A third type of inference is sometimes distinguished, notably by Charles Sanders Peirce, contradistinguishing abduction from induction.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Stochastic_process\n",
      "In probability theory and related fields, a stochastic (/stəˈkæstɪk/) or random process is a mathematical object usually defined as a sequence of random variables, where the index of the sequence has the interpretation of time. Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner. Examples include the growth of a bacterial population, an electrical current fluctuating due to thermal noise, or the movement of a gas molecule.[1][4][5] Stochastic processes have applications in many disciplines such as biology,[6] chemistry,[7] ecology,[8] neuroscience,[9] physics,[10] image processing, signal processing,[11] control theory,[12] information theory,[13] computer science,[14] and telecommunications.[15] Furthermore, seemingly random changes in financial markets have motivated the extensive use of stochastic processes in finance.[16][17][18]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Independent_component_analysis\n",
      "In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that at most one subcomponent is Gaussian and that the subcomponents are statistically independent from each other.[1] ICA is a special case of blind source separation. A common example application is the \"cocktail party problem\" of listening in on one person's speech in a noisy room.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ray_Solomonoff\n",
      "Ray Solomonoff (July 25, 1926 – December 7, 2009)[1][2] was the inventor of algorithmic probability,[3] his General Theory of Inductive Inference (also known as Universal Inductive Inference),[4] and was a founder of algorithmic information theory.[5] He was an originator of the branch of artificial intelligence based on machine learning, prediction and probability. He circulated the first report on non-semantic machine learning in 1956.[6]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Michael_I._Jordan\n",
      "Michael Irwin Jordan ForMemRS[6] (born February 25, 1956) is an American scientist, professor at the University of California, Berkeley and researcher in machine learning, statistics, and artificial intelligence.[7][8][9]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm\n",
      "In statistics, an expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables.[1] The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.\n",
      "\n",
      "https://en.wikipedia.org/wiki/SAS_(software)#Components\n",
      "SAS (previously \"Statistical Analysis System\")[1] is a statistical software suite developed by SAS Institute for  data management, advanced analytics, multivariate analysis, business intelligence, criminal investigation,[2] and predictive analytics.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Computer_science\n",
      "This category has the following 25 subcategories, out of 25 total.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Image_compression\n",
      "Image compression is a type of data compression applied to digital images, to reduce their cost for storage or transmission. Algorithms may take advantage of visual perception and the statistical properties of image data to provide superior results compared with generic data compression methods which are used for other digital data.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Vision_processing_unit\n",
      "A vision processing unit (VPU) is (as of 2023) an emerging class of microprocessor; it is a specific type of AI accelerator, designed to accelerate machine vision tasks.[1][2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/OpenAI_Five\n",
      "\n",
      "OpenAI Five is a computer program by OpenAI that plays the five-on-five video game Dota 2. Its first public appearance occurred in 2017, where it was demonstrated in a live one-on-one game against the professional player Dendi, who lost to it. The following year, the system had advanced to the point of performing as a full team of five, and began playing against and showing the capability to defeat professional teams.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Residual_neural_network\n",
      "A Residual Neural Network (a.k.a. Residual Network, ResNet)[1] is a deep learning model in which the weight layers learn residual functions with reference to the layer inputs. A Residual Network[1] is a network with skip connections that perform identity mappings, merged with the layer outputs by addition. It behaves like a Highway Network[2] whose gates are opened through strongly positive bias weights. This enables deep learning models with tens or hundreds of layers to train easily and approach better accuracy when going deeper. The identity skip connections, often referred to as \"residual connections\", are also used in the 1997 LSTM networks,[3] Transformer models (e.g., BERT, GPT models such as ChatGPT), the AlphaGo Zero system, the AlphaStar system, and the AlphaFold system.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Anomaly_detection\n",
      "In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behaviour.[1] Such examples may arouse suspicions of being generated by a different mechanism,[2] or appear inconsistent with the remainder of that set of data.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/XGBoost\n",
      "XGBoost[2] (eXtreme Gradient Boosting) is an open-source software library which provides a regularizing gradient boosting framework for C++, Java, Python,[3] R,[4] Julia,[5] Perl,[6] and Scala. It works on Linux, Microsoft Windows,[7] and macOS.[8] From the project description, it aims to provide a \"Scalable, Portable and Distributed Gradient Boosting (GBM, GBRT, GBDT) Library\". It runs on a single machine, as well as the distributed processing frameworks Apache Hadoop, Apache Spark, Apache Flink, and Dask.[9][10]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Information_geometry\n",
      "Information geometry is an interdisciplinary field that applies the techniques of differential geometry to study probability theory and statistics. [1]  It studies statistical manifolds, which are Riemannian manifolds whose points correspond to probability distributions.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Alan_Mackworth\n",
      "Alan Mackworth is a professor emeritus in the Department of Computer Science at the University of British Columbia. He is known as \"The Founding Father\" of RoboCup. He is a former president of the Association for the Advancement of Artificial Intelligence (AAAI) and former Canada Research Chair in Artificial Intelligence from 2001–2014.\n",
      "\n",
      "https://en.wikipedia.org/wiki/IBM_Watson_Studio\n",
      "Watson Studio, formerly Data Science Experience or DSX, is IBM’s software platform for data science. The platform consists of a workspace that includes multiple collaboration and open-source tools for use in data science.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computer_accessibility\n",
      "Computer accessibility (also known as accessible computing) refers to the accessibility of a computer system to all people, regardless of disability type or severity of impairment. The term accessibility is most often used in reference to specialized hardware or software, or a combination of both, designed to enable the use of a computer by a person with a disability or impairment. Computer accessibility often has direct positive effects on people with disabilities.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Automated_medical_diagnosis\n",
      "Computer-aided detection (CADe), also called computer-aided diagnosis (CADx), are systems that assist doctors in the interpretation of medical images. Imaging techniques in X-ray, MRI, Endoscopy, and ultrasound diagnostics yield a great deal of information that the radiologist or other medical professional has to analyze and evaluate comprehensively in a short time. CAD systems process digital images or videos for typical appearances and to highlight conspicuous sections, such as possible diseases, in order to offer input to support a decision taken by the professional.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Apache_Spark#MLlib_Machine_Learning_Library\n",
      "Apache Spark  is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Simulation-based_optimization\n",
      "Simulation-based optimization (also known as simply simulation optimization) integrates optimization techniques into simulation modeling and analysis. Because of the complexity of the simulation, the objective function may become difficult and expensive to evaluate. Usually, the underlying simulation model is stochastic, so that the objective function must be estimated using statistical estimation techniques (called output analysis in simulation methodology).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Hardware_acceleration\n",
      "Hardware acceleration is the use of computer hardware designed to perform specific functions more efficiently when compared to software running on a general-purpose central processing unit (CPU). Any transformation of data that can be calculated in software running on a generic CPU can also be calculated in custom-made hardware, or in some mix of both.\n",
      "\n",
      "https://en.wikipedia.org/wiki/LIONsolver\n",
      "LIONsolver is an integrated software for data mining, business intelligence, analytics, and modeling and reactive business intelligence approach.[1] A non-profit version is also available as LIONoso.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Concurrent_computing\n",
      "Concurrent computing is a form of computing in which several computations are executed concurrently—during overlapping time periods—instead of sequentially—with one completing before the next starts.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Non-negative_matrix_factorization\n",
      "Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Project_Debater\n",
      "Project Debater is an IBM artificial intelligence project, designed to participate in a full live debate with expert human debaters.[1][2][3][4] It follows on from the Watson project which played Jeopardy![5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Gaussian_processes\n",
      "In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.\n",
      "\n",
      "https://en.wikipedia.org/wiki/K-SVD\n",
      "In applied mathematics, k-SVD is a dictionary learning algorithm for creating a dictionary for sparse representations, via a singular value decomposition approach. k-SVD is a generalization of the k-means clustering method, and it works by iteratively alternating between sparse coding the input data based on the current dictionary, and updating the atoms in the dictionary to better fit the data. It is structurally related to the expectation maximization (EM) algorithm.[1][2] k-SVD can be found widely in use in applications such as image processing, audio processing, biology, and document analysis.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Weight_(mathematics)\n",
      "The process of weighting involves emphasizing the contribution of particular aspects of a phenomenon (or of a set of data) over others to an outcome or result; thereby highlighting those aspects in comparison to others in the analysis. That is, rather than each variable in the data set contributing equally to the final result, some of the data is adjusted to make a greater contribution than others. This is analogous to the practice of adding (extra) weight to one side of a pair of scales in order to favour either the buyer or seller.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Word_processor\n",
      "A word processor (WP)[1][2] is a device or computer program that provides for input, editing, formatting, and output of text, often with some additional features.\n",
      "\n",
      "https://en.wikipedia.org/wiki/STATISTICA\n",
      "Statistica is an advanced analytics software package originally developed by StatSoft and currently maintained by TIBCO Software Inc.[1]\n",
      "Statistica provides data analysis, data management, statistics, data mining, machine learning, text analytics and data visualization procedures.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Software_suite\n",
      "A software suite[1] (also known as an application suite) is a collection of computer programs (application software, or programming software) of related functionality, sharing a similar user interface and the ability to easily exchange data with each other.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Exploratory_data_analysis\n",
      "In statistics, exploratory data analysis (EDA) is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling and thereby contrasts traditional hypothesis testing. Exploratory data analysis has been promoted by John Tukey since 1970 to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA),[1][2] which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Principal_component_analysis\n",
      "Principal component analysis (PCA) is a popular technique for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data. Formally, PCA is a statistical technique for reducing the dimensionality of a dataset. This is accomplished by linearly transforming the data into a new coordinate system where (most of) the variation in the data can be described with fewer dimensions than the initial data. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points. Principal component analysis has applications in many fields such as population genetics, microbiome studies, and atmospheric science.[1]\n",
      "https://en.wikipedia.org/wiki/Gated_recurrent_unit\n",
      "Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al.[1] The GRU is like a long short-term memory (LSTM) with a gating mechanism to input or forget certain features,[2] but lacks a context vector or output gate, resulting in fewer parameters than LSTM.[3] \n",
      "GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM.[4][5] GRUs showed that gating is indeed helpful in general, and Bengio's team came to no concrete conclusion on which of the two gating units was better.[6][7]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n",
      "Bootstrapping is any test or metric that uses random sampling with replacement (e.g. mimicking the sampling process), and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates.[1][2] This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.[3][4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Netflix_Prize\n",
      "The Netflix Prize was an open competition for the best collaborative filtering algorithm to predict user ratings for films, based on previous ratings without any other information about the users or films, i.e. without the users being identified except by numbers assigned for the contest.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Hierarchical_clustering\n",
      "In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis that seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two categories:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ubiquitous_computing\n",
      " Ubiquitous computing (or \"ubicomp\") is a concept in software engineering, hardware engineering and computer science where computing is made to appear anytime and everywhere. In contrast to desktop computing, ubiquitous computing can occur using any device, in any location, and in any format. A user interacts with the computer, which can exist in many different forms, including laptop computers, tablets, smart phones and terminals in everyday objects such as a refrigerator or a pair of glasses. The underlying technologies to support ubiquitous computing include Internet, advanced middleware, operating system, mobile code, sensors, microprocessors, new I/O and user interfaces, computer networks, mobile protocols, location and positioning, and new materials.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Probability_theory\n",
      "Probability theory or probability calculus is the branch of mathematics concerned with probability. Although there are several different probability interpretations, probability theory treats the concept in a rigorous mathematical manner by expressing it through a set of axioms. Typically these axioms formalise probability in terms of a probability space, which assigns a measure taking values between 0 and 1, termed the probability measure, to a set of outcomes called the sample space. Any specified subset of the sample space is called an event.\n",
      "\n",
      "https://en.wikipedia.org/wiki/K-means_clustering\n",
      "k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Naomi_Altman\n",
      "Naomi Altman is a statistician known for her work on kernel smoothing[KS] and kernel regression,[KR]\n",
      "and interested in applications of statistics to gene expression and genomics. She is a professor of statistics at Pennsylvania State University,[1] and a regular columnist for the \"Points of Significance\" column in Nature Methods.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Template_talk:Differentiable_computing\n",
      "I don't want to remove the group because I didn't contribute anything to the template and removing stuff added by other users seems rude to me. But now using JS or GO for programming gradient descent based algorithm is not that uncommon, does it implies that wikipedia should also add JS/GO to the group. To put it simply, I don't think having a group for popular programming languages used for Differentiable computing helps anyone but rather might push the idea that Python is the language for AI ignoring many other important stuff, for example the C++ core of the python interface. I don't hate python/julia and not a Go/JS fanboy. -- 1e100 (talk) 11:16, 18 December 2021 (UTC)Reply[reply]\n",
      "\n",
      "https://en.wikipedia.org/wiki/U-Net\n",
      "U-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg.[1] The network is based on a fully convolutional neural network[2] whose architecture was modified and extended to work with fewer training images and to yield more precise segmentation. Segmentation of a 512 × 512 image takes less than a second on a modern GPU.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Control_variable_(programming)\n",
      "In computer science, control flow (or flow of control) is the order in which individual statements, instructions or function calls of an imperative program are executed or evaluated. The emphasis on explicit control flow distinguishes an imperative programming language from a declarative programming language.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Matrix_(mathematics)\n",
      "In mathematics, a matrix (pl.: matrices) is a rectangular array or table of numbers, symbols, or expressions, arranged in rows and columns, which is used to represent a mathematical object or a property of such an object.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Software_construction\n",
      "Software construction is a software engineering discipline. It is the detailed creation of working meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging. It is linked to all the other software engineering disciplines, most strongly to software design and software testing.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Bayesian_optimization\n",
      "Bayesian optimization is a sequential design strategy for global optimization of black-box functions[1][2][3]  that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ordinary_least_squares\n",
      "In statistics, ordinary least squares (OLS) is a type of linear least squares method for choosing the unknown parameters in a linear regression model (with fixed level-one effects of a linear function of a set of explanatory variables) by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the input dataset and the output of the (linear) function of the independent variable.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Database\n",
      "In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Software_maintenance\n",
      "Software maintenance in software engineering is the modification of a software product after delivery to correct faults, to improve performance or other attributes.[1][2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/GPT-J\n",
      "GPT-J or GPT-J-6B is an open-source large language model (LLM) developed by EleutherAI in 2021.[1] As the name suggests, it is a generative pre-trained transformer model designed to produce human-like text that continues from a prompt. The optional \"6B\" in the name refers to the fact that it has 6 billion parameters.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Vision_transformer\n",
      "A vision transformer (ViT) is a transformer designed for computer vision. Transformers were introduced in 2017,[1] and have found widespread use in natural language processing. In 2020, they were adapted for computer vision, yielding ViT.[2] The basic structure is to break down input images as a series of patches, then tokenized, before applying the tokens to a standard Transformer architecture.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Conference_on_Knowledge_Discovery_and_Data_Mining\n",
      "SIGKDD, representing the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining, hosts an influential annual conference.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Trevor_Hastie\n",
      "Trevor John Hastie (born 27 June 1953) is an American statistician and computer scientist. He is currently serving as the John A. Overdeck Professor of Mathematical Sciences and Professor of Statistics at Stanford University.[1] Hastie is known for his contributions to applied statistics, especially in the field of machine learning, data mining, and bioinformatics. He has authored several popular books in statistical learning, including The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Hastie has been listed as an ISI Highly Cited Author in Mathematics by the ISI Web of Knowledge.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Polynomial_regression\n",
      "In statistics, polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y |x). Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function E(y | x) is linear in the unknown parameters that are estimated from the data.  For this reason, polynomial regression is considered to be a special case of multiple linear regression.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Dimensionality_reduction\n",
      "Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with). Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/ELKI\n",
      "ELKI (Environment for Developing KDD-Applications Supported by Index-Structures) is a data mining (KDD, knowledge discovery in databases) software framework developed for use in research and teaching. It was originally at the database systems research unit of Professor Hans-Peter Kriegel at the Ludwig Maximilian University of Munich, Germany, and now continued at the Technical University of Dortmund, Germany. It aims at allowing the development and evaluation of advanced data mining algorithms and their interaction with database index structures.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Information_system\n",
      "An information system (IS) is a formal, sociotechnical, organizational system designed to collect, process, store, and distribute information.[1] From a sociotechnical perspective, information systems are composed by four components: task, people, structure (or roles), and technology.[2] Information systems can be defined as an integration of components for collection, storage and processing of data of which the data is used to provide information, contribute to knowledge as well as digital products that facilitate decision making.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action\n",
      "State–action–reward–state–action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was proposed by Rummery and Niranjan in a technical note[1] with the name \"Modified Connectionist Q-Learning\" (MCQ-L). The alternative name SARSA, proposed by Rich Sutton, was only mentioned as a footnote.\n",
      "\n",
      "https://en.wikipedia.org/wiki/International_Conference_on_Intelligent_Robots_and_Systems\n",
      "IROS, the IEEE/RSJ International Conference on Intelligent Robots and Systems,[1] is an annual academic conference covering advances in robotics.[2] It is one of the premier conferences of its field (alongside ICRA, International Conference on Robotics and Automation) with an 'A' rating from the Australian Ranking of ICT Conferences obtained in 2010 and an 'A1' rating from the Brazilian ministry of education in 2012.[3][4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time\n",
      "In theoretical computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a constant factor.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Matrix_decomposition\n",
      "In the mathematical discipline of linear algebra, a matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. There are many different matrix decompositions; each finds use among a particular class of problems.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Geographic_information_system\n",
      "A geographic information system (GIS) consists of integrated computer hardware and software that store, manage, analyze, edit, output, and visualize geographic data.[1][2] Much of this often happens within a spatial database, however, this is not essential to meet the definition of a GIS.[1] In a broader sense, one may consider such a system also to include human users and support staff, procedures and workflows, the body of knowledge of relevant concepts and methods, and institutional organizations.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Leaf_node\n",
      "In computer science, a tree is a widely used abstract data type that represents a hierarchical tree structure with a set of connected nodes. Each node in the tree can be connected to many children (depending on the type of tree), but must be connected to exactly one parent,[1] except for the root node, which has no parent (i.e., the root node as the top-most node in the tree hierarchy). These constraints mean there are no cycles or \"loops\" (no node can be its own ancestor), and also that each child can be treated like the root node of its own subtree, making recursion a useful technique for tree traversal. In contrast to linear data structures, many trees cannot be represented by relationships between neighboring nodes (parent and children nodes of a node under consideration if they exists) in a single straight line (called edge or link between two adjacent nodes).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Imprecise_probability\n",
      "Imprecise probability generalizes probability theory to allow for partial probability specifications, and is applicable when information is scarce, vague, or conflicting, in which case a unique probability distribution may be hard to identify. Thereby, the theory aims to represent the available knowledge more accurately.  Imprecision is useful for dealing with expert elicitation, because:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Data_collection\n",
      "Data collection or data gathering is the process of gathering and measuring information on targeted variables in an established system, which then enables one to answer relevant questions and evaluate outcomes. Data collection is a research component in all study fields, including physical and social sciences, humanities,[2] and business. While methods vary by discipline, the emphasis on ensuring accurate and honest collection remains the same. The goal for all data collection is to capture evidence that allows data analysis to lead to the formulation of credible answers to the questions that have been posed.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Tensor\n",
      "In mathematics, a tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects related to a vector space. Tensors may map between different objects such as vectors, scalars, and even other tensors. There are many types of tensors, including scalars and vectors (which are the simplest tensors), dual vectors, multilinear maps between vector spaces, and even some operations such as the dot product. Tensors are defined independent of any basis, although they are often referred to by their components in a basis related to a particular coordinate system; those components form an array, which can be thought of as a high-dimensional matrix. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Enterprise_software\n",
      "Enterprise software, also known as enterprise application software (EAS), is computer software used to satisfy the needs of an organization rather than its individual users. Enterprise software is an integral part of a computer-based information system, handling a number of business operations, for example to enhance business and management reporting tasks, or support production operations and back office functions. Enterprise systems must process information at a relatively high speed.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Network_service\n",
      "In computer networking, a network service is an application running at the network application layer and above, that provides data storage, manipulation, presentation, communication or other capability which is often implemented using a client–server or peer-to-peer architecture based on application layer network protocols.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Sample_(statistics)\n",
      "In statistics, quality assurance, and survey methodology, sampling is the selection of a subset or a statistical sample (termed sample for short) of individuals from within a statistical population to estimate characteristics of the whole population. Statisticians attempt to collect samples that are representative of the population. Sampling has lower costs and faster data collection compared to recording data from the entire population, and thus, it can provide insights in cases where it is infeasible to measure an entire population. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Software_configuration_management\n",
      "In software engineering, software configuration management (SCM or S/W CM; also expanded as source configuration management process and software change and configuration management[1]) is the task of tracking and controlling changes in the software, part of the larger cross-disciplinary field of configuration management.[2]  SCM practices include revision control and the establishment of baselines.  If something goes wrong, SCM can determine the \"what, when, why and who\" of the change.  If a configuration is working well, SCM can determine how to replicate it across many hosts.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Force_control\n",
      "Force control is the control of the force with which a machine or the manipulator of a robot acts on an object or its environment. By controlling the contact force, damage to the machine as well as to the objects to be processed and injuries when handling people can be prevented. In manufacturing tasks, it can compensate for errors and reduce wear by maintaining a uniform contact force. Force control achieves more consistent results than position control, which is also used in machine control. Force control can be used as an alternative to the usual motion control, but is usually used in a complementary way, in the form of hybrid control concepts. The acting force for control is usually measured via force transducers or estimated via the motor current.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Very_Large_Scale_Integration\n",
      "Very-large-scale integration (VLSI) is the process of creating an integrated circuit (IC) by combining millions or billions of MOS transistors onto a single chip. VLSI began in the 1970s when MOS integrated circuit (Metal Oxide Semiconductor) chips were developed and then widely adopted, enabling complex semiconductor and telecommunication technologies. The microprocessor and memory chips are VLSI devices.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Syntactic_pattern_recognition\n",
      "Syntactic pattern recognition or structural pattern recognition is a form of pattern recognition, in which each object can be represented by a variable-cardinality set of symbolic, nominal features. This allows for representing pattern structures, taking into account more complex interrelationships between attributes than is possible in the case of flat, numerical feature vectors of fixed dimensionality, that are used in statistical classification.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Real-time_computing\n",
      "Real-time computing (RTC) is the computer science term for hardware and software systems subject to a \"real-time constraint\", for example from event to system response.[1] Real-time programs must guarantee response within specified time constraints, often referred to as \"deadlines\".[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Network_architecture\n",
      "Network architecture is the design of a computer network. It is a framework for the specification of a network's physical components and their functional organization and configuration, its operational principles and procedures, as well as communication protocols used.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Dynamic_Bayesian_network\n",
      "A dynamic Bayesian network (DBN) is a Bayesian network (BN) which relates variables to each other over adjacent time steps. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Adaptive_website\n",
      "An adaptive website is a website that builds a model of user activity and modifies the information and/or presentation of information to the user in order to better address the user's needs.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Loss_function\n",
      "In mathematical optimization and decision theory, a loss function or cost function (sometimes also called an error function) [1] is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its opposite (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. The loss function could include terms from several levels of the hierarchy.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Random_variables\n",
      "A random variable (also called random quantity, aleatory variable, or stochastic variable) is a mathematical formalization of a quantity or object which depends on random events.[1] The term 'random variable' can be misleading as its mathematical definition is not actually random nor a variable,[2] but rather it is a function from possible outcomes (e.g., the possible upper sides of a flipped coin such as heads \n",
      "\n",
      "\n",
      "\n",
      "H\n",
      "\n",
      "\n",
      "{\\displaystyle H}\n",
      "\n",
      " and tails \n",
      "\n",
      "\n",
      "\n",
      "T\n",
      "\n",
      "\n",
      "{\\displaystyle T}\n",
      "\n",
      ") in a sample space (e.g., the set \n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "H\n",
      ",\n",
      "T\n",
      "}\n",
      "\n",
      "\n",
      "{\\displaystyle \\{H,T\\}}\n",
      "\n",
      ") to a measurable space (e.g., \n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "−\n",
      "1\n",
      ",\n",
      "1\n",
      "}\n",
      "\n",
      "\n",
      "{\\displaystyle \\{-1,1\\}}\n",
      "\n",
      " in which 1 is corresponding to \n",
      "\n",
      "\n",
      "\n",
      "H\n",
      "\n",
      "\n",
      "{\\displaystyle H}\n",
      "\n",
      " and −1 is corresponding to \n",
      "\n",
      "\n",
      "\n",
      "T\n",
      "\n",
      "\n",
      "{\\displaystyle T}\n",
      "\n",
      ", respectively), often to the real numbers.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Echo_state_network\n",
      "An echo state network (ESN)[1][2] is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can produce or reproduce specific temporal patterns. The main interest of this network is that although its behavior is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Social_software\n",
      "Social software, also known as social apps or social platform includes communications and interactive tools that are often based on the Internet. Communication tools typically handle capturing, storing and presenting communication, usually written but increasingly including audio and video as well. Interactive tools handle mediated interactions between a pair or group of users. They focus on establishing and maintaining a connection among users, facilitating the mechanics of conversation and talk.[1] Social software generally refers to software that makes collaborative behaviour, the organisation and moulding of communities, self-expression, social interaction and feedback possible for individuals. Another element of the existing definition of social software is that it allows for the structured mediation of opinion between people, in a centralized or self-regulating manner. The most improved area for social software is that Web 2.0 applications can all promote co-operation between people and the creation of online communities more than ever before. The opportunities offered by social software are instant connections and opportunities to learn.[2]An additional defining feature of social software is that apart from interaction and collaboration, it aggregates the collective behaviour of its users, allowing not only crowds to learn from an individual but individuals to learn from the crowds as well.[3] Hence, the interactions enabled by social software can be one-to-one, one-to-many, or many-to-many.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Process_control\n",
      "An industrial process control or simply process control in continuous production processes is a discipline that uses industrial control systems and control theory to achieve a production level of consistency, economy and safety which could not be achieved purely by human manual control. It is implemented widely in industries such as automotive, mining, dredging, oil refining, pulp and paper manufacturing, chemical processing and power generating plants.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mutation_(genetic_algorithm)\n",
      "Mutation is a genetic operator used to maintain genetic diversity of the chromosomes of a population of a genetic or, more generally, an evolutionary algorithm (EA). It is analogous to biological mutation.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Medical_diagnostics\n",
      "Medical diagnosis (abbreviated Dx,[1] Dx, or Ds) is the process of determining which disease or condition explains a person's symptoms and signs. It is most often referred to as diagnosis with the medical context being implicit. The information required for diagnosis is typically collected from a history and physical examination of the person seeking medical care. Often, one or more diagnostic procedures, such as medical tests, are also done during the process. Sometimes the posthumous diagnosis is considered a kind of medical diagnosis.\n",
      "\n",
      "https://en.wikipedia.org/wiki/SequenceL\n",
      "SequenceL is a general purpose functional programming language and auto-parallelizing (Parallel computing) compiler and tool set, whose primary design objectives are performance on multi-core processor hardware, ease of programming, platform portability/optimization, and code clarity and readability.  Its main advantage is that it can be used to write straightforward code that automatically takes full advantage of all the processing power available, without programmers needing to be concerned with identifying parallelisms, specifying vectorization, avoiding race conditions, and other challenges of manual directive-based programming approaches such as OpenMP.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Climate_Science\n",
      "Climatology (from Greek κλίμα, klima, \"slope\"; and -λογία, -logia) or climate science is the scientific study of Earth's climate, typically defined as weather conditions averaged over a period of at least 30 years.[1] Climate concerns the atmospheric condition during an extended to indefinite period of time; weather is the condition of the atmosphere during a relative brief period of time. The main topics of research are the study of climate variability, mechanisms of climate changes and modern climate change.[2][3] This topic of study is regarded as part of the atmospheric sciences and a subdivision of physical geography, which is one of the Earth sciences. Climatology includes some aspects of oceanography and biogeochemistry.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Convolution\n",
      "In mathematics (in particular, functional analysis), convolution is a mathematical operation on two functions (f and g) that produces a third function (\n",
      "\n",
      "\n",
      "\n",
      "f\n",
      "∗\n",
      "g\n",
      "\n",
      "\n",
      "{\\displaystyle f*g}\n",
      "\n",
      ") that expresses how the shape of one is modified by the other. The term convolution refers to both the result function and to the process of computing it. It is defined as the integral of the product of the two functions after one is reflected about the y-axis and shifted. The choice of which function is reflected and shifted before the integral does not change the integral result (see commutativity). The integral is evaluated for all values of shift, producing the convolution function.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Application_security\n",
      "Application security (short AppSec) includes all tasks that introduce a secure software development life cycle to development teams. Its final goal is to improve security practices and, through that, to find, fix and preferably prevent security issues within applications. It encompasses the whole application life cycle from requirements analysis, design, implementation, verification as well as maintenance.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Nature_Methods\n",
      "Nature Methods is a monthly peer-reviewed scientific journal covering new scientific techniques. It was established in 2004 and is published by Springer Nature under the Nature Portfolio. Like other Nature journals, there is no external editorial board and editorial decisions are made by an in-house team, although peer review by external experts forms a part of the review process.[1] The editor-in-chief is Allison Doerr.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Template_talk:Computer_science\n",
      "This part is a bit short. But does OLAP really belong into here? is it really \"major\"?\n",
      "\n",
      "https://en.wikipedia.org/wiki/Anthropic\n",
      "Anthropic PBC is an American artificial intelligence (AI) startup company, founded by former members of OpenAI.[3][4] Anthropic develops general AI systems and large language models.[5] It is a public-benefit corporation, and has been connected to the effective altruism movement.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ridge_regression\n",
      "Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated.[1] It has been used in many fields including econometrics, chemistry, and engineering.[2] Also known as Tikhonov regularization, named for Andrey Tikhonov, it is a method of regularization of ill-posed problems.[a] It is particularly useful to mitigate the problem of multicollinearity in linear regression, which commonly occurs in models with large numbers of parameters.[3] In general, the method provides improved efficiency in parameter estimation problems in exchange for a tolerable amount of bias (see bias–variance tradeoff).[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Health_informatics\n",
      "  \n",
      "  \n",
      "  \n",
      "\n",
      "https://en.wikipedia.org/wiki/Tomasz_Imieli%C5%84ski\n",
      "Tomasz Imieliński (born July 11, 1954, in Toruń, Poland) is a Polish-American computer scientist, most known in the areas of data mining, mobile computing, data extraction, and search engine technology. He is currently a professor of computer science at Rutgers University in New Jersey, United States.\n",
      "\n",
      "https://en.wikipedia.org/wiki/False_negative_rate\n",
      "A false positive is an error in binary classification in which a test result incorrectly indicates the presence of a condition (such as a disease when the disease is not present), while a false negative is the opposite error, where the test result incorrectly indicates the absence of a condition when it is actually present. These are the two kinds of errors in a binary test, in contrast to the two kinds of correct result (a true positive and a true negative). They are also known in medicine as a false positive (or false negative) diagnosis, and in statistical classification as a false positive (or false negative) error.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/User_behavior_analytics\n",
      "User behavior analytics (UBA) or user and entity behavior analytics (UEBA),[1] is the concept of analyzing the behavior of users, subjects, visitors, etc. for a specific purpose.[2] It allows cybersecurity tools to build a profile of each individual's normal activity, by looking at patterns of human behavior, and then highlighting deviations from that profile (or anomalies) that may indicate a potential compromise.[3][4][5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Factor_analysis\n",
      "Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors plus \"error\" terms, hence factor analysis can be thought of as a special case of errors-in-variables models.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/ACM_Computing_Surveys\n",
      "ACM Computing Surveys is peer-reviewed quarterly scientific journal and is published by the Association for Computing Machinery. It publishes survey articles and tutorials related to computer science and computing. The journal was established in 1969 with William S. Dorn as founding editor-in-chief.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Statistical_manifold\n",
      "In mathematics, a statistical manifold is a Riemannian manifold, each of whose points is a probability distribution.  Statistical manifolds provide a setting for the field of information geometry.  The Fisher information metric provides a metric on these manifolds. Following this definition, the log-likelihood function is a differentiable map and the score is an inclusion.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Neural_network_with_dark_background.png\n",
      "Original file ‎(1,280 × 1,039 pixels, file size: 837 KB, MIME type: image/png)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
      "In the context of artificial neural networks, the rectifier or ReLU (rectified linear unit) activation function[1][2] is an activation function defined as the positive part of its argument:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Document_management_system\n",
      "\n",
      "A document management system (DMS) is usually a computerized system used to store, share, track and manage files or documents. Some systems include history tracking where a log of the various versions created and modified by different users is recorded. The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Operations_research\n",
      "\n",
      "Operations research (British English: operational research) (U.S. Air Force Specialty Code: Operations Analysis), often shortened to the initialism OR, is a discipline that deals with the development and application of analytical methods to improve decision-making.[1] The term management science is occasionally used as a synonym.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ehud_Shapiro\n",
      "Ehud Shapiro (Hebrew: אהוד שפירא; born 1955) is an Israeli scientist, artist, and entrepreneur, who is Professor of Computer Science and Biology at the Weizmann Institute of Science.[2] With international reputation, he made fundamental contributions to many scientific disciplines,[3] laying in each a long-term research agenda by asking a novel basic question and offering a first step towards answering it, including how to computerize the process of scientific discovery, by providing an algorithmic interpretation to Karl Popper's methodology of conjectures and refutations;[4][5] how to automate program debugging, by algorithms for fault localization;[6] how to unify parallel, distributed, and systems programming with a high-level logic-based programming language;[7] how to use the metaverse as a foundation for social networking;[8] how to devise molecular computers that can function as smart programmable drugs;[9][10] how to uncover the human cell lineage tree, via single-cell genomics;[11][12] how to support digital democracy, by devising an alternative architecture to the digital realm.[13][14]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Diffusion_process\n",
      "In probability theory and statistics, diffusion processes are a class of continuous-time Markov process with almost surely continuous sample paths. Diffusion process is stochastic in nature and hence is used to model many real-life stochastic systems. Brownian motion, reflected Brownian motion and Ornstein–Uhlenbeck processes are examples of diffusion processes. It is used heavily in statistical physics, statistical analysis, information theory, data science, neural networks, finance and marketing.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Microcontrollers\n",
      "A microcontroller (MC, UC, or μC) or microcontroller unit (MCU) is a small computer on a single integrated circuit. A microcontroller contains one or more CPUs (processor cores) along with memory and programmable input/output peripherals. Program memory in the form of ferroelectric RAM, NOR flash or OTP ROM is also often included on chip, as well as a small amount of RAM. Microcontrollers are designed for embedded applications, in contrast to the microprocessors used in personal computers or other general purpose applications consisting of various discrete chips.\n",
      "\n",
      "https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\n",
      "t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map. It is based on Stochastic Neighbor Embedding originally developed by Geoffrey Hinton and Sam Roweis,[1] where Laurens van der Maaten proposed the t-distributed variant.[2] It is a nonlinear dimensionality reduction technique for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Hardware_security\n",
      "Hardware security is a discipline originated from the cryptographic engineering and involves hardware design, access control, secure multi-party computation, secure key storage, ensuring code authenticity, measures to ensure that the supply chain that built the product is secure among other things.[1][2][3][4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Crossover_(genetic_algorithm)\n",
      "In genetic algorithms and evolutionary computation, crossover, also called recombination, is a genetic operator used to combine the genetic information of two parents to generate new offspring. It is one way to stochastically generate new solutions from an existing population, and is analogous to the crossover that happens during sexual reproduction in biology. Solutions can also be generated by cloning an existing solution, which is analogous to asexual reproduction. Newly generated solutions may be mutated before being added to the population.\n",
      "\n",
      "https://en.wikipedia.org/wiki/CiteSeerX_(identifier)\n",
      "CiteSeerX (formerly called CiteSeer) is a public search engine and digital library for scientific and academic papers, primarily in the fields of computer and information science.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Tensor_calculus\n",
      "In mathematics, tensor calculus, tensor analysis, or Ricci calculus is an extension of vector calculus to tensor fields (tensors that may vary over a manifold, e.g. in spacetime).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Intrusion_detection\n",
      "An intrusion detection system (IDS; also intrusion prevention system or IPS) is a device or software application that monitors a network or systems for malicious activity or policy violations.[1] Any intrusion activity or violation is typically reported either to an administrator or collected centrally using a security information and event management (SIEM) system. A SIEM system combines outputs from multiple sources and uses alarm filtering techniques to distinguish malicious activity from false alarms.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Digital_library\n",
      "A digital library, also called an online library, an internet library, a digital repository,  a library without walls, or a digital collection, is an online database of digital objects that can include text, still images, audio, video, digital documents, or other digital media formats or a library accessible through the internet. Objects can consist of digitized content like print or photographs, as well as originally produced digital content like word processor files or social media posts. In addition to storing content, digital libraries provide means for organizing, searching, and retrieving the content contained in the collection. Digital libraries can vary immensely in size and scope, and can be maintained by individuals or organizations.[1] The digital content may be stored locally, or accessed remotely via computer networks. These information retrieval systems are able to exchange information with each other through interoperability and sustainability.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Software_quality\n",
      "In the context of software engineering, software quality refers to two related but distinct notions:[citation needed]\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Decision_Tree.jpg\n",
      "Decision_Tree.jpg ‎(457 × 473 pixels, file size: 17 KB, MIME type: image/jpeg)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Possibility_theory\n",
      "Possibility theory is a mathematical theory for dealing with certain types of uncertainty and is an alternative to probability theory. It uses measures of possibility and necessity between 0 and 1, ranging from impossible to possible and unnecessary to necessary, respectively. Professor Lotfi Zadeh first introduced possibility theory in 1978 as an extension of his theory of fuzzy sets and fuzzy logic. Didier Dubois and Henri Prade further contributed to its development. Earlier, in the 1950s, economist G. L. S. Shackle proposed the min/max algebra to describe degrees of potential surprise.\n",
      "\n",
      "https://en.wikipedia.org/wiki/BIRCH\n",
      "BIRCH (balanced iterative reducing and clustering using hierarchies) is an unsupervised data mining algorithm used to perform hierarchical clustering over particularly large data-sets.[1] With modifications it can also be used to accelerate k-means clustering and Gaussian mixture modeling with the expectation–maximization algorithm.[2] An advantage of BIRCH is its ability to incrementally and dynamically cluster incoming, multi-dimensional metric data points in an attempt to produce the best quality clustering for a given set of resources (memory and time constraints). In most cases, BIRCH only requires a single scan of the database.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Networking_hardware\n",
      "\n",
      "Networking hardware, also known as network equipment or computer networking devices, are electronic devices that are required for communication and interaction between devices on a computer network. Specifically, they mediate data transmission in a computer network.[1] Units which are the last receiver or generate data are called hosts, end systems or data terminal equipment.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Network_security\n",
      "Network security consists of the policies, processes and practices adopted to prevent, detect and monitor unauthorized access, misuse, modification, or denial of a computer network and network-accessible resources.[1] Network security involves the authorization of access to data in a network, which is controlled by the network administrator. Users choose or are assigned an ID and password or other authenticating information that allows them access to information and programs within their authority. Network security covers a variety of computer networks, both public and private, that are used in everyday jobs: conducting transactions and communications among businesses, government agencies and individuals. Networks can be private, such as within a company, and others which might be open to public access. Network security is involved in organizations, enterprises, and other types of institutions. It does as its title explains: it secures the network, as well as protecting and overseeing operations being done. The most common and simple way of protecting a network resource is by assigning it a unique name and a corresponding password.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Data_quality\n",
      "Data quality refers to the state of qualitative or quantitative pieces of information. There are many definitions of data quality, but data is generally considered high quality if it is \"fit for [its] intended uses in operations, decision making and planning\".[1][2][3] Moreover, data is deemed of high quality if it correctly represents the real-world construct to which it refers. Furthermore, apart from these definitions, as the number of data sources increases, the question of internal data consistency becomes significant, regardless of fitness for use for any particular external purpose. People's views on data quality can often be in disagreement, even when discussing the same set of data used for the same purpose. When this is the case, data governance is used to form agreed upon definitions and standards for data quality. In such cases, data cleansing, including standardization, may be required in order to ensure data quality.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Software_repository\n",
      "A software repository, or repo for short, is a storage location for software packages. Often a table of contents is also stored, along with metadata. A software repository is typically managed by source or version control, or repository managers. Package managers allow automatically installing and updating repositories, sometimes called \"packages\".\n",
      "\n",
      "https://en.wikipedia.org/wiki/Sensor\n",
      "A sensor is a device that produces an output signal for the purpose of sensing a physical phenomenon.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Isolation_forest\n",
      "Isolation Forest is an algorithm for data anomaly detection initially developed by Fei Tony Liu in 2008.[1] Isolation Forest detects anomalies using binary trees. The algorithm has a linear time complexity and a low memory requirement, which works well with high-volume data.[2][3]\n",
      "In essence, the algorithm relies upon the characteristics of anomalies, i.e., being few and different, in order to detect anomalies. No density estimation is performed in the algorithm. The algorithm is different from decision tree algorithms in that only the path-length measure or approximation is being used to generate the anomaly score, no leaf node statistics on class distribution or target value is needed.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Dempster%E2%80%93Shafer_theory\n",
      "The theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory (DST), is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. First introduced by Arthur P. Dempster[1] in the context of statistical inference, the theory was later developed by Glenn Shafer into a general framework for modeling epistemic uncertainty—a mathematical theory of evidence.[2][3] The theory allows one to combine evidence from different sources and arrive at a degree of belief (represented by a mathematical object called belief function) that takes into account all the available evidence.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ranking\n",
      "A ranking is a relationship between a set of items such that, for any two items, the first is either \"ranked higher than\", \"ranked lower than\", or \"ranked equal to\" the second.[1] In mathematics, this is known as a weak order or total preorder of objects. It is not necessarily a total order of objects because two different objects can have the same ranking. The rankings themselves are totally ordered. For example, materials are totally preordered by hardness, while degrees of hardness are totally ordered. If two items are the same in rank it is considered a tie.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Dependability\n",
      "In systems engineering, dependability is a measure of a system's availability, reliability, maintainability, and in some cases, other characteristics such as durability, safety and security.[1]  In real-time computing, dependability is the ability to provide services that can be trusted within a time-period.[2] The service guarantees must hold even when the system is subject to attacks or natural failures. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Operational_definition\n",
      "An operational definition specifies concrete, replicable procedures designed to represent a construct. In the words of American psychologist S.S. Stevens (1935), \"An operation is the performance which we execute in order to make known a concept.\"[1][2] For example, an operational definition of \"fear\" (the construct) often includes measurable physiologic responses that occur in response to a perceived threat. Thus, \"fear\" might be operationally defined as specified changes in heart rate, galvanic skin response, pupil dilation, and blood pressure.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Differentiable_function\n",
      "In mathematics, a differentiable function of one real variable is a function whose derivative exists at each point in its domain. In other words, the graph of a differentiable function has a non-vertical tangent line at each interior point in its domain. A differentiable function is smooth (the function is locally well approximated as a linear function at each interior point) and does not contain any break, angle, or cusp.\n",
      "\n",
      "https://en.wikipedia.org/wiki/CURE_algorithm\n",
      "CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases[citation needed]. Compared with K-means clustering it is more robust to outliers and able to identify clusters having non-spherical shapes and size variances.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Multimedia_database\n",
      "A Multimedia database (MMDB) is a collection of related for multimedia data.[1] The multimedia data include one or more primary media data types such as text, images, graphic objects (including drawings, sketches and illustrations) animation sequences, audio and video.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Multivariate_normal_distribution\n",
      "In probability theory and statistics, the multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions.  One definition is that a random vector is said to be k-variate normally distributed if every linear combination of its k components has a univariate normal distribution. Its importance derives mainly from the multivariate central limit theorem. The multivariate normal distribution is often used to describe, at least approximately, any set of (possibly) correlated real-valued random variables each of which clusters around a mean value.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Electrochemical_RAM\n",
      "Electrochemical Random-Access Memory (ECRAM) is a type of non-volatile memory (NVM) with multiple levels per cell (MLC) designed for deep learning analog acceleration.[1][2][3] An ECRAM cell is a three-terminal device composed of a conductive channel, an insulating electrolyte, an ionic reservoir, and metal contacts. The resistance of the channel is modulated by ionic exchange at the interface between the channel and the electrolyte upon application of an electric field. The charge-transfer process allows both for state retention in the absence of applied power, and for programming of multiple distinct levels, both differentiating ECRAM operation from that of a field-effect transistor (FET). The write operation is deterministic and can result in symmetrical potentiation and depression, making ECRAM arrays attractive for acting as artificial synaptic weights in physical implementations of artificial neural networks (ANN). The technological challenges include open circuit potential (OCP) and semiconductor foundry compatibility associated with energy materials. Universities, government laboratories, and corporate research teams have contributed to the development of ECRAM for analog computing. Notably, Sandia National Laboratories designed a lithium-based cell inspired by solid-state battery materials,[4] Stanford University built an organic proton-based cell,[5] and International Business Machines (IBM) demonstrated in-memory selector-free parallel programming for a logistic regression task in an array of metal-oxide ECRAM designed for insertion in the back end of line (BEOL).[6] In 2022, researchers at Massachusetts Institute of Technology built an inorganic, CMOS-compatible protonic technology that achieved near-ideal modulation characteristics using nanosecond fast pulses [7]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Softmax_function\n",
      "The softmax function, also known as softargmax[1]: 184  or normalized exponential function,[2]: 198  converts a vector of K real numbers into a probability distribution of K possible outcomes. It is a generalization of the logistic function to multiple dimensions, and used in multinomial logistic regression. The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes, based on Luce's choice axiom.\n",
      "\n",
      "https://en.wikipedia.org/wiki/White-box_testing\n",
      "White-box testing (also known as clear box testing, glass box testing, transparent box testing, and structural testing) is a method of software testing that tests internal structures or workings of an application, as opposed to its functionality (i.e. black-box testing). In white-box testing, an internal perspective of the system is used to design test cases. The tester chooses inputs to exercise paths through the code and determine the expected outputs. This is analogous to testing nodes in a circuit, e.g. in-circuit testing (ICT).\n",
      "White-box testing can be applied at the unit, integration and system levels of the software testing process. Although traditional testers tended to think of white-box testing as being done at the unit level, it is used for integration and system testing more frequently today. It can test paths within a unit, paths between units during integration, and between subsystems during a system–level test. Though this method of test design can uncover many errors or problems, it has the potential to miss unimplemented parts of the specification or missing requirements. Where white-box testing is design-driven,[1] that is, driven exclusively by agreed specifications of how each component of software is required to behave (as in DO-178C and ISO 26262 processes), white-box test techniques can accomplish assessment for unimplemented or missing requirements.\n",
      "\n",
      "https://en.wikipedia.org/wiki/ROOT\n",
      "ROOT is an object-oriented computer program and library developed by CERN. It was originally designed for particle physics data analysis and contains several features specific to the field, but it is also used in other applications such as astronomy and data mining.  The latest minor release is 6.28, as of 2023-02-03.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Jerome_H._Friedman\n",
      "Jerome Harold Friedman (born December 29, 1939) is an American statistician, consultant and Professor of Statistics at Stanford University, known for his contributions in the field of statistics and data mining.[2][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Neuron\n",
      "Within a nervous system, a neuron, neurone, or nerve cell is an electrically excitable cell that fires electric signals called action potentials across a neural network. Neurons communicate with other cells via synapses, which are specialized connections that commonly use minute amounts of chemical neurotransmitters to pass the electric signal from the presynaptic neuron to the target cell through the synaptic gap. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Logical_conjunction\n",
      "In logic, mathematics and linguistics, and (\n",
      "\n",
      "\n",
      "\n",
      "∧\n",
      "\n",
      "\n",
      "{\\displaystyle \\wedge }\n",
      "\n",
      ") is the truth-functional operator of conjunction or logical conjunction. The logical connective of this operator is typically represented as \n",
      "\n",
      "\n",
      "\n",
      "∧\n",
      "\n",
      "\n",
      "{\\displaystyle \\wedge }\n",
      "\n",
      "[1] or \n",
      "\n",
      "\n",
      "\n",
      "&\n",
      "\n",
      "\n",
      "{\\displaystyle \\&}\n",
      "\n",
      " or \n",
      "\n",
      "\n",
      "\n",
      "K\n",
      "\n",
      "\n",
      "{\\displaystyle K}\n",
      "\n",
      " (prefix) or \n",
      "\n",
      "\n",
      "\n",
      "×\n",
      "\n",
      "\n",
      "{\\displaystyle \\times }\n",
      "\n",
      " or \n",
      "\n",
      "\n",
      "\n",
      "⋅\n",
      "\n",
      "\n",
      "{\\displaystyle \\cdot }\n",
      "\n",
      "[2] in which \n",
      "\n",
      "\n",
      "\n",
      "∧\n",
      "\n",
      "\n",
      "{\\displaystyle \\wedge }\n",
      "\n",
      " is the most modern and widely used.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Continuous_production\n",
      "Continuous production is a flow production method used to manufacture, produce, or process materials without interruption.  Continuous production is called a continuous process or a continuous flow process because the materials, either dry bulk or fluids that are being processed are continuously in motion, undergoing chemical reactions or subject to mechanical or heat treatment.  Continuous processing is contrasted with batch production.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Tomographic_reconstruction\n",
      "Tomographic reconstruction is a type of multidimensional inverse problem where the challenge is to yield an estimate of a specific system from a finite number of projections. The mathematical basis for tomographic imaging was laid down by Johann Radon. A notable example of applications is the reconstruction of computed tomography (CT) where cross-sectional images of patients are obtained in non-invasive manner. Recent developments have seen the Radon transform and its inverse used for tasks related to realistic object insertion required for testing and evaluating computed tomography use in airport security.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Backdoor_(computing)\n",
      "A backdoor is a typically covert method of bypassing normal authentication or encryption in a computer, product, embedded device (e.g. a home router), or its embodiment (e.g. part of a cryptosystem, algorithm, chipset, or even a \"homunculus computer\"—a tiny computer-within-a-computer such as that found in Intel's AMT technology).[1][2] Backdoors are most often used for securing remote access to a computer, or obtaining access to plaintext in cryptosystems. From there it may be used to gain access to privileged information like passwords, corrupt or delete data on hard drives, or transfer information within autoschediastic networks.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Chromosome_(genetic_algorithm)\n",
      "In genetic algorithms (GA), or more general, evolutionary algorithms (EA), a chromosome (also sometimes called a genotype) is a set of parameters which define a proposed solution of the problem that the evolutionary algorithm is trying to solve. The set of all solutions, also called individuals according to the biological model, is known as the population.[1][2] The genome of an individual consists of one, more rarely of several,[3][4] chromosomes and corresponds to the genetic representation of the task to be solved. A chromosome is composed of a set of genes, where a gene consists of one or more semantically connected parameters, which are often also called decision variables. They determine one or more phenotypic characteristics of the individual or at least have an influence on them.[2]  In the basic form of genetic algorithms, the chromosome is represented as a binary string,[5] while in later variants[6][7] and in EAs in general, a wide variety of other data structures are used.[8][9][10]\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Colored_neural_network.svg\n",
      "Original file ‎(SVG file, nominally 296 × 356 pixels, file size: 206 KB)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Influence_diagram\n",
      "An influence diagram (ID) (also called a relevance diagram, decision diagram or a decision network) is a compact graphical and mathematical representation of a decision situation.  It is a generalization of a Bayesian network, in which not only probabilistic inference problems but also decision making problems (following the maximum expected utility criterion) can be modeled and solved.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Entailment\n",
      "Logical consequence (also entailment) is a fundamental concept in logic which describes the relationship between statements that hold true when one statement logically follows from one or more statements. A valid logical argument is one in which the conclusion is entailed by the premises, because the conclusion is the consequence of the premises. The philosophical analysis of logical consequence involves the questions: In what sense does a conclusion follow from its premises? and What does it mean for a conclusion to be a consequence of premises?[1] All of philosophical logic is meant to provide accounts of the nature of logical consequence and the nature of logical truth.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Sparse_matrix\n",
      "In numerical analysis and scientific computing, a sparse matrix or sparse array is a matrix in which most of the elements are zero.[1] There is no strict definition regarding the proportion of zero-value elements for a matrix to qualify as sparse but a common criterion is that the number of non-zero elements is roughly equal to the number of rows or columns. By contrast, if most of the elements are non-zero, the matrix is considered dense.[1] The number of zero-valued elements divided by the total number of elements (e.g., m × n for an m × n matrix) is sometimes referred to as the sparsity of the matrix.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Leo_Breiman\n",
      "Leo Breiman (January 27, 1928 – July 5, 2005) was a distinguished statistician at the University of California, Berkeley. He was the recipient of numerous honors and awards,[citation needed] and was a member of the United States National Academy of Sciences.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Enterprise_information_system\n",
      "An Enterprise Information System (EIS) is any kind of information system which improves the functions of enterprise business processes by integration. This means typically offering high quality of service, dealing with large volumes of data and capable of supporting some large and possibly complex organization or enterprise. An EIS must be able to be used by all parts and all levels of an enterprise.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Information_security\n",
      "\n",
      "Information security, sometimes shortened to InfoSec,[1] is the practice of protecting information by mitigating information risks. It is part of information risk management.[2][3] It typically involves preventing or reducing the probability of unauthorized or inappropriate access to data or the unlawful use, disclosure, disruption, deletion, corruption, modification, inspection, recording, or devaluation of information.[citation needed] It also involves actions intended to reduce the adverse impacts of such incidents. Protected information may take any form, e.g., electronic or physical, tangible (e.g., paperwork), or intangible (e.g., knowledge).[4][5] Information security's primary focus is the balanced protection of data confidentiality, integrity, and availability (also known as the \"CIA\" triad) while maintaining a focus on efficient policy implementation, all without hampering organization productivity.[6] This is largely achieved through a structured risk management process that involves: \n",
      "\n",
      "https://en.wikipedia.org/wiki/PMC_(identifier)\n",
      "PubMed Central (PMC) is a free digital repository that archives open access full-text scholarly articles that have been published in biomedical and life sciences journals. As one of the major research databases developed by the National Center for Biotechnology Information (NCBI), PubMed Central is more than a document repository. Submissions to PMC are indexed and formatted for enhanced metadata, medical ontology, and unique identifiers which enrich the XML structured data for each article.[1] Content within PMC can be linked to other NCBI databases and accessed via Entrez search and retrieval systems, further enhancing the public's ability to discover, read and build upon its biomedical knowledge.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Synapse\n",
      "In the nervous system, a synapse[1] is a structure that permits a neuron (or nerve cell) to pass an electrical or chemical signal to another neuron or to the target effector cell.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Springer_Science%2BBusiness_Media\n",
      "Springer Science+Business Media, commonly known as Springer, is a German multinational publishing company of books, e-books and peer-reviewed journals in science, humanities, technical and medical (STM) publishing.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Strongly_NP-hard\n",
      "In computational complexity, strong NP-completeness is a property of computational problems that is a special case of NP-completeness. A general computational problem may have numerical parameters.  For example, the input to the bin packing problem is a list of objects of specific sizes and a size for the bins that must contain the objects—these object sizes and bin size are numerical parameters.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Internet_fraud\n",
      "Internet fraud is a type of cybercrime fraud or deception which makes use of the Internet and could involve hiding of information or providing incorrect information for the purpose of tricking victims out of money, property, and inheritance.[1] Internet fraud is not considered a single, distinctive crime but covers a range of illegal and illicit actions that are committed in cyberspace.[1] It is, however, differentiated from theft since, in this case, the victim voluntarily and knowingly provides the information, money or property to the perpetrator.[2] It is also distinguished by the way it involves temporally and spatially separated offenders.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Density_estimation\n",
      "In statistics, probability density estimation or simply density estimation is the construction of an estimate, based on observed data, of an unobservable underlying probability density function.  The unobservable density function is thought of as the density according to which a large population is distributed; the data are usually thought of as a random sample from that population.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
      "In medicine and statistics, sensitivity and specificity mathematically describe the accuracy of a test that reports the presence or absence of a medical condition. If individuals who have the condition are considered \"positive\" and those who do not are considered \"negative\", then sensitivity is a measure of how well a test can identify true positives and specificity is a measure of how well a test can identify true negatives:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Manifold\n",
      "In mathematics, a manifold is a topological space that locally resembles Euclidean space near each point. More precisely, an \n",
      "\n",
      "\n",
      "\n",
      "n\n",
      "\n",
      "\n",
      "{\\displaystyle n}\n",
      "\n",
      "-dimensional manifold, or \n",
      "\n",
      "\n",
      "\n",
      "n\n",
      "\n",
      "\n",
      "{\\displaystyle n}\n",
      "\n",
      "-manifold for short, is a topological space with the property that each point has a neighborhood that is homeomorphic to an open subset of \n",
      "\n",
      "\n",
      "\n",
      "n\n",
      "\n",
      "\n",
      "{\\displaystyle n}\n",
      "\n",
      "-dimensional Euclidean space.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Conditional_independence\n",
      "In probability theory, conditional independence describes situations wherein an observation is irrelevant or redundant when evaluating the certainty of a hypothesis. Conditional independence is usually formulated in terms of conditional probability, as a special case where the probability of the hypothesis given the uninformative observation is equal to the probability without. If \n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "\n",
      "\n",
      "{\\displaystyle A}\n",
      "\n",
      " is the hypothesis, and \n",
      "\n",
      "\n",
      "\n",
      "B\n",
      "\n",
      "\n",
      "{\\displaystyle B}\n",
      "\n",
      " and \n",
      "\n",
      "\n",
      "\n",
      "C\n",
      "\n",
      "\n",
      "{\\displaystyle C}\n",
      "\n",
      " are observations, conditional independence can be stated as an equality:\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Svm_max_sep_hyperplane_with_margin.png\n",
      "Original file ‎(800 × 862 pixels, file size: 78 KB, MIME type: image/png)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Robert_Tibshirani\n",
      "Robert Tibshirani FRS FRSC (born July 10, 1956) is a professor in the Departments of Statistics and Biomedical Data Science at Stanford University. He was a professor at the University of Toronto from 1985 to 1998. In his work, he develops statistical tools for the analysis of complex datasets, most recently in genomics and proteomics.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Memtransistor\n",
      "The memtransistor (a blend word from Memory Transfer Resistor) is an experimental multi-terminal passive electronic component that might be used in the construction of artificial neural networks.[1] It is a combination of the memristor and transistor technology.[2] This technology is different from the 1T-1R approach since the devices are merged into one single entity. Multiple memristers can be embedded with a single transistor, enabling it to more accurately model a neuron with its multiple synaptic connections. A neural network produced from these would provide hardware-based artificial intelligence with a good foundation.[1][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Local_outlier_factor\n",
      "In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/DBSCAN\n",
      "Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996.[1]\n",
      "It is a density-based clustering non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away).\n",
      "DBSCAN is one of the most common, and most commonly cited, clustering algorithms.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Wikipedia:Contents\n",
      "\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Articles_with_short_description\n",
      "This category is for articles with short descriptions defined on Wikipedia by {{short description}} (either within the page itself or via another template).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Random_sample_consensus\n",
      "Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method.[1] It is a non-deterministic algorithm in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are allowed. The algorithm was first published by Fischler and Bolles at SRI International in 1981. They used RANSAC to solve the Location Determination Problem (LDP), where the goal is to determine the points in the space that project onto an image into a set of landmarks with known locations.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Social_network\n",
      "A social network is a social structure made up of a set of social actors (such as individuals or organizations), sets of dyadic ties, and other social interactions between actors. The social network perspective provides a set of methods for analyzing the structure of whole social entities as well as a variety of theories explaining the patterns observed in these structures.[1] The study of these structures uses social network analysis to identify local and global patterns, locate influential entities, and examine network dynamics.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Interaction_design\n",
      "\n",
      "Interaction design, often abbreviated as IxD, is \"the practice of designing interactive digital products, environments, systems, and services.\"[1]: xxvii, 30  While interaction design has an interest in form (similar to other design fields), its main area of focus rests on behavior.[1]: xxvii, 30  Rather than analyzing how things are, interaction design synthesizes and imagines things as they could be. This element of interaction design is what characterizes IxD as a design field, as opposed to a science or engineering field.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Real_number\n",
      "In mathematics, a real number is a number that can be used to measure a continuous one-dimensional quantity such as a distance, duration or temperature. Here, continuous means that pairs of values can have arbitrarily small differences.[a] Every real number can be almost uniquely represented by an infinite decimal expansion.[b][1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Articles_with_unsourced_statements_from_May_2022\n",
      "This category combines all articles with unsourced statements from May 2022 (2022-05) to enable us to work through the backlog more systematically. It is a member of Category:Articles with unsourced statements.\n",
      "To add an article to this category add     {{Citation needed|date=May 2022}} to the article. If you omit the date a bot will add it for you at some point.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Oracle_Cloud#Platform_as_a_Service_(PaaS)\n",
      "Oracle Cloud is a cloud computing service offered by Oracle Corporation providing servers, storage, network, applications and services through a global network of Oracle Corporation managed data centers. The company allows these services to be provisioned on demand over the Internet.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Canonical_correlation\n",
      "In statistics, canonical-correlation analysis (CCA), also called canonical variates analysis, is a way of inferring information from cross-covariance matrices. If we have two vectors X = (X1, ..., Xn) and Y = (Y1, ..., Ym)  of random variables, and there are correlations among the variables, then canonical-correlation analysis will find linear combinations of X and Y which have maximum correlation with each other.[1] T. R. Knapp notes that \"virtually all of the commonly encountered parametric tests of significance can be treated as special cases of canonical-correlation analysis, which is the general procedure for investigating the relationships between two sets of variables.\"[2] The method was first introduced by Harold Hotelling in 1936,[3] although in the context of angles between flats the mathematical concept was published by Jordan in 1875.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Network_scheduler\n",
      "A network scheduler, also called packet scheduler, queueing discipline (qdisc) or queueing algorithm, is an arbiter on a node in a packet switching communication network. It manages the sequence of network packets in the transmit and receive queues of the protocol stack and network interface controller. There are several network schedulers available for the different operating systems, that implement many of the existing network scheduling algorithms.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Chemical_synapse\n",
      "Chemical synapses are biological junctions through which neurons' signals can be sent to each other and to non-neuronal cells such as those in muscles or glands. Chemical synapses allow neurons to form circuits within the central nervous system. They are crucial to the biological computations that underlie perception and thought. They allow the nervous system to connect to and control other systems of the body.\n",
      "\n",
      "https://en.wikipedia.org/wiki/RCASE\n",
      "Root Cause Analysis Solver Engine (informally RCASE) is a proprietary algorithm developed from research originally at the Warwick Manufacturing Group (WMG) at Warwick University.[1][2] RCASE development commenced in 2003 to provide an automated version of root cause analysis, the method of problem solving that tries to identify the root causes of faults or problems.[3] RCASE is now owned by the spin-out company Warwick Analytics where it is being applied to automated predictive analytics software.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Paraphrase\n",
      "A paraphrase (/ˈpærəˌfreɪz/) is a restatement of the meaning of a text or passage using other words. The term itself is derived via Latin paraphrasis, from Ancient Greek  παράφρασις (paráphrasis) 'additional manner of expression'. The act of paraphrasing is also called paraphrasis.\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:AI_hierarchy.svg\n",
      "Original file ‎(SVG file, nominally 399 × 399 pixels, file size: 8 KB)\n",
      "\n",
      "https://en.wikipedia.org/wiki/New_England_Journal_of_Medicine\n",
      "The New England Journal of Medicine (NEJM) is a weekly medical journal published by the Massachusetts Medical Society. It is among the most prestigious peer-reviewed medical journals[1][2] as well as the oldest continuously published one.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Errors_and_residuals\n",
      "In statistics and optimization, errors and residuals are two closely related and easily confused measures of the deviation of an observed value of an element of a statistical sample from its \"true value\" (not necessarily observable). The error of an observation is the deviation of the observed value from the true value of a quantity of interest (for example, a population mean). The residual is the difference between the observed value and the estimated value of the quantity of interest (for example, a sample mean). The distinction is most important in regression analysis, where the concepts are sometimes called the regression errors and regression residuals and where they lead to the concept of studentized residuals.\n",
      "In econometrics, \"errors\" are also called disturbances.[1][2][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/DeepSpeed\n",
      "DeepSpeed is an open source deep learning optimization library for PyTorch.[1] The library is designed to reduce computing power and memory use and to train large distributed models with better parallelism on existing computer hardware.[2][3] DeepSpeed is optimized for low latency, high throughput training. It includes the Zero Redundancy Optimizer (ZeRO) for training models with 1 trillion or more parameters.[4] Features include mixed precision training, single-GPU, multi-GPU, and multi-node training as well as custom model parallelism. The DeepSpeed source code is licensed under MIT License and available on GitHub.[5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Pricing\n",
      "Pricing is the process whereby a business sets the price at which it will sell its products and services, and may be part of the business's marketing plan. In setting prices, the business will take into account the price at which it could acquire the goods, the manufacturing cost, the marketplace, competition, market condition, brand, and quality of product.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Piecewise\n",
      "In mathematics, a piecewise-defined function (also called a piecewise function, a hybrid function, or definition by cases) is a function defined by multiple sub-functions, where each sub-function applies to a different interval in the domain.[1][2][3] Piecewise definition is actually a way of expressing the function, rather than a characteristic of the function itself.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Peripheral\n",
      "A peripheral  device, or simply peripheral, is an auxiliary hardware device used to transfer information into and out of a computer.[1] The term peripheral device refers to all hardware components that are attached to a computer and are controlled by the computer system, but they are not the core components of the computer.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Network_performance\n",
      "Network performance refers to measures of service quality of a network as seen by the customer.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Basis_function\n",
      "In mathematics,  a basis function is an element of a particular basis for a function space. Every function in the function space can be represented as a linear combination of basis functions, just as every vector in a vector space can be represented as a linear combination of basis vectors.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Indel\n",
      "Indel (insertion-deletion) is a molecular biology term for an insertion or deletion of bases in the genome of an organism. Indels ≥ 50 bases in length are classified as structural variants.[1][2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Product_placement\n",
      "Product placement, also known as embedded marketing,[1][2][3][4] is a marketing technique where references to specific brands or products are incorporated into another work, such as a film or television program, with specific promotional intent.  Much of this is done by loaning products, especially when expensive items, such as vehicles, are involved.[5]  In 2021, the agreements between brand owners and films and television programs were worth more than US$20 billion.[5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Security_service_(telecommunication)\n",
      "Security service is a service, provided by a layer of communicating open systems, which ensures adequate security of the systems or of data transfers[1] as defined by ITU-T X.800 Recommendation. \n",
      "X.800 and ISO 7498-2 (Information processing systems – Open systems interconnection – Basic Reference Model – Part 2: Security architecture)[2]  are technically aligned. This model is widely recognized [3]\n",
      "[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Template:Computer_science\n",
      "This template's initial visibility currently defaults to autocollapse, meaning that if there is another collapsible item on the page (a navbox, sidebar, or table with the collapsible attribute), it is hidden apart from its title bar; if not, it is fully visible.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Articles_with_GND_identifiers\n",
      "This category has only the following subcategory.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Pan-genome\n",
      "In the fields of molecular biology and genetics, a pan-genome (pangenome or supragenome) is the entire set of genes from all strains within a clade. More generally, it is the union of all the genomes of a clade.[2][3][4][5] The pan-genome can be broken down into a \"core pangenome\" that contains genes present in all individuals, a \"shell pangenome\" that contains genes present in two or more strains, and a \"cloud pangenome\" that contains genes only found in a single strain.[3][4][6] Some authors also refer to the cloud genome as \"accessory genome\" containing 'dispensable' genes present in a subset of the strains and strain-specific genes.[2][3][4] Note that the use of the term 'dispensable' has been questioned, at least in plant genomes, as accessory genes play \"an important role in genome evolution and in the complex interplay between the genome and the environment\".[5] The field of study of pangenomes is called pangenomics.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Chartered_Financial_Analyst_(CFA)#Curriculum\n",
      "The Chartered Financial Analyst (CFA) program is a postgraduate professional certification offered internationally by the America based CFA Institute (formerly the Association for Investment Management and Research, or AIMR) to investment and financial professionals. The program teaches a wide range of subjects relating to advanced investment analysis—including security analysis, statistics, probability theory, fixed income, derivatives, economics, financial analysis, corporate finance, alternative investments, portfolio management—and provides a generalist knowledge of other areas of finance.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Help:Contents\n",
      "\n",
      "\n",
      "\n",
      "https://en.wikipedia.org/wiki/Financial_market\n",
      "A financial market is a market in which people trade financial securities and derivatives at low transaction costs. Some of the securities include stocks and bonds, raw materials and precious metals, which are known in the financial markets as commodities.\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Overfitted_Data.png\n",
      "Overfitted_Data.png ‎(377 × 256 pixels, file size: 14 KB, MIME type: image/png)\n",
      "\n",
      "https://en.wikipedia.org/wiki/False_positive_rate\n",
      "In statistics, when performing multiple comparisons, a false positive ratio (also known as fall-out or false alarm ratio) is the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positives) and the total number of actual negative events (regardless of classification).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:All_articles_with_unsourced_statements\n",
      "\n",
      "This is a category to help keep count of the total number of articles with the {{citation needed}} template.  They should all be in one of the dated categories, which can be found at Category:Articles with unsourced statements.\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Linear_regression.svg\n",
      "Original file ‎(SVG file, nominally 438 × 289 pixels, file size: 71 KB)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Julia_Angwin\n",
      "Julia Angwin is a Pulitzer Prize-winning[1] American investigative journalist,[2] New York Times bestselling author, and entrepreneur. She was a co-founder and editor-in-chief of The Markup, a nonprofit newsroom that investigates the impact of technology on society. She was a senior reporter at ProPublica from 2014 to April 2018[3] and staff reporter at the New York bureau of The Wall Street Journal from 2000 to 2013. Angwin is author of non-fiction books, Stealing MySpace: The Battle to Control the Most Popular Website in America (2009) and Dragnet Nation (2014).[4] She is a winner and two-time finalist for the Pulitzer Prize in journalism.[5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:SpecialPages\n",
      "This page contains a list of special pages. Most of the content of these pages is automatically generated and cannot be edited. To suggest a change to the parts that can be edited, find the appropriate text on Special:AllMessages and then request your change on the talk page of the message (using {{editprotected}} to draw the attention of administrators).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Edge_device\n",
      "In computer networking, an edge device is a device that provides an entry point into enterprise or service provider core networks. Examples include routers, routing switches, integrated access devices (IADs), multiplexers, and a variety of metropolitan area network (MAN) and wide area network (WAN) access devices.  Edge devices also provide connections into carrier and service provider networks. An edge device that connects a local area network to a high speed switch or backbone (such as an ATM switch) may be called an edge concentrator.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Goof\n",
      "A goof is a mistake. The term is also used in a number of specific senses: in cinema, it is an error or oversight during production that is visible in the released version of the film.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Structural_health_monitoring\n",
      "Structural health monitoring (SHM) involves the observation and analysis of a system over time using periodically sampled response measurements to monitor changes to the material and geometric properties of engineering structures such as bridges and buildings.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Covariance_function\n",
      "In probability theory and statistics, the covariance function describes how much two random variables change together (their covariance) with varying spatial or temporal separation. For a random field or stochastic process Z(x) on a domain D, a covariance function C(x, y) gives the covariance of the values of the random field at the two locations x and y:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Springer_Nature\n",
      "Springer Nature or the Springer Nature Group[1][2] is a German-British academic publishing company created by the May 2015 merger of Springer Science+Business Media and Holtzbrinck Publishing Group's Nature Publishing Group, Palgrave Macmillan, and Macmillan Education.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Wikipedia:File_upload_wizard\n",
      "Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand copyright and the image use policy before proceeding.\n",
      "\n",
      "https://en.wikipedia.org/wiki/IEEE_Spectrum\n",
      "IEEE Spectrum is a magazine edited by the Institute of Electrical and Electronics Engineers.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Bank_fraud\n",
      "Bank fraud is the use of potentially illegal means to obtain money, assets, or other property owned or held by a financial institution, or to obtain money from depositors by fraudulently posing as a bank or other financial institution.[1] In many instances, bank fraud is a criminal offence. While the specific elements of particular banking fraud laws vary depending on jurisdictions, the term bank fraud applies to actions that employ a scheme or artifice, as opposed to bank robbery or theft. For this reason, bank fraud is sometimes considered a white-collar crime.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Short_description_is_different_from_Wikidata\n",
      "This category contains articles with short descriptions that do not match the description field on Wikidata. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:RecentChanges\n",
      "This is a list of recent changes to Wikipedia.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Total_operating_characteristic\n",
      "The total operating characteristic (TOC)  is a statistical method to compare a Boolean variable versus a rank variable. TOC can measure the ability of an index variable to diagnose either presence or absence of a characteristic. The diagnosis of presence or absence depends on whether the value of the index is above a threshold. TOC considers multiple possible thresholds. Each threshold generates a two-by-two contingency table, which contains four entries: hits, misses, false alarms, and correct rejections.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Hdl_(identifier)\n",
      "The Handle System is the Corporation for National Research Initiatives's proprietary registry assigning persistent identifiers, or handles, to information resources, and for resolving \"those handles into the information necessary to locate, access, and otherwise make use of the resources\".[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Angoss\n",
      "Angoss Software Corporation, headquartered in Toronto, Ontario, Canada, with offices in the United States and UK, acquired by Datawatch and now owned by Altair, was a provider of predictive analytics systems through software licensing and services. Angoss' customers represent industries including finance, insurance, mutual funds, retail, health sciences, telecom and technology. The company was founded in 1984, and publicly traded on the TSX Venture Exchange from 2008-2013 under the ticker symbol ANC.[citation needed]\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:SimpleBayesNetNodes.svg\n",
      "Original file ‎(SVG file, nominally 246 × 128 pixels, file size: 5 KB)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Peter_E._Hart\n",
      "Peter E. Hart (born 1941[2]) is an American computer scientist and entrepreneur. He was chairman and president of Ricoh Innovations, which he founded in 1997. He made significant contributions in the field of computer science in a series of widely cited publications from the years 1967 to 1975 while associated with the Artificial Intelligence Center of SRI International, a laboratory where he also served as director.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Webarchive_template_wayback_links\n",
      "The following 200 pages are in this category, out of approximately 507,488 total. This list may not reflect recent changes.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:BookSources/978-94-010-6610-5\n",
      "This page allows users to search multiple sources for a book given a 10- or 13-digit International Standard Book Number. Spaces and dashes in the ISBN do not matter.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Portal:Current_events\n",
      "Edit instructions\n",
      "\n",
      "https://en.wikipedia.org/wiki/Proper_generalized_decomposition\n",
      "The proper generalized decomposition (PGD) is an iterative numerical method for solving boundary value problems (BVPs), that is, partial differential equations constrained by a set of boundary conditions, such as the Poisson's equation or the Laplace's equation.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Haplotype\n",
      "A haplotype (haploid genotype) is a group of alleles in an organism that are inherited together from a single parent.[1][2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Point-of-sale\n",
      "The point of sale (POS) or point of purchase (POP) is the time and place at which a retail transaction is completed.  At the point of sale, the merchant calculates the amount owed by the customer, indicates that amount, may prepare an invoice for the customer (which may be a cash register printout), and indicates the options for the customer to make payment.  It is also the point at which a customer makes a payment to the merchant in exchange for goods or after provision of a service.  After receiving payment, the merchant may issue a receipt for the transaction, which is usually printed but can also be dispensed with or sent electronically.[1][2][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/OPTICS_algorithm\n",
      "Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based[1] clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and Jörg Sander.[2]\n",
      "Its basic idea is similar to DBSCAN,[3] but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. To do so, the points of the database are (linearly) ordered such that spatially closest points become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster. This is represented as a dendrogram.\n",
      "\n",
      "https://en.wikipedia.org/wiki/ISSN_(identifier)\n",
      "An International Standard Serial Number (ISSN) is an eight-digit serial number used to uniquely identify a serial publication, such as a magazine.[1] The ISSN is especially helpful in distinguishing between serials with the same title. ISSNs are used in ordering, cataloging, interlibrary loans, and other practices in connection with serial literature.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Bibcode_(identifier)\n",
      "The bibcode (also known as the refcode) is a compact identifier used by several astronomical data systems to uniquely specify literature references.\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:CLIPS.jpg\n",
      "Original file ‎(3,866 × 921 pixels, file size: 328 KB, MIME type: image/jpeg)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Raytheon_Company\n",
      "The Raytheon Company was a major U.S. defense contractor and industrial corporation with manufacturing concentrations in weapons and military and commercial electronics. It was previously involved in corporate and special-mission aircraft until early 2007. Raytheon was the world's largest producer of guided missiles.[3] In April 2020, the company merged with United Technologies Corporation to form Raytheon Technologies,[4] which, since July 2023, is known as RTX Corporation.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Machine_learning\n",
      "Enter a page name to see changes on pages linked to or from that page. (To see members of a category, enter Category:Name of category). Changes to pages on your Watchlist are shown in bold with a green bullet. See more at Help:Related changes.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Peptide_sequence\n",
      "Protein primary structure is the linear sequence of amino acids in a peptide or protein.[1] By convention, the primary structure of a protein is reported starting from the amino-terminal (N) end to the carboxyl-terminal (C) end. Protein biosynthesis is most commonly performed by ribosomes in cells. Peptides can also be synthesized in the laboratory. Protein primary structures can be directly sequenced, or inferred from DNA sequences.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:Random\n",
      "Inger Wikstrom (born 11 December 1939) is a Swedish pianist, composer and conductor.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
      "Sources: Fawcett (2006),[1] Piryonesi and El-Diraby (2020),[2]\n",
      "Powers (2011),[3] Ting (2011),[4] CAWCR,[5] D. Chicco & G. Jurman (2020, 2021, 2023),[6][7][8]  Tharwat (2018).[9] Balayla (2020)[10]\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Computer_Retro.svg\n",
      "Original file ‎(SVG file, nominally 512 × 512 pixels, file size: 499 KB)\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Wikiquote-logo.svg\n",
      "Original file ‎(SVG file, nominally 300 × 355 pixels, file size: 1,012 bytes)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:MyContributions\n",
      "No changes were found matching these criteria.\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Regressions_sine_demo.svg\n",
      "Original file ‎(SVG file, nominally 900 × 450 pixels, file size: 582 KB)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Main_Page\n",
      "Florence Petty (1 December 1870 – 18 November 1948) was a Scottish social worker, cookery writer and broadcaster. During the 1900s she undertook social work in the deprived area of Somers Town in North London, demonstrating for working-class women how to cook inexpensive and nutritious foods. Much of the instruction was done in their homes. She published cookery-related works aimed at those also involved in social work, and a cookery book and pamphlet aimed at the public. From 1914 until the mid-1940s she toured Britain giving lecture-demonstrations of cost-efficient and nutritious ways to cook, including dealing with food shortages during the First World War. In the late 1920s and early 1930s, she was a BBC broadcaster on food and budgeting. Petty worked until she was in her seventies. She is considered to be a pioneer of social work innovations. Her approach to teaching the use of cheap nutritious food was a precursor to the method adopted by the Ministry of Food during the Second World War. (Full article...)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:MyTalk\n",
      "People on Wikipedia can use this talk page to post a public message about edits made from the IP address you are currently using.\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Symbol_portal_class.svg\n",
      "Original file ‎(SVG file, nominally 180 × 185 pixels, file size: 12 KB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the links and paragraphs of the articles\n",
    "for wiki in wiki_list:\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load the stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_en = stopwords.words('english')\n",
    "stop_words_ext = list(stop_en)\n",
    "vectorizer = CountVectorizer(stop_words=stop_words_ext, token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z_-]+\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine learning', 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\\n', 'Category:Machine learning', 'Machine learning is a branch of statistics and computer science which studies algorithms and architectures that learn from observed facts.\\n', 'Automated machine learning', 'Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. \\n', 'Deep learning', 'Deep learning is the subset of machine learning methods which are based on artificial neural networks with representation learning. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.[2]\\n', 'Rule-based machine learning', \"Rule-based machine learning (RBML) is a term in computer science intended to encompass any machine learning method that identifies, learns, or evolves 'rules' to store, manipulate or apply.[1][2][3] The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[clarification needed][citation needed]\\n\", 'Outline of machine learning', 'The following outline is provided as an overview of and topical guide to machine learning:\\n', 'Online machine learning', 'In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.\\n', 'Robot learning', 'Robot learning is a research field at the intersection of machine learning and robotics. It studies techniques allowing a robot to acquire novel skills or adapt to its environment through learning algorithms. The embodiment of the robot, situated in a physical embedding, provides at the same time specific difficulties (e.g. high-dimensionality, real time constraints for collecting data and learning) and opportunities for guiding the learning process (e.g. sensorimotor synergies, motor primitives).\\n', 'Machine learning control', 'Machine learning control (MLC) is a subfield of machine learning, intelligent control and control theory\\nwhich solves optimal control problems with methods of machine learning.\\nKey applications are complex nonlinear systems\\nfor which linear control theory methods are not applicable.\\n', 'Meta-learning (computer science)', 'Meta learning[1][2]\\nis a subfield of machine learning where automatic learning algorithms are applied to metadata about machine learning experiments. As of 2017, the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to learn.[1]\\n', 'Artificial intelligence in healthcare', 'Artificial intelligence in healthcare is a term used to describe the use of machine-learning algorithms and software, or artificial intelligence (AI), to copy human cognition in the analysis, presentation, and understanding of complex medical and health care data, or to exceed human capabilities by providing new ways to diagnose, treat, or prevent disease.[1][2] Specifically, AI is the ability of computer algorithms to approximate conclusions based solely on input data.\\n', 'Adversarial machine learning', 'Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks.[1] A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.[2]\\n', 'Computational learning theory', 'In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.[1]\\n', 'Progress in artificial intelligence', 'Progress in artificial intelligence (AI) refers to the advances, milestones, and breakthroughs that have been achieved in the field of artificial intelligence over time. AI is a multidisciplinary branch of computer science that aims to create machines and systems capable of performing tasks that typically require human intelligence. Artificial intelligence applications have been used in a wide range of fields including medical diagnosis, economic-financial applications, robot control, law, scientific discovery, video games, and toys. However, many AI applications are not perceived as AI:  \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"[1][2] \"Many thousands of AI applications are deeply embedded in the infrastructure of every industry.\"[3] In the late 1990s and early 21st century, AI technology became widely used as elements of larger systems,[3][4] but the field was rarely credited for these successes at the time.\\n', 'Machine Learning (journal)', 'Machine Learning  is a peer-reviewed scientific journal, published since 1986.\\n', 'Quantum machine learning', 'Quantum machine learning is the integration of quantum algorithms within machine learning programs.[1][2][3][4][5][6][7][8]\\n', 'Natural language processing', 'Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\\n', 'Statistical learning theory', 'Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis.[1][2][3] Statistical learning theory deals with the statistical inference problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics.\\n', 'Ensemble learning', 'In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.[1][2][3]\\nUnlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.\\n', 'Supervised learning', 'Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data on expected output values.[1]  An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\\n', 'Multi-task learning', 'Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately.[1][2][3] Early versions of MTL were called \"hints\".[4][5]\\n', 'Active learning (machine learning)', 'Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs.[1][2][3] In statistics literature, it is sometimes also called optimal experimental design.[4] The information source is also called teacher or oracle.\\n', 'Feature learning', 'In machine learning, feature learning or representation learning[2] is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform  a specific task.\\n', 'Machine learning in physics', 'Applying classical methods of machine learning to the study of quantum systems is the focus of an emergent area of physics research. A basic example of this is quantum state tomography, where a quantum state is learned from measurement.[1] Other examples include learning Hamiltonians,[2][3] learning quantum phase transitions,[4][5] and automatically generating new quantum experiments.[6][7][8][9] Classical machine learning is effective at processing large amounts of experimental or calculated data in order to characterize an unknown quantum system, making its application useful in contexts including quantum information theory, quantum technologies development, and computational materials design. In this context, it can be used for example as a tool to interpolate pre-calculated interatomic potentials[10] or directly solving the Schrödinger equation with a variational method.[11]\\n', 'Learning to rank', 'Learning to rank[1] or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems.[2] Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. \"relevant\" or \"not relevant\") for each item. The goal of constructing the ranking model is to rank new, unseen lists in a similar way to rankings in the training data.\\n', 'Similarity learning', 'Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.\\n', 'Artificial intelligence in industry', 'Industrial artificial intelligence, or industrial AI, usually refers to the application of artificial intelligence to industry. Unlike general artificial intelligence which is a frontier research discipline to build computerized systems that perform tasks requiring human intelligence, industrial AI is more concerned with the application of such technologies to address industrial pain-points for customer value creation, productivity improvement, cost reduction, site optimization, predictive analysis[1]  and insight discovery.[2]\\n', 'Reinforcement learning', 'Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\\n', 'Unsupervised learning', 'Unsupervised learning is a paradigm in machine learning where, in contrast to supervised learning and semi-supervised learning, algorithms learn patterns exclusively from unlabeled data.\\n', 'International Conference on Machine Learning', 'The International Conference on Machine Learning (ICML) is the leading international academic conference in machine learning. Along with NeurIPS and ICLR, it is one of the three primary conferences of high impact in machine learning and artificial intelligence research.[1] It is supported by the (IMLS). Precise dates vary year to year, but paper submissions are generally due at the end of January, and the conference is generally held the following July. The first ICML was held 1980 in Pittsburgh.[2][3]\\n', 'Machine perception', 'Machine perception is the capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them.[1][2][3] The basic method that the computers take in and respond to their environment is through the attached hardware.  Until recently input was limited to a keyboard, or a mouse, but advances in technology, both in hardware and software, have allowed computers to take in sensory input in a way similar to humans.[1][2]\\n', 'Boosting (machine learning)', 'In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance[1] in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones.[2] Boosting is based on the question posed by Kearns and Valiant (1988, 1989):[3][4] \"Can a set of weak learners create a single strong learner?\" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\\n', 'Grammar induction', 'Grammar induction (or grammatical inference)[1] is the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.\\n', 'Decision tree learning', 'Decision tree learning is a supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations.\\n', 'Multimodal learning', 'Multimodal learning, in context of machine learning, is deep learning from a combination of various modalities of data, often arising in real-world applications. An example of multi-modal data is data that combines text (typically represented as feature vector) with imaging data consisting of pixel intensities and annotation tags. As these modalities have fundamentally different statistical properties, combining them is non-trivial, which is why specialized modelling strategies and algorithms are required.\\n', 'Self-supervised learning', 'Self-supervised learning (SSL) is a paradigm in machine learning where a model is trained on a task using the data itself to generate supervisory signals, rather than relying on external labels provided by humans. In the context of neural networks, self-supervised learning aims to leverage inherent structures or relationships within the input data to create meaningful training signals. SSL tasks are designed so that solving it requires capturing essential features or relationships in the data. The input data is typically augmented or transformed in a way that creates pairs of related samples. One sample serves as the input, and the other is used to formulate the supervisory signal. This augmentation can involve introducing noise, cropping, rotation, or other transformations. Self-supervised learning more closely imitates the way humans learn to classify objects.[1]\\n', 'Machine ethics', 'Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence, otherwise known as artificial intelligent agents.[1] Machine ethics differs from other ethical fields related to engineering and technology.  Machine ethics should not be confused with computer ethics, which focuses on human use of computers. It should also be distinguished from the philosophy of technology, which concerns itself with the grander social effects of technology.[2]\\n', 'Predictive analytics', 'Predictive analytics is a form of business analytics applying machine learning to generate a predictive model for certain business applications. As such, it encompasses a variety of statistical techniques from predictive modeling and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events.[1] It represents a major subset of machine learning applications; in some contexts, it is synonymous with machine learning.[2]\\n', 'Explainable artificial intelligence', 'Explainable AI (XAI), often overlapping with Interpretable AI, or Explainable Machine Learning (XML), either refers to an AI system over which it is possible for humans to retain intellectual oversight, or to the methods to achieve this.[1] The main focus is usually on the reasoning behind the decisions or predictions made by the AI[2] which are made more understandable and transparent.[3] XAI counters the \"black box\" tendency of machine learning, where even the AI\\'s designers cannot explain why it arrived at a specific decision.[4][5]\\n', 'Neural machine translation', 'Neural machine translation (NMT) is an approach to machine translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modeling and then translating entire sentences in a single integrated model.\\n', 'Learning classifier system', 'Learning classifier systems, or LCS, are a paradigm of rule-based machine learning methods that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning).[2]  Learning classifier systems seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions (e.g. behavior modeling,[3] classification,[4][5] data mining,[5][6][7] regression,[8] function approximation,[9] or game strategy).  This approach allows complex solution spaces to be broken up into smaller, simpler parts.\\n', 'Fairness (machine learning)', \"Fairness in machine learning refers to the various attempts at correcting algorithmic bias in automated decision processes based on machine learning models. Decisions made by computers after a machine-learning process may be considered unfair if they were based on variables considered sensitive. Examples of these kinds of variable include gender, ethnicity, sexual orientation, disability and more. As it is the case with many ethical concepts, definitions of fairness and bias are always controversial. In general, fairness and bias are considered relevant when the decision process impacts people's lives. In machine learning, the problem of algorithmic bias is well known and well studied. Outcomes may be skewed by a range of factors and thus might be considered unfair with respect to certain groups or individuals. An example would be the way social media sites deliver personalized news to consumers.\\n\", 'Statistical learning in language acquisition', 'Statistical learning is the ability for humans and other animals to extract statistical regularities from the world around them to learn about the environment. Although statistical learning is now thought to be a generalized learning mechanism, the phenomenon was first identified in human infant language acquisition.\\n', 'Kernel method', 'In machine learning, kernel machines are a class of algorithms for pattern analysis, whose best known member is the support-vector machine (SVM). These methods involve using linear classifiers to solve nonlinear problems.[1] The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over all pairs of data points computed using inner products. The feature map in kernel machines is infinite dimensional but only requires a finite dimensional matrix from user-input according to the Representer theorem. Kernel machines are slow to compute for datasets larger than a couple of thousand examples without parallel processing. \\n', 'Data mining', 'Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1]\\n', 'Generalization (learning)', 'Generalization is the concept that humans, other animals, and artificial neural networks use past learning in present situations of learning if the conditions in the situations are regarded as similar.[1] The learner uses generalized patterns, principles, and other similarities between past experiences and novel experiences to more efficiently navigate the world.[2] For example, if a person has learned in the past that every time they eat an apple, their throat becomes itchy and swollen, they might assume they are allergic to all fruit. When this person is offered a banana to eat, they reject it upon assuming they are also allergic to it through generalizing that all fruits cause the same reaction. Although this generalization about being allergic to all fruit based on experiences with one fruit could be correct in some cases, it may not be correct in all. Both positive and negative effects have been shown in education through learned generalization and its contrasting notion of discrimination learning.\\n', 'Ensemble averaging (machine learning)', 'In machine learning, particularly in the creation of artificial neural networks, ensemble averaging is the process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models \"average out.\"\\n', 'Support vector machine', 'In machine learning, support vector machines (SVMs, also support vector networks[1]) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995,[1] Vapnik et al., 1997[citation needed]) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\\n', 'Federated learning', 'Federated learning (also known as collaborative learning) is a machine learning technique that trains an algorithm via multiple independent sessions, each using its own dataset. This approach stands in contrast to traditional centralized machine learning techniques where local datasets are merged into one training session, as well as to approaches that assume that local data samples are identically distributed.\\n', 'ML.NET', 'ML.NET is a free software machine learning library for the C# and F# programming languages.[4][5][6] It also supports Python models when used together with NimbusML. The preview release of ML.NET included transforms for feature engineering like n-gram creation, and learners to handle binary classification, multi-class classification, and regression tasks.[7] Additional ML tasks like anomaly detection and recommendation systems have since been added, and other approaches like deep learning will be included in future versions.[8][9]\\n', 'Apprenticeship learning', 'In artificial intelligence, apprenticeship learning (or learning from demonstration or imitation learning) is the process of learning by observing an expert.[1][2] It can be viewed as a form of supervised learning, where the training dataset consists of task executions by a demonstration teacher.[2]\\n', 'Linear classifier', \"In the field of machine learning, the goal of statistical classification is to use an object's characteristics to identify which class (or group) it belongs to.  A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics.  An object's characteristics are also known as feature values and are typically presented to the machine in a vector called a feature vector. Such classifiers work well for practical problems such as document classification, and more generally for problems with many variables (features), reaching accuracy levels comparable to non-linear classifiers while taking less time to train and use.[1]\\n\", 'Computational linguistics', 'Computational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon linguistics, computer science, artificial intelligence, mathematics, logic, philosophy, cognitive science, cognitive psychology, psycholinguistics, anthropology and neuroscience, among others.\\n', 'Pages that link to \"Machine learning\"', 'The following pages link to Machine learning \\n', 'Automated decision-making', 'Automated decision-making (ADM) involves the use of data, machines and algorithms to make decisions in a range of contexts, including public administration, business, health, education, law, employment, transport, media and entertainment, with varying degrees of human oversight or intervention. ADM involves large-scale data from a range of sources, such as databases, text, social media, sensors, images or speech, that is processed using various technologies including computer software, algorithms, machine learning, natural language processing, artificial intelligence, augmented intelligence and robotics. The increasing use of automated decision-making systems (ADMS) across a range of contexts presents many benefits and challenges to human society requiring consideration of the technical, legal, ethical, societal, educational, economic and health consequences.[1][2][3]\\n', 'Large language model', 'A large language model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by using massive amounts of data to learn billions of parameters during training and consuming large computational resources during their training and operation.[1] LLMs are artificial neural networks (mainly transformers[2]) and are (pre-)trained using self-supervised learning and semi-supervised learning.\\n', 'Nature Machine Intelligence', 'Nature Machine Intelligence is a monthly peer-reviewed scientific journal published by Nature Portfolio covering machine learning and artificial intelligence. The editor-in-chief is Liesbeth Venema.[1]\\n', 'Automated planning and scheduling', 'Automated planning and scheduling, sometimes denoted as simply AI planning,[1] is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory.\\n', 'Artificial immune system', \"In artificial intelligence, artificial immune systems (AIS) are a class of computationally intelligent, rule-based machine learning systems inspired by the principles and processes of the vertebrate immune system. The algorithms are typically modeled after the immune system's characteristics of learning and memory for use in problem-solving.\\n\", 'Generative model', 'In statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent,[a] but three major types can be distinguished, following Jebara (2004):\\n', 'scikit-learn', 'scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language.[3]\\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.[4]\\n', 'Outline of artificial intelligence', 'The following outline is provided as an overview of and topical guide to artificial intelligence:\\n', 'Natural-language understanding', 'Natural-language understanding (NLU) or natural-language interpretation (NLI)[1] is a subtopic  of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem.[2]\\n', 'Journal of Machine Learning Research', 'The Journal of Machine Learning Research is a peer-reviewed open access scientific journal covering machine learning. It was established in 2000 and the first editor-in-chief was Leslie Kaelbling.[1] The current editors-in-chief are Francis Bach (Inria) and David Blei (Columbia University).\\n', 'Feature (machine learning)', 'In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon.[1] Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of \"feature\" is related to that of explanatory variable used in statistical techniques such as linear regression.\\n', 'Mila (research institute)', 'Mila - Quebec AI Institute (originally Montreal Institute for Learning Algorithms) is a research institute in Montreal, Quebec, focusing mainly on machine learning research. Approximately 1000 students and researchers and 100 faculty members, were part of Mila in 2022.[1] Mila is part of the Pan-Canadian AI Strategy. [2]\\n', 'Template talk:Machine learning', 'This section title and contents seem pretty much random to me. How are contents chosen? One regression, one random clustering algorithm, 4 standard classificators; but no decision tree; which is probably the grandfather of all classificators. --Chire (talk) 12:41, 22 October 2013 (UTC)Reply[reply]\\n', 'Multilayer perceptron', 'A multilayer perceptron (MLP) is a misnomer for a modern feedforward artificial neural network, consisting of fully connected neurons with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not linearly separable.[1] It is a misnomer because the original perceptron used a Heaviside step function, instead of a nonlinear kind of activation function (used by modern networks).\\n', 'General game playing', 'General game playing (GGP) is the design of artificial intelligence programs to be able to play more than one game successfully.[1][2][3] For many games like chess, computers are programmed to play these games using a specially designed algorithm, which cannot be transferred to another context. For instance, a chess-playing computer program cannot play checkers. General game playing is considered as a necessary milestone on the way to artificial general intelligence.[4]\\n', 'Semantic analysis (machine learning)', 'In machine learning, semantic analysis of a corpus is the task of building structures that approximate concepts from a large set of documents. It generally does not involve prior semantic understanding of the documents. A metalanguage based on predicate logic can analyze the speech of humans.[1]:\\u200a93-\\u200a Another strategy to understand the semantics of a text is symbol grounding. If language is grounded, it is equal to recognizing a machine readable meaning. For the restricted domain of spatial analysis, a computer based language understanding system was demonstrated.[2]:\\u200a123\\u200a\\n', 'The Master Algorithm', 'The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.\\n', 'Neural Turing machine', 'A neural Turing machine (NTM) is a recurrent neural network model of a Turing machine. The approach was published by Alex Graves et al. in 2014.[1] NTMs combine the fuzzy pattern matching capabilities of neural networks with the algorithmic power of programmable computers. \\n', 'Structured prediction', 'Structured prediction or structured (output) learning is an umbrella term for supervised machine learning techniques that involves predicting structured objects, rather than scalar discrete or real values.[1]\\n', 'Attention (machine learning)', 'Machine learning-based attention is a mechanism mimicking cognitive attention. It calculates \"soft\" weights for each word, more precisely for its embedding, in the context window. It can do it either in parallel (such as in transformers) or sequentially (such as recurrent neural networks). \"Soft\" weights can change during each runtime, in contrast to \"hard\" weights, which are (pre-)trained and fine-tuned and remain frozen afterwards.  \\n', 'LightGBM', 'LightGBM, short for light gradient-boosting machine, is a free and open-source distributed gradient-boosting framework for machine learning, originally developed by Microsoft.[4][5] It is based on decision tree algorithms and used for ranking, classification and other machine learning tasks. The development focus is on performance and scalability.\\n', 'Symbolic artificial intelligence', 'In artificial intelligence, symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search.[1] Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems),  symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to seminal ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems.\\n', 'Sentiment analysis', '\\nSentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. With the rise of deep language models, such as RoBERTa, also more difficult data domains can be analyzed, e.g., news texts where authors typically express their opinion/sentiment less explicitly.[1]\\n', 'Ontology learning', \"Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process.\\n\", 'Computer vision', 'Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions.[1][2][3][4] Understanding in this context means the transformation of visual images (the input to the retina in the human analog) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\\n', 'Expert system', 'In artificial intelligence, an expert system is a computer system emulating the decision-making ability of a human expert.[1]\\nExpert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural code.[2] The first expert systems were created in the 1970s and then proliferated in the 1980s.[3] Expert systems were among the first truly successful forms of artificial intelligence (AI) software.[4][5][6][7][8] \\nAn expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.\\n', 'Computing Machinery and Intelligence', '\\n\\n\"Computing Machinery and Intelligence\" is a seminal paper written by Alan Turing on the topic of artificial intelligence. The paper, published in 1950 in Mind, was the first to introduce his concept of what is now known as the Turing test to the general public.\\n', 'Affective computing', \"Affective computing is the study and development of systems and devices that can recognize, interpret, process, and simulate human affects. It is an interdisciplinary field spanning computer science, psychology, and cognitive science.[1] While some core ideas in the field may be traced as far back as to early philosophical inquiries into emotion,[2] the more modern branch of computer science originated with Rosalind Picard's 1995 paper[3] on affective computing and her book Affective Computing[4] published by MIT Press.[5][6] One of the motivations for the research is the ability to give machines emotional intelligence, including to simulate empathy. The machine should interpret the emotional state of humans and adapt its behavior to them, giving an appropriate response to those emotions.\\n\", 'Backpropagation', 'As a machine-learning algorithm, backpropagation is a crucial step in a common method used to iteratively train a neural network model. It is used to calculate the necessary parameter adjustments, to gradually minimize error.\\n', 'AI safety', 'AI safety is an interdisciplinary field concerned with preventing accidents, misuse, or other harmful consequences that could result from artificial intelligence (AI) systems. It encompasses machine ethics and AI alignment, which aim to make AI systems moral and beneficial, and AI safety encompasses technical problems including monitoring systems for risks and making them highly reliable. Beyond AI research, it involves developing norms and policies that promote safety.\\n', 'Neural Designer', 'Neural Designer is a software tool for machine learning based on neural networks, a main area of artificial intelligence research, and contains a graphical user interface which simplifies data entry and interpretation of results.\\n', 'Learning curve (machine learning)', \"In machine learning, a learning curve (or training curve) plots the optimal value of a model's loss function for a training set against this loss function evaluated on a validation data set with same parameters as produced the optimal function.[1] Synonyms include error curve, experience curve, improvement curve and generalization curve.[2]\\n\", 'Probabilistic classification', 'In machine learning, a probabilistic classifier is a classifier that is able to predict, given an observation of an input, a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to. Probabilistic classifiers provide classification that can be useful in its own right[1] or when combining classifiers into ensembles.\\n', 'Artificial intelligence in government', 'Artificial intelligence (AI) has a range of uses in government. It can be used to further public policy objectives (in areas such as emergency services, health and welfare), as well as assist the public to interact with the government  (through the use of virtual assistants, for example). According to the Harvard Business Review, \"Applications of artificial intelligence to the public sector are broad and growing, with early experiments taking place around the world.\"[1] Hila Mehr from the Ash Center for Democratic Governance and Innovation at Harvard University notes that AI in government is not new, with postal services using machine methods in the late 1990s to recognise handwriting on envelopes to automatically route letters.[2] The use of AI in government comes with significant benefits, including efficiencies resulting in cost savings (for instance by reducing the number of front office staff), and reducing the opportunities for corruption.[3] However, it also carries risks.[citation needed][further explanation needed]\\n', 'Editing Template:Machine learning', 'Copy and paste: – — ° ′ ″ ≈ ≠ ≤ ≥ ± − × ÷ ← → · § \\xa0 Sign your posts on talk pages: ~~~~ \\xa0 Cite your sources: <ref></ref> \\n', 'Weak supervision', 'Weak supervision is a paradigm in machine learning, the relevance and notability of which increased with the advent of large language models due to large amount of data required to train them.  It is characterized by using a combination of a small amount of human-labeled data (exclusively used in more expensive and time-consuming supervised learning paradigm), followed by a large amount of unlabeled data (used exclusively in unsupervised learning paradigm). In other words, the desired output values are provided only for a subset of the training data. The remaining data is unlabeled or imprecisely labeled. Intuitively, it can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam. Technically, it could be viewed as performing clustering and then labeling the clusters with the labeled data, pushing the decision boundary away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.\\n', 'Template:Artificial intelligence', 'This template shows topics in the area of artificial intelligence.\\n', 'Perceptron', 'In machine learning, the perceptron (or McCulloch-Pitts neuron) is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.[1]  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.\\n', 'Philosophy of artificial intelligence', 'The philosophy of artificial intelligence is a branch of the philosophy of mind and the philosophy of computer science[1] that explores artificial intelligence and its implications for knowledge and understanding of intelligence, ethics, consciousness, epistemology, and free will.[2][3] Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; see artificial life) so the discipline is of considerable interest to philosophers.[4] These factors contributed to the emergence of the philosophy of artificial intelligence. \\n', 'Manifold regularization', 'In machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is smooth: data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition.\\n', 'Modeling language', 'A modeling language is any artificial language that can be used to express data, information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure Programing language.\\n', 'Outline of computer science', 'Computer science (also called computing science) is the study of the theoretical foundations of information and computation and their implementation and application in computer systems. One well known subject classification system for computer science is the ACM Computing Classification System devised by the Association for Computing Machinery.\\n', 'Regularization (mathematics)', 'In mathematics, statistics, finance,[1] computer science, particularly in machine learning and inverse problems, regularization is a process that changes the result answer to be \"simpler\". It is often used to obtain results for ill-posed problems or to prevent overfitting.[2]\\n', 'Meta AI', \"Meta AI is an artificial intelligence laboratory that belongs to Meta Platforms Inc. (formerly known as Facebook, Inc.)[1] Meta AI intends to develop various forms of artificial intelligence, improving augmented and artificial reality technologies.[2] Meta AI is an academic research laboratory focused on generating knowledge for the AI community.[3] This is in contrast to Facebook's Applied Machine Learning (AML) team, which focuses on practical applications of its products.[3]\\n\", 'Graph neural network', 'A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs.[1][2][3][4][5]\\n', 'Association rule learning', 'Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.[1] In any given transaction with a variety of items, association rules are meant to discover the rules that determine how or why certain items are connected.\\n', 'AI takeover', 'An AI takeover is a hypothetical scenario in which artificial intelligence (AI) becomes the dominant form of intelligence on Earth, as computer programs or robots effectively take control of the planet away from the human species. Possible scenarios include replacement of the entire human workforce, takeover by a superintelligent AI, and the popular notion of a robot uprising. Stories of AI takeovers are very popular throughout science fiction. Some public figures, such as Stephen Hawking and Elon Musk, have advocated research into precautionary measures to ensure future superintelligent machines remain under human control.[1]\\n', 'Word2vec', 'Word2vec is a technique for natural language processing (NLP) published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that they capture the semantic and syntactic qualities of words; as such, a simple mathematical function (cosine similarity) can indicate the level of semantic similarity between the words represented by those vectors.\\n', 'Inductive bias', 'The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered.[1]\\nInductive bias is anything which makes the algorithm learn one pattern instead of another pattern (e.g. step-functions in decision trees instead of continuous function in a linear regression model).\\n', 'Hyperparameter optimization', 'In machine learning, hyperparameter optimization[1] or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\\n', 'Arthur Samuel (computer scientist)', 'Arthur Lee Samuel (December 5, 1901 – July 29, 1990)[3] was an American pioneer in the field of computer gaming and artificial intelligence.[2] He popularized the term \"machine learning\" in 1959.[4] The Samuel Checkers-playing Program was among the world\\'s first successful self-learning programs, and as such a very early demonstration of the fundamental concept of artificial intelligence (AI).[5] He was also a senior member in the TeX community who devoted much time giving personal attention to the needs of users and wrote an early TeX manual in 1983.[6]\\n', 'Knowledge representation and reasoning', 'Knowledge representation and reasoning (KRR, KR&R, KR²) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can use to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. Knowledge representation incorporates findings from psychology[1] about how humans solve problems and represent knowledge in order to design formalisms that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from logic to automate various kinds of reasoning, such as the application of rules or the relations of sets and subsets.\\n', 'Relevance vector machine', 'In mathematics, a Relevance Vector Machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification.[1]\\nThe RVM has an identical functional form to the support vector machine, but provides probabilistic classification.\\n', 'Mallet (software project)', 'MALLET is a Java \"Machine Learning for Language Toolkit\".\\n', 'Ethics of artificial intelligence', 'The ethics of artificial intelligence is the branch of the ethics of technology specific to artificially intelligent systems.[1] It is sometimes divided into a concern with the moral behavior of humans as they design, make, use and treat artificially intelligent systems, and a concern with the behavior of machines, in machine ethics. \\n', 'Differentiable neural computer', 'In artificial intelligence, a differentiable neural computer (DNC) is a memory augmented neural network architecture (MANN), which is typically (but not by definition) recurrent in its implementation. The model was published in 2016 by Alex Graves et al. of DeepMind.[1]\\n', 'Artificial Intelligence: A Modern Approach', 'Artificial Intelligence: A Modern Approach (AIMA) is a university textbook on artificial intelligence, written by Stuart J. Russell and Peter Norvig. It was first published in 1995 and the fourth edition of the book was released on 28 April 2020.[1] It is used in over 1400 universities worldwide[2] and has been called \"the most popular artificial intelligence textbook in the world\".[3] It is considered the standard text in the field of artificial intelligence.[4][5]\\nThe book is intended for an undergraduate audience but can also be used for graduate-level studies with the suggestion of adding some of the primary sources listed in the extensive bibliography.  Programs in the book are presented in pseudo code with implementations in Java, Python, Lisp, JavaScript and Scala available online.[6][7] There are also unsupported implementations in Prolog, C++, C#, and several other languages.\\n', 'Semantic Scholar', 'Semantic Scholar is a research tool powered by artificial intelligence for scientific literature. It was developed at the Allen Institute for AI and publicly released in November 2015.[2] It uses advances in natural language processing to provide summaries for scholarly papers.[3] The Semantic Scholar team is actively researching the use of artificial intelligence in natural language processing, machine learning, human–computer interaction, and information retrieval.[4]\\n', 'Pattern recognition', 'Pattern recognition is the automated recognition of patterns and regularities in data. While similar, pattern recognition (PR) is not to be confused with pattern machines (PM) which may possess (PR) capabilities but their primary function is to distinguish and create emergent  pattern.   PR has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power.\\n', 'Feature engineering', 'Feature engineering or feature extraction  or feature discovery is the process of extracting features (characteristics, properties, attributes) from raw data.[1] This can be done with deep learning networks such as convolutional neural networks that are able to learn features by themselves.[citation needed]\\n', 'Differentiable programming', 'Differentiable programming is a programming paradigm in which a numeric computer program can be differentiated throughout via automatic differentiation.[1][2][3][4][5] This allows for gradient-based optimization of parameters in the program, often via gradient descent, as well as other learning approaches that are based on higher order derivative information. Differentiable programming has found use in a wide variety of areas, particularly scientific computing and artificial intelligence.[5] One of the early proposals to adopt such a framework in a systematic fashion to improve upon learning algorithms was made by the Advanced Concepts Team at the European Space Agency in early 2016.[6]\\n', 'Computational mathematics', '\\nComputational mathematics is an area of mathematics devoted to the interaction between mathematics and computer computation.[1]\\n', 'Music and artificial intelligence', 'Artificial intelligence and music (AIM) is a common subject in the International Computer Music Conference, the Computing Society Conference[1] and the International Joint Conference on Artificial Intelligence. The first International Computer Music Conference (ICMC) was held in 1974 at Michigan State University.[2] Current research includes the application of AI in music composition, performance, theory and digital sound processing.\\n', 'Deep learning speech synthesis', 'Deep learning speech synthesis uses Deep Neural Networks (DNN) to produce\\nartificial speech from text (text-to-speech) or spectrum (vocoder).\\nThe deep neural networks are trained using a large amount of recorded speech and, in the case of\\na text-to-speech system, the associated labels and/or input text.\\n', 'Speech recognition', '\\nSpeech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.\\n', 'Hybrid intelligent system', '\\nHybrid intelligent system denotes a software system which employs, in parallel, a combination of methods and techniques from artificial intelligence subfields, such as:\\n', 'Transformer (machine-learning model)', 'A transformer is a deep learning architecture, initially proposed in 2017, that relies on the parallel multi-head attention mechanism.[1] It is notable for requiring less training time than previous recurrent neural architectures, such as long short-term memory (LSTM),[2] and its later variation has been prevalently adopted for training large language models on large (language) datasets, such as the Wikipedia corpus and Common Crawl, by virtue of the parallelized processing of input sequence.[3]\\nInput text is split into n-grams encoded as tokens and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished. Though the transformer paper was published in 2017, the softmax-based attention mechanism was proposed in 2014 for machine translation,[4][5] and the Fast Weight Controller, similar to a transformer, was proposed in 1992.[6][7][8]\\n', 'Computational engineering', 'Computational Engineering is an emerging discipline that deals with the development and application of computational models for engineering, known as Computational Engineering Models[1] or CEM. Computational engineering uses computers to solve engineering design problems important to a variety of industries.[2] At this time, various different approaches are summarized under the term Computational Engineering, including using computational geometry and virtual design for engineering tasks,[3][4] often coupled with a simulation-driven approach[5] In Computational Engineering, algorithms solve mathematical and logical models[6] that describe engineering challenges, sometimes coupled with some aspect of AI, specifically Reinforcement Learning.[7]\\n', 'Inductive programming', 'Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.\\n', 'Data augmentation', 'Data augmentation is a technique in machine learning used to reduce overfitting when training a machine learning model,[1] by training models on several slightly-modified copies of existing data.\\n', 'Recurrent neural network', 'A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs[1][2][3] makes them applicable to tasks such as unsegmented, connected handwriting recognition[4] or speech recognition.[5][6] The term \"recurrent neural network\" is used to refer to the class of networks with an infinite impulse response, whereas \"convolutional neural network\" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior.[7] A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.\\n', 'mlpack', 'mlpack is a machine learning software library for C++, built on top of the Armadillo library and the ensmallen numerical optimization library.[3] mlpack has an emphasis on scalability, speed, and ease-of-use. Its aim is to make machine learning possible for novice users by means of a simple, consistent API, while simultaneously exploiting C++ language features to provide maximum performance and maximum flexibility for expert users.[4] Its intended target users are scientists and engineers.\\n', 'Situated approach (artificial intelligence)', 'In artificial intelligence research, the situated approach builds agents that are designed to behave effectively successfully in their environment. This requires designing AI \"from the bottom-up\" by focussing on the basic perceptual and motor skills required to survive. The situated approach gives a much lower priority to abstract reasoning or problem-solving skills.\\n', 'MIT Computer Science and Artificial Intelligence Laboratory', 'Computer Science and Artificial Intelligence Laboratory (CSAIL) is a research institute at the Massachusetts Institute of Technology (MIT) formed by the 2003 merger of the Laboratory for Computer Science (LCS) and the Artificial Intelligence Laboratory (AI Lab). Housed within the Ray and Maria Stata Center, CSAIL is the largest on-campus laboratory as measured by research scope and membership. It is part of the Schwarzman College of Computing[1] but is also overseen by the MIT Vice President of Research.[2]\\n', 'Seq2seq', 'Seq2seq is a family of machine learning approaches used for natural language processing.[1] Applications include language translation, image captioning, conversational models, and text summarization.[2]\\nSeq2seq uses sequence transformation: it turns one sequence into another sequence.\\n', 'Torch (machine learning)', 'Torch is an open-source machine learning library, \\na scientific computing framework, and a scripting language based on Lua.[3] It provides LuaJIT interfaces to deep learning algorithms implemented in C. It was created at IDIAP at EPFL. Torch development moved in 2017 to PyTorch, a port of the library to Python.[4][5][better\\xa0source\\xa0needed]\\n', 'Mathematical optimization', 'Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criterion, from some set of available alternatives.[1] It is generally divided into two subfields: discrete optimization and continuous optimization. Optimization problems arise in all quantitative disciplines from computer science and engineering[2] to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.[3]\\n', 'Convolutional neural network', 'Convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters (or kernel) optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections.[1][2] For example, for each neuron in the fully-connected layer 10,000 weights would be required for processing an image sized 100 × 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels,[3][4]  only 25 neurons are required to process 5x5-sized tiles.[5][6] Higher-layer features are extracted  from wider context windows, compared to lower-layer features.\\n', 'Wolfram Mathematica', 'Wolfram Mathematica is a software system with built-in libraries for several areas of technical computing that allow machine learning, statistics, symbolic computation, data manipulation, network analysis, time series analysis, NLP, optimization, plotting functions and various types of data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other programming languages. It was conceived by Stephen Wolfram, and is developed by Wolfram Research of Champaign, Illinois.[8][9] The Wolfram Language is the programming language used in Mathematica.[10] Mathematica 1.0 was released on June 23, 1988 in Champaign, Illinois and Santa Clara, California.[11][12][13]\\n', 'Programming language theory', 'Programming language theory (PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of formal languages known as programming languages. Programming language theory is closely related to other fields including mathematics, software engineering, and linguistics. There are a number of academic conferences and journals in the area.\\n', 'Approximate computing', 'Approximate computing is an emerging paradigm for energy-efficient and/or high-performance design.[1] It includes a plethora of computation techniques that return a possibly inaccurate result rather than a guaranteed accurate result, and that can be used for applications where an approximate result is sufficient for its purpose.[2] One example of such situation is for a search engine where no exact answer may exist for a certain search query and hence, many answers may be acceptable. Similarly, occasional dropping of some frames in a video application can go undetected due to perceptual limitations of humans. Approximate computing is based on the observation that in many scenarios, although performing exact computation requires large amount of resources, allowing bounded approximation can provide disproportionate gains in performance and energy, while still achieving acceptable result accuracy.[clarification needed]  For example, in k-means clustering algorithm, allowing only 5% loss in classification accuracy can provide 50 times energy saving compared to the fully accurate classification.\\n', 'Theoretical computer science', 'Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, formal language theory, the lambda calculus and type theory.\\n', 'Yooreeka', 'Yooreeka is a library for data mining, machine learning, soft computing, and mathematical analysis. The project started with the code of the book \"Algorithms of the Intelligent Web\".[1] Although the term \"Web\" prevailed in the title, in essence, the algorithms are valuable in any software application.\\n', 'IBM Watson', \"IBM Watson is a computer system capable of answering questions posed in natural language.[1] It was developed in IBM's DeepQA project by a research team led by principal investigator David Ferrucci.[2] Watson was named after IBM's founder and first CEO, industrialist Thomas J. Watson.[3][4]\\n\", 'Probably approximately correct learning', 'In computational learning theory, probably approximately correct (PAC) learning is a framework for mathematical analysis of machine learning. It was proposed in 1984 by Leslie Valiant.[1]\\n', 'Search algorithm', 'In computer science, a search algorithm is an algorithm designed to solve a search problem. Search algorithms work to retrieve information stored within particular data structure, or calculated in the search space of a problem domain, with either discrete or continuous values.\\n', 'Multi-agent reinforcement learning', 'Multi-agent reinforcement learning (MARL) is a sub-field of reinforcement learning. It focuses on studying the behavior of multiple learning agents that coexist in a shared environment.[1] Each agent is motivated by its own rewards, and does actions to advance its own interests; in some environments these interests are opposed to the interests of other agents, resulting in complex group dynamics.\\n', 'Category:Learning', 'Learning is the process of acquiring new or modifying existing knowledge, behaviors, skills, values, or preferences based on instruction.\\n', 'Category:Artificial neural networks', 'This category are for articles about artificial neural networks (ANN).\\n', 'Regression analysis', \"In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome' or 'response' variable, or a 'label' in machine learning parlance) and one or more independent variables (often called 'predictors', 'covariates', 'explanatory variables' or 'features'). The most common form of regression analysis is linear regression, in which one finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared differences between the true data and that line (or hyperplane). For specific mathematical reasons (see linear regression), this allows the researcher to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values. Less common forms of regression use slightly different procedures to estimate alternative location parameters (e.g., quantile regression or Necessary Condition Analysis[1]) or estimate the conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression).\\n\", 'Computational statistics', 'Computational statistics, or statistical computing, is the bond between statistics and computer science, and refers to the statistical methods that are enabled by using computational methods. It is the area of computational science (or scientific computing) specific to the mathematical science of statistics. This area is also developing rapidly, leading to calls that a broader concept of computing should be taught as part of general statistical education.[1]\\n', 'Portal:Computer programming', 'Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks. It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages. Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit. Proficient programming usually requires expertise in several different subjects, including knowledge of the application domain, details of programming languages and generic code libraries, specialized algorithms, and formal logic.\\n', 'Feedforward neural network', 'A feedforward neural network (FNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers.[2] Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops,[2] in contrast to recurrent neural networks,[3] which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method[4][5][6][7][8] and are colloquially referred to as the \"vanilla\" neural networks.[9]\\n', 'Knowledge graph embedding', \"In representation learning, knowledge graph embedding (KGE), also referred to as knowledge representation learning (KRL), or multi-relation learning,[1] is a machine learning task of learning a low-dimensional representation of a knowledge graph's entities and relations while preserving their semantic meaning.[1][2][3]  Leveraging their embedded representation, knowledge graphs (KGs) can be used for various applications such as link prediction, triple classification, entity recognition, clustering, and relation extraction.[1][4]\\n\", 'Distributed artificial intelligence', 'Distributed Artificial Intelligence (DAI) also called Decentralized Artificial Intelligence[1] is a subfield of artificial intelligence research dedicated to the development of distributed solutions for problems. DAI is closely related to and a predecessor of the field of multi-agent systems. \\n', 'Predictive modelling', 'Predictive modelling uses statistics to predict outcomes.[1] Most often the event one wants to predict is in the future, but predictive modelling can be applied to any type of unknown event, regardless of when it occurred. For example, predictive models are often used to detect crimes and identify suspects, after the crime has taken place.[2]\\n', 'Occam learning', 'In computational learning theory, Occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to probably approximately correct (PAC) learning, where the learner is evaluated on its predictive power of a test set.\\n', 'Weka (software)', 'Waikato Environment for Knowledge Analysis (Weka) is a collection of machine learning and data analysis free software licensed under the GNU General Public License. It was developed at the University of Waikato, New Zealand and is the companion software to the book \"Data Mining: Practical Machine Learning Tools and Techniques\".[1]\\n', 'Theory of computation', 'In theoretical computer science and mathematics, the theory of computation is the branch that deals with what problems can be solved on a model of computation, using an algorithm, how efficiently they can be solved or to what degree (e.g., approximate solutions versus precise ones).  The field is divided into three major branches: automata theory and formal languages, computability theory, and computational complexity theory, which are linked by the question: \"What are the fundamental capabilities and limitations of computers?\".[1]\\n', 'Evolutionary algorithm', 'In computational intelligence (CI), an evolutionary algorithm (EA) is a subset of evolutionary computation,[1] a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.\\n', 'Computational biology', 'Computational biology refers to the use of data analysis, mathematical modeling and computational simulations to understand biological systems and relationships.[1] An intersection of computer science, biology, and big data, the field also has foundations in applied mathematics, chemistry, and genetics.[2] It differs from biological computing, a subfield of computer science and engineering which uses bioengineering to build computers.\\n', 'Knowledge extraction', 'Knowledge extraction is the creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criterion is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data.\\n', 'Long short-term memory', 'Long short-term memory (LSTM)[1] network is a recurrent neural network (RNN), aimed to deal with the vanishing gradient problem[2] present in traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus \"long short-term memory\".[1] It is applicable to classification, processing and predicting data based on time series, such as in handwriting,[3] speech recognition,[4][5] machine translation,[6][7] speech activity detection,[8] robot control,[9][10] video games,[11][12] and healthcare.[13]\\n', 'NeuroSolutions', 'NeuroSolutions is a neural network development environment developed by NeuroDimension. It combines a modular, icon-based (component-based) network design interface with an implementation of advanced learning procedures, such as conjugate gradients, the Levenberg-Marquardt algorithm, and backpropagation through time.[citation needed] The software is used to design, train and deploy neural network (supervised learning and unsupervised learning) models to perform a wide variety of tasks such as data mining, classification, function approximation, multivariate regression and time-series prediction.[citation needed]\\n', 'Kubeflow', 'Kubeflow is an open-source platform for machine learning and MLOps on Kubernetes introduced by Google. The different stages in a typical machine learning lifecycle are represented with different software components in Kubeflow, including model development (Kubeflow Notebooks[4]), model training (Kubeflow Pipelines,[5] Kubeflow Training Operator[6]), model serving (KServe[a][7]), and automated machine learning (Katib[8]).\\n', 'AAAI Conference on Artificial Intelligence', 'The AAAI Conference on Artificial Intelligence (AAAI) is one of the leading international academic conference in artificial intelligence held annually.[1][2][3] Along with ICML, NeurIPS and ICLR, it is one of the primary conferences of high impact in machine learning and artificial intelligence research.[4] It is supported by the Association for the Advancement of Artificial Intelligence. Precise dates vary from year to year, but paper submissions are generally due at the end of August to beginning of September, and the conference is generally held during the following February. The first AAAI was held in 1980 at Stanford University, Stanford California.[5]\\n', 'Generalized linear model', 'In statistics, a generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\\n', 'Connectionism', \"Connectionism (coined by Edward Thorndike in the 1930s) is the name of an approach to the study of human mental processes and cognition that utilizes mathematical models known as connectionist networks or artificial neural networks.[1] Connectionism has had many 'waves' since its beginnings.\\n\", 'ECML PKDD', 'ECML PKDD, the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, is one of the leading[1][2] academic conferences on machine learning and knowledge discovery, held in Europe every year.\\n', 'Google JAX', \"Google JAX is a machine learning framework for transforming numerical functions.[1][2][3] It is described as bringing together a modified version of autograd (automatic obtaining of the gradient function through differentiation of a function) and TensorFlow's XLA (Accelerated Linear Algebra). It is designed to follow the structure and workflow of NumPy as closely as possible and works with various existing frameworks such as TensorFlow and PyTorch.[4][5] The primary functions of JAX are:[1]\\n\", 'Robot control', 'Robotic control is the system that contributes to the movement of robots. This involves the mechanical aspects and programmable systems that makes it possible to control robots. Robotics can be controlled by various means including manual, wireless, semi-autonomous (a mix of fully automatic and wireless control), and fully autonomous (using artificial intelligence).\\n', 'Computational science', '\\nComputational science, also known as scientific computing, technical computing or scientific computation (SC), is a division of science that uses advanced computing capabilities to understand and solve complex physical problems. This includes \\n', 'Mathematical software', 'Mathematical software is software used to model, analyze or calculate numeric, symbolic or geometric data.[1]\\n', 'Astroinformatics', 'Astroinformatics is an interdisciplinary field of study involving the combination of astronomy, data science, machine learning, informatics, and information/communications technologies.[2][3] The field is closely related to astrostatistics.\\n', 'Diffusion model', 'In machine learning, diffusion models, also known as diffusion probabilistic models or score-based generative models, are a class of generative models. The goal of diffusion models is to learn a diffusion process that generates the probability distribution of a given dataset. It mainly consists of three major components: the forward process, the reverse process, and the sampling procedure.[1] Three examples of generic diffusion modeling frameworks used in computer vision are denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations.[2]\\n', 'Naive Bayes classifier', 'In statistics, naive Bayes classifiers are a family of linear \"probabilistic classifiers\" based on applying Bayes\\' theorem with strong (naive) independence assumptions between the features (see Bayes classifier). They are among the simplest Bayesian network models,[1] but coupled with kernel density estimation, they can achieve high accuracy levels.[2]\\n', 'Model of computation', 'In computer science, and more specifically in computability theory and computational complexity theory, a model of computation is a model which describes how an output of a mathematical function is computed given an input. A model describes how units of computations, memories, and communications are organized.[1] The computational complexity of an algorithm can be measured given a model of computation. Using a model allows studying the performance of algorithms independently of the variations that are specific to particular implementations and specific technology.\\n', 'Topic model', 'In statistics and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear approximately equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document\\'s balance of topics is.\\n', 'OpenNN', 'OpenNN (Open Neural Networks Library) is a software library written in the C++ programming language which implements neural networks, a main area of deep learning research.[1] The library is open-source, licensed under the GNU Lesser General Public License.\\n', 'Computational complexity theory', '\\nIn theoretical computer science and mathematics, computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.\\n', 'k-nearest neighbors algorithm', 'In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951,[1] and later expanded by Thomas Cover.[2] It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression:\\n', 'Microsoft Cognitive Toolkit', 'Microsoft Cognitive Toolkit,[3] previously known as CNTK and sometimes styled as The Microsoft Cognitive Toolkit, is a deprecated[4] deep learning framework developed by Microsoft Research. Microsoft Cognitive Toolkit describes neural networks as a series of computational steps via a directed graph.\\n', 'Computer graphics', 'Computer graphics deals with generating images and art with the aid of computers. Today, computer graphics is a core technology in digital photography, film, video games, digital art, cell phone and computer displays, and many specialized applications. A great deal of specialized hardware and software has been developed, with the displays of most devices being driven by computer graphics hardware. It is a vast and recently developed area of computer science. The phrase was coined in 1960 by computer graphics researchers Verne Hudson and William Fetter of Boeing. It is often abbreviated as CG, or typically in the context of film as computer generated imagery (CGI). The non-artistic aspects of computer graphics are the subject of computer science research.[1]\\n', 'LLaMA', 'LLaMA (Large Language Model Meta AI) is a family of large language models (LLMs), released by Meta AI starting in February 2023. \\n', 'Human-in-the-loop', 'Human-in-the-loop  or HITL is used in multiple contexts. It can be defined as a model requiring human interaction.[1][2] HITL is associated with modeling and simulation (M&S) in the live, virtual, and constructive taxonomy. HITL along with the related human-on-the-loop are also used in relation to lethal autonomous weapons.[3] Further, HITL is used in the context of machine learning.[4]\\n', 'MuZero', \"MuZero is a computer program developed by artificial intelligence research company DeepMind to master games without knowing their rules.[1][2][3] Its release in 2019 included benchmarks of its performance in go, chess, shogi, and a standard suite of Atari games. The algorithm uses an approach similar to AlphaZero. It matched AlphaZero's performance in chess and shogi, improved on its performance in Go (setting a new world record), and improved on the state of the art in mastering a suite of 57 Atari games (the Arcade Learning Environment), a visually-complex domain.\\n\", 'Cluster analysis', 'Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.\\n', 'AI winter', '\\nIn the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1]  The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.\\n', 'Reservoir computing', 'Reservoir computing is a framework for computation derived from recurrent neural network theory that maps input signals into higher dimensional computational spaces through the dynamics of a fixed, non-linear system called a reservoir.[1] After the input signal is fed into the reservoir, which is treated as a \"black box,\" a simple readout mechanism is trained to read the state of the reservoir and map it to the desired output.[1] The first key benefit of this framework is that training is performed only at the readout stage, as the reservoir dynamics are fixed.[1] The second is that the computational power of naturally available systems, both classical and quantum mechanical, can be used to reduce the effective computational cost.[2]\\n', 'Computational geometry', 'Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with a history stretching back to antiquity.\\n', 'Apache Mahout', 'Apache Mahout is a project of the Apache Software Foundation to produce free implementations of distributed or otherwise scalable machine learning algorithms focused primarily on linear algebra. In the past, many of the implementations use the Apache Hadoop platform, however today it is primarily focused on Apache Spark.[3][4] Mahout also provides Java/Scala libraries for common math operations (focused on linear algebra and statistics) and primitive Java collections. Mahout is a work in progress; a number of algorithms have been implemented.[5]\\n', 'Graphics processing unit', 'A graphics processing unit (GPU) is a specialized electronic circuit initially designed to accelerate computer graphics and image processing (either on a video card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles). After their initial design, GPUs were found to be useful for non-graphic calculations involving embarrassingly parallel problems due to their parallel structure. Other non-graphical uses include the training of neural networks and cryptocurrency mining.\\n', 'List of datasets in computer vision and image processing', 'This is a list of datasets for machine learning research. It is part of the list of datasets for machine-learning research. These datasets consist primarily of images or videos for tasks such as object detection, facial recognition, and multi-label classification.\\n', 'Computational social science', 'Computational social science is the academic sub-discipline concerned with computational approaches to the social sciences. This means that computers are used to model, simulate, and analyze social phenomena.  Fields include computational economics, computational sociology, cliodynamics, culturomics, nonprofit studies,[1] and the automated analysis of contents, in social and traditional media. It focuses on investigating social and behavioral relationships and interactions through social simulation, modeling, network analysis, and media analysis.[2]\\n', 'Computational anatomy', 'Computational anatomy is an interdisciplinary field of biology focused on quantitative investigation and modelling of anatomical shapes variability.[1][2] It involves the development and application of mathematical, statistical and data-analytical methods for modelling and simulation of biological structures.\\n', 'Multilinear subspace learning', 'Multilinear subspace learning is an approach for disentangling the causal factor of data formation and performing  dimensionality reduction.[1][2][3][4][5]   \\nThe Dimensionality reduction can be performed on a data tensor that contains a collection of observations have been vectorized,[1] or observations that are treated as matrices and concatenated into a data tensor.[6][7]  Here are some examples of data tensors whose observations are vectorized  or whose observations are matrices concatenated into data tensor images (2D/3D), video sequences (3D/4D), and hyperspectral cubes (3D/4D).\\n', 'Deeplearning4j', 'Eclipse Deeplearning4j is a programming library written in Java for the Java virtual machine (JVM).[2][3] It is a framework with wide support for deep learning algorithms.[4] Deeplearning4j includes implementations of the restricted Boltzmann machine, deep belief net, deep autoencoder, stacked denoising autoencoder and recursive neural tensor network, word2vec, doc2vec, and GloVe. These algorithms all include distributed parallel versions that integrate with Apache Hadoop and Spark.[5]\\n', 'Platt scaling', \"In machine learning, Platt scaling or Platt calibration is a way of transforming the outputs of a classification model into a probability distribution over classes. The method was invented by John Platt in the context of support vector machines,[1]\\nreplacing an earlier method by Vapnik,\\nbut can be applied to other classification models.[2]\\nPlatt scaling works by fitting a logistic regression model to a classifier's scores.\\n\", 'Loss functions for classification', 'In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to).[1]  Given \\n\\n\\n\\n\\n\\nX\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\mathcal {X}}}\\n\\n as the space of all possible inputs (usually \\n\\n\\n\\n\\n\\nX\\n\\n\\n⊂\\n\\n\\nR\\n\\n\\nd\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\mathcal {X}}\\\\subset \\\\mathbb {R} ^{d}}\\n\\n), and \\n\\n\\n\\n\\n\\nY\\n\\n\\n=\\n{\\n−\\n1\\n,\\n1\\n}\\n\\n\\n{\\\\displaystyle {\\\\mathcal {Y}}=\\\\{-1,1\\\\}}\\n\\n as the set of labels (possible outputs), a typical goal of classification algorithms is to find a function \\n\\n\\n\\nf\\n:\\n\\n\\nX\\n\\n\\n→\\n\\n\\nY\\n\\n\\n\\n\\n{\\\\displaystyle f:{\\\\mathcal {X}}\\\\to {\\\\mathcal {Y}}}\\n\\n which best predicts a label \\n\\n\\n\\ny\\n\\n\\n{\\\\displaystyle y}\\n\\n for a given input \\n\\n\\n\\n\\n\\n\\nx\\n→\\n\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\vec {x}}}\\n\\n.[2]  However, because of incomplete information, noise in the measurement, or probabilistic components in the underlying process, it is possible for the same \\n\\n\\n\\n\\n\\n\\nx\\n→\\n\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\vec {x}}}\\n\\n to generate different \\n\\n\\n\\ny\\n\\n\\n{\\\\displaystyle y}\\n\\n.[3]  As a result, the goal of the learning problem is to minimize expected loss (also known as the risk), defined as\\n', 'Whisper (speech recognition system)', 'Whisper is a machine learning model for speech recognition and transcription, created by OpenAI and first released as open-source software in September 2022.[2]\\n', 'Bootstrap aggregating', 'Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.\\n', 'Algorithmic efficiency', '\\nIn computer science, algorithmic efficiency is a property of an algorithm which relates to the amount of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on the usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.\\n', 'Statistical model', 'A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process.[1] When referring specifically to probabilities, the corresponding term is probabilistic model.\\n', 'Apache MXNet', 'Apache MXNet is an open-source deep learning software framework that trains and deploys deep neural networks. It is scalable, allows fast model training, and supports a flexible programming model and multiple programming languages (including C++, Python, Java, Julia, MATLAB, JavaScript, Go, R, Scala, Perl, and Wolfram Language). The MXNet library is portable and can scale to multiple GPUs[2] and machines. It was co-developed by Carlos Guestrin at the University of Washington (along with GraphLab).[3]\\n', 'Email filtering', 'Email filtering is the processing of email to organize it according to specified criteria. The term can apply to the intervention of human intelligence, but most often refers to the automatic processing of messages at an SMTP server, possibly applying anti-spam techniques. Filtering can be applied to incoming emails as well as to outgoing ones.\\n', 'Neural Computation (journal)', 'Neural Computation is a monthly peer-reviewed scientific journal covering all aspects of neural computation, including modeling the brain and the design and construction of neurally-inspired information processing systems. It was established in 1989 and is published by MIT Press. The editor-in-chief is Terrence J. Sejnowski (Salk Institute for Biological Studies).[1]\\n', 'Linear regression', 'In statistics, linear regression is a linear approach for modelling a predictive relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables), which are measured without error. The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression.[1] This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.[2]If the explanatory variables are measured with error then errors-in-variables models are required, also known as measurement error models.\\n', 'International Conference on Learning Representations', 'The International Conference on Learning Representations (ICLR) is a machine learning conference typically held in late April or early May each year. The conference includes invited talks as well as oral and poster presentations of refereed papers. Since its inception in 2013, ICLR has employed an open peer review process to referee paper submissions (based on models proposed by Yann LeCun[1]). In 2019, there were 1591 paper submissions, of which 500 accepted with poster presentations (31%) and 24 with oral presentations (1.5%).[2]. In 2021, there were 2997 paper submissions, of which 860 were accepted (29%).[3].\\n', 'Social computing', 'Social computing is an area of computer science that is concerned with the intersection of social behavior and computational systems. It is based on creating or recreating social conventions and social contexts through the use of software and technology. Thus, blogs, email, instant messaging, social network services, wikis, social bookmarking and other instances of what is often called social software illustrate ideas from social computing.   \\n', 'Automatic differentiation', 'In mathematics and computer algebra, automatic differentiation (auto-differentiation, autodiff, or AD), also called algorithmic differentiation, computational differentiation,[1][2] is a set of techniques to evaluate the partial derivative of a function specified by a computer program.\\n', 'Prompt engineering', 'Prompt engineering is the process of structuring text that can be interpreted and understood by a generative AI model.[1][2] A prompt is natural language text describing the task that an AI should perform.[3]\\n', 'Inductive logic programming', 'Inductive logic programming (ILP) is a subfield of symbolic artificial intelligence  which uses logic programming as a uniform representation for examples, background knowledge and hypotheses.  The term \"inductive\" here refers to philosophical (i.e. suggesting a theory to explain observed facts) rather than mathematical (i.e. proving a property for all members of a well-ordered set) induction. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesised logic program which entails all the positive and none of the negative examples.\\n', 'Manifold hypothesis', 'The manifold hypothesis posits that many high-dimensional data sets that occur in the real world actually lie along low-dimensional latent manifolds inside that high-dimensional space.[1][2][3][4] As a consequence of the manifold hypothesis, many data sets that appear to initially require many variables to describe, can actually be described by a comparatively small number of variables, likened to the local coordinate system of the underlying manifold. It is suggested that this principle underpins the effectiveness of machine learning algorithms in describing high-dimensional data sets by considering a few common features.\\n', 'Semantics (computer science)', 'In programming language theory, semantics is the rigorous mathematical study of the meaning of programming languages.[1] Semantics assigns computational meaning to valid strings in a programming language syntax. It is closely related to, and often crosses over with, the semantics of mathematical proofs.\\n', 'Nonlinear dimensionality reduction', 'Nonlinear dimensionality reduction, also known as manifold learning, refers to various related techniques that aim to project high-dimensional data onto lower-dimensional latent manifolds, with the goal of either visualizing the data in the low-dimensional space, or learning the mapping (either from the high-dimensional space to the low-dimensional embedding or vice versa) itself.[1][2] The techniques described below can be understood as generalizations of linear decomposition methods used for dimensionality reduction, such as singular value decomposition and principal component analysis.\\n', 'Parallel computing', 'Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously.[1] Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing, but has gained broader interest due to the physical constraints preventing frequency scaling.[2] As power consumption (and consequently heat generation) by computers has become a concern in recent years,[3] parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.[4]\\n', 'Infer.NET', 'Infer.NET is a free and open source .NET software library for machine learning.[2] It supports running Bayesian inference in graphical models and can also be used for probabilistic programming.[3]\\n', 'Toronto Declaration', 'The Toronto Declaration: Protecting the Rights to Equality and Non-Discrimination in Machine Learning Systems is a declaration that advocates responsible practices for machine learning practitioners and governing bodies. It is a joint statement issued by groups including Amnesty International and Access Now, with other notable signatories including Human Rights Watch and The Wikimedia Foundation.[1] It was published at RightsCon on May 16, 2018.[2][3]\\n', 'Logic programming', 'Logic programming is a programming, database and knowledge-representation and reasoning paradigm which is based on formal logic. A program, database or knowledge base in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain.  Major logic programming language families include Prolog, Answer Set Programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:\\n', 'Q-learning', 'Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. It does not require a model of the environment (hence \"model-free\"), and it can handle problems with stochastic transitions and rewards without requiring adaptations [1].\\n', 'Self-play', 'Self-play is a technique for improving the performance of reinforcement learning agents. Intuitively, agents learn to improve their performance by playing \"against themselves\".\\n', 'Binary classification', 'Binary classification is the task of classifying the elements of a set into two groups (each called class) on the basis of a classification rule. Typical binary classification problems include:\\n', 'Logic in computer science', 'Logic in computer science covers the overlap between the field of logic and that of computer science. The topic can essentially be divided into three main areas:\\n', 'Random forest', \"\\nRandom forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned.[1][2] Random decision forests correct for decision trees' habit of overfitting to their training set.[3]:\\u200a587–588\\u200a\\n\", 'PyTorch', 'PyTorch is a machine learning framework based on the Torch library,[4][5][6] used for applications such as computer vision and natural language processing,[7] originally developed by Meta AI and now part of the Linux Foundation umbrella.[8][9][10][11] It is free and open-source software released under the modified BSD license. Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface.[12]\\n', 'Swarm intelligence', 'Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems.[1]\\n', 'Computational economics', 'Computational economics is an interdisciplinary research discipline that involves computer science, economics, and management science.[1]  This subject encompasses computational modeling of economic systems. Some of these areas are unique, while others established areas of economics by allowing robust data analytics and solutions of problems that would be arduous to research without computers and associated numerical methods.[2]\\n', 'AlexNet', \"AlexNet is the name of a convolutional neural network (CNN) architecture, designed by Alex Krizhevsky in collaboration with Ilya Sutskever and Geoffrey Hinton, who was Krizhevsky's Ph.D. advisor at the University of Toronto.[1][2]\\n\", 'Domain-specific language', 'A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains. There are a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software, such as MUSH soft code. DSLs can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. Special-purpose computer languages have always existed in the computer age, but the term \"domain-specific language\" has become more popular due to the rise of domain-specific modeling. Simpler DSLs, particularly ones used by a single application, are sometimes informally called mini-languages.\\n', 'DeepDream', 'DeepDream is a computer vision program created by Google engineer Alexander Mordvintsev that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dream-like appearance reminiscent of a psychedelic experience in the deliberately overprocessed images.[1][2][3]\\n', 'Alex Graves (computer scientist)', 'Alex Graves is a computer scientist. Before working as a research scientist at DeepMind, he earned a BSc in Theoretical Physics from the University of Edinburgh and a PhD in artificial intelligence under Jürgen Schmidhuber at IDSIA.[1] He was also a postdoc under Schmidhuber at the Technical University of Munich and under Geoffrey Hinton[2] at the University of Toronto.\\n', 'Automated theorem proving', 'Automated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs. Automated reasoning over mathematical proof was a major impetus for the development of computer science.\\n', 'Corinna Cortes', 'Corinna Cortes (born 31 March, 1961) is a Danish computer scientist known for her contributions to machine learning. She is a Vice President at Google Research in New York City.[3] Cortes is an ACM Fellow and a recipient of the Paris Kanellakis Award for her work on theoretical foundations of support vector machines.[4][5][3][6]\\n', 'Formal methods', 'In computer science, formal methods are mathematically rigorous techniques for the specification, development, analysis, and verification of software and hardware systems.[1] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.[2]\\n', 'GPT-1', 'Generative Pre-trained Transformer 1 (GPT-1) was the first of OpenAI\\'s large language models following Google\\'s invention of the transformer architecture in 2017.[2] In June 2018, OpenAI released a paper entitled \"Improving Language Understanding by Generative Pre-Training\",[3] in which they introduced that initial model along with the general concept of a generative pre-trained transformer.[4]\\n', 'Heuristic (computer science)', 'In mathematical optimization and computer science, heuristic (from Greek εὑρίσκω \"I find, discover\") is a technique designed for problem solving more quickly when classic methods are too slow for finding an exact or approximate solution, or when classic methods fail to find any exact solution in a search space.  This is achieved by trading optimality, completeness, accuracy, or precision for speed.  In a way, it can be considered a shortcut.\\n', 'Bias–variance tradeoff', \"In statistics and machine learning, the bias–variance tradeoff describes the relationship between a model's complexity, the accuracy of its predictions, and how well it can make predictions on previously unseen data that were not used to train the model. In general, as we increase the number of tunable parameters in a model, it becomes more flexible, and can better fit a training data set. It is said to have lower error, or bias. However, for more flexible models, there will tend to be greater variance to the model fit each time we take a set of samples to create a new training data set. It is said that there is greater variance in the model's estimated parameters.\\n\", 'Robot locomotion', 'Robot locomotion is the collective name for the various methods that robots use to transport themselves from place to place.\\n', 'Cheminformatics', 'Cheminformatics (also known as chemoinformatics) refers to the use of physical chemistry theory with computer and information science techniques—so called \"in silico\" techniques—in application to a range of descriptive and prescriptive problems in the field of chemistry, including in its applications to biology and related molecular fields. Such in silico techniques are used, for example, by pharmaceutical companies and in academic settings to aid and inform the process of drug discovery, for instance in the design of well-defined combinatorial libraries of synthetic compounds, or to assist in structure-based drug design. The methods can also be used in chemical and allied industries, and such fields as environmental science and pharmacology, where chemical processes are involved or studied.[1]\\n', 'Graphical model', 'A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statistics—particularly Bayesian statistics—and machine learning.\\n', 'Statistical inference', 'Statistical inference is the process of using data analysis to infer properties of an underlying distribution of probability.[1] Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population.\\n', 'Computer animation', \"Computer animation is the process used for digitally generating animations. The more general term computer-generated imagery (CGI) encompasses both static scenes (still images) and dynamic images (moving images), while computer animation only refers to moving images. Modern computer animation usually uses 3D computer graphics. The animation's target is sometimes the computer itself, while other times it is film.\\n\", 'Dartmouth workshop', 'The Dartmouth Summer Research Project on Artificial Intelligence was a 1956 summer workshop widely considered[1][2][3] to be the founding event of artificial intelligence as a field.\\n', 'Human–computer interaction', 'Human–computer interaction (HCI) is research in the design and the use of computer technology, which focuses on the interfaces between people (users) and computers. HCI researchers observe the ways humans interact with computers and design technologies that allow humans to interact with computers in novel ways. A device that allows interaction between human being and a computer is known as a \"Human-computer Interface (HCI)\".\\n', 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'IEEE Transactions on Pattern Analysis and Machine Intelligence (sometimes abbreviated as IEEE PAMI or simply PAMI) is a monthly peer-reviewed scientific journal published by the IEEE Computer Society. \\n', 'Artificial neuron', \"An artificial neuron is a mathematical function conceived as a model of biological neurons in a neural network. Artificial neurons are the elementary units of artificial neural networks.[1] The artificial neuron receives one or more inputs (representing excitatory postsynaptic potentials and inhibitory postsynaptic potentials at neural dendrites) and sums them to produce an output (or activation, representing a neuron's action potential which is transmitted along its axon). Usually, each input is separately weighted (representing the synaptic weight), and the sum is often added to a term known as a bias (loosely corresponding to the threshold potential), before being passed through a non-linear function known as an activation function or transfer function[clarification needed]. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often monotonically increasing, continuous, differentiable and bounded. Non-monotonic, unbounded and oscillating activation functions with multiple zeros that outperform sigmoidal and ReLU-like activation functions on many tasks have also been recently explored. The thresholding function has inspired building logic gates referred to as threshold logic; applicable to building logic circuits resembling brain processing. For example, new devices such as memristors have been extensively used to develop such logic in recent times.[2]\\n\", 'Statistical classification', 'In statistics, classification is the problem of identifying which of a set of categories (sub-populations) an observation (or observations) belongs to.  Examples are assigning a given email to the \"spam\" or \"non-spam\" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).\\n', 'BERT (language model)', '\\nBidirectional Encoder Representations from Transformers (BERT) is a family of language models introduced in October 2018 by researchers at Google.[1][2] A 2020 literature survey concluded that \"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\"[3]\\n', 'Apache SystemDS', 'Apache SystemDS (Previously, Apache SystemML) is an open source ML system for the end-to-end data science lifecycle. \\n', 'Vapnik–Chervonenkis theory', 'Vapnik–Chervonenkis theory (also known as VC theory) was developed during 1960–1990 by Vladimir Vapnik and Alexey Chervonenkis. The theory is a form of computational learning theory, which attempts to explain the learning process from a statistical point of view.\\n', 'Programming paradigm', 'Programming paradigms are a way to classify programming languages based on their features. Languages can be classified into multiple paradigms.\\n', 'Computational physics', 'Computational physics is the study and implementation of numerical analysis to solve problems in physics.[1] Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science. It is sometimes regarded as a subdiscipline (or offshoot) of theoretical physics, but others consider it an intermediate branch between theoretical and experimental physics — an area of study which supplements both theory and experiment.[2]\\n', 'ADALINE', 'ADALINE (Adaptive Linear Neuron or later Adaptive Linear Element) is an early single-layer artificial neural network and the name of the physical device that implemented this network.[1][2][3][4][5] The network uses memistors. It was developed by professor Bernard Widrow and his doctoral student Ted Hoff at Stanford University in 1960. It is based on the perceptron. It consists of a weight, a bias and a summation function.\\n', 'Conference on Neural Information Processing Systems', 'The Conference and Workshop on Neural Information Processing Systems (abbreviated as NeurIPS and formerly NIPS)  is a machine learning and computational neuroscience conference held every December. The conference is currently a double-track meeting (single-track until 2015) that includes invited talks as well as oral and poster presentations of refereed papers, followed by parallel-track workshops that up to 2013 were held at ski resorts.\\n', 'Dynamic programming', 'Dynamic programming is both a mathematical optimization method and an algorithmic paradigm. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics.\\n', 'Interpreter (computing)', 'In computer science, an interpreter is a computer program that directly executes instructions written in a programming or scripting language, without requiring them previously to have been compiled into a machine language program. An interpreter generally uses one of the following strategies for program execution:\\n', 'Empirical risk minimization', 'Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an algorithm will work in practice (the true \"risk\") because we don\\'t know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the \"empirical\" risk).\\n', 'Analysis of algorithms', \"In computer science, the analysis of algorithms is the process of finding the computational complexity of algorithms—the amount of time, storage, or other resources needed to execute them. Usually, this involves determining a function that relates the size of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity). An algorithm is said to be efficient when this function's values are small, or grow slowly compared to a growth in the size of the input. Different inputs of the same size may cause the algorithm to have different behavior, so best, worst and average case descriptions might all be of practical interest.  When not otherwise specified, the function describing the performance of an algorithm is usually an upper bound, determined from the worst case inputs to the algorithm.\\n\", 'Overfitting', 'In mathematical modeling, overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably\".[1] An overfitted model is a mathematical model that contains more parameters than can be justified by the data.[2] In a mathematical sense, these parameters represent the degree of a polynomial. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.[3]:\\u200a45\\u200a\\n', 'Self-organizing map', 'A self-organizing map (SOM) or self-organizing feature map (SOFM) is an unsupervised machine learning technique used to produce a low-dimensional (typically two-dimensional) representation of a higher dimensional data set while preserving the topological structure of the data. For example, a data set with \\n\\n\\n\\np\\n\\n\\n{\\\\displaystyle p}\\n\\n variables measured in \\n\\n\\n\\nn\\n\\n\\n{\\\\displaystyle n}\\n\\n observations could be represented as clusters of observations with similar values for the variables. These clusters then could be visualized as a two-dimensional \"map\" such that observations in proximal clusters have more similar values than observations in distal clusters. This can make high-dimensional data easier to visualize and analyze.\\n', 'Sequential pattern mining', 'Sequential pattern mining is a topic of data mining concerned with finding statistically relevant patterns between data examples where the values are delivered in a sequence.[1][2] It is usually presumed that the values are discrete, and thus time series mining is closely related, but usually considered a different activity.  Sequential pattern mining is a special case of structured data mining.\\n', 'Rendering (computer graphics)', 'Rendering or image synthesis is the process of generating a photorealistic or non-photorealistic image from a 2D or 3D model by means of a computer program.[citation needed]  The resulting image is referred to as the render.  Multiple models can be defined in a scene file containing objects in a strictly defined language or data structure.  The scene file contains geometry, viewpoint, texture, lighting, and shading information describing the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file. The term \"rendering\" is analogous to the concept of an artist\\'s impression of a scene.  The term \"rendering\" is also used to describe the process of calculating effects in a video editing program to produce the final video output.\\n', 'Shogun (toolbox)', 'Shogun is a free, open-source machine learning software library  written in C++. It offers numerous algorithms and data structures for machine learning problems. It offers interfaces for Octave, Python, R, Java, Lua, Ruby and C# using SWIG.\\n', 'Probabilistic logic', 'Probabilistic logic (also probability logic and probabilistic reasoning) involves the use of probability and logic to deal with uncertain situations. Probabilistic logic extends traditional logic truth tables with probabilistic expressions. A difficulty of probabilistic logics is their tendency to multiply the computational complexities of their probabilistic and logical components.  Other difficulties include the possibility of counter-intuitive results, such as in case of belief fusion in Dempster–Shafer theory. Source trust and epistemic uncertainty about the probabilities they provide, such as defined in subjective logic, are additional elements to consider. The need to deal with a broad variety of contexts and issues has led to many different proposals.\\n', 'Template talk:Artificial intelligence', \"The current pic, which I have added to the template, is at top, and the previous one is at bottom. I do not think the old one was very good; it is an illustration of the contours of a human brain with a random circuit board overlaid on it. What circuit board? We don't know. It looks like there is supposed to be a pad for a CPU in the middle... and there is part of a ball grid array or something there... but there is also a gigantic randomly-shaped splotch of copper there, what is that for? I am confident that this is not an actual PCB, nor is it a plausible design for one, and I object to illustrating articles about artificial intelligence with a ridiculously fake image.\\n\", 'Kernel regression', 'In statistics, kernel regression is a non-parametric technique to estimate the conditional expectation of a random variable. The objective is to find a non-linear relation between a pair of random variables X and Y.\\n', 'Chinchilla AI', 'Chinchilla is a family of large language models developed by the research team at DeepMind, presented in March of 2022.[1] It is named \"chinchilla\" because it is a further development over a previous model family named Gopher. Both model families were trained in order to investigate the scaling laws of large language models.[2]\\n', 'Neural coding', 'Neural coding (or neural representation) is a neuroscience field concerned with characterising the hypothetical relationship between the stimulus and the individual or ensemble neuronal responses and the relationship among the electrical activity of the neurons in the ensemble.[1][2] Based on the theory that\\nsensory and other information is represented in the brain by networks of neurons, it is thought that neurons can encode both digital and analog information.[3]\\n', 'International Joint Conference on Artificial Intelligence', 'The International Joint Conference on Artificial Intelligence (IJCAI) is the leading conference in the field of artificial intelligence. The conference series has been organized by the nonprofit IJCAI Organization since 1969, making it the oldest premier AI conference series in the world.[1] It was held biennially in odd-numbered years from 1969 to 2015 and annually starting from 2016. More recently, IJCAI was held jointly every four years with ECAI since 2018 and PRICAI since 2020 to promote collaboration of AI researchers and practitioners. IJCAI covers a broad range of research areas in the field of AI. It is a large and highly selective conference, with only about 20% or less of the submitted papers accepted after peer review in the 5 years leading up to 2022.[2] A lower acceptance rate usually means better quality papers and a higher reputation conference.\\n', 'Programming team', 'A programming team is a team of people who develop or maintain computer software.[1]  They may be organised in numerous ways, but the egoless programming team and chief programmer team have been common structures.[2]\\n', 'Statistics', 'Statistics (from German: Statistik, orig. \"description of a state, a country\")[1][2] is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data.[3][4][5] In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.[6]\\n', 'PaLM', 'PaLM (Pathways Language Model) is a 540 billion parameter transformer-based large language model developed by Google AI.[1] Researchers also trained smaller versions of PaLM, 8 and 62 billion parameter models, to test the effects of model scale.[2]\\n', 'AlphaFold', 'AlphaFold is an artificial intelligence (AI) program developed by DeepMind, a subsidiary of Alphabet, which performs predictions of protein structure.[1] The program is designed as a deep learning system.[2]\\n', 'Temporal difference learning', 'Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods.[1]\\n', 'Markov decision process', \"In mathematics, a Markov decision process (MDP) is a discrete-time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming. MDPs were known at least as early as the 1950s;[1] a core body of research on Markov decision processes resulted from Ronald Howard's 1960 book, Dynamic Programming and Markov Processes.[2] They are used in many disciplines, including robotics, automatic control, economics and manufacturing. The name of MDPs comes from the Russian mathematician Andrey Markov as they are an extension of Markov chains.\\n\", 'Spiking neural network', 'Spiking neural networks (SNNs) are artificial neural networks that more closely mimic natural neural networks.[1] In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather transmit information only when a membrane potential—an intrinsic quality of the neuron related to its membrane electrical charge—reaches a specific value, called the threshold. When the membrane potential reaches the threshold, the neuron fires, and generates a signal that travels to other neurons which, in turn, increase or decrease their potentials in response to this signal. A neuron model  that fires at the moment of threshold crossing is also called a spiking neuron model.[2]\\n', 'Massive Online Analysis', 'Massive Online Analysis (MOA) is a free open-source software project specific for data stream mining with concept drift. It is written in Java and developed at the University of Waikato, New Zealand.[2]\\n', 'Behaviorism', \"\\nBehaviorism (also spelled behaviourism)[1] is a systematic approach to understanding the behavior of humans and other animals.[2] It assumes that behavior is either a reflex evoked by the pairing of certain antecedent stimuli in the environment, or a consequence of that individual's history, including especially reinforcement and punishment contingencies, together with the individual's current motivational state and controlling stimuli. Although behaviorists generally accept the important role of heredity in determining behavior, they focus primarily on environmental events.\\n\", 'Hugging Face', 'Hugging Face, Inc. is a French-American company that develops tools for building applications using machine learning, based in New York City. It is most notable for its transformers library built for natural language processing applications and its platform that allows users to share machine learning models and datasets and showcase their work.\\n', 'File:Artificial intelligence prompt completion by dalle mini.jpg', 'Original file \\u200e(1,024 × 1,024 pixels, file size: 211 KB, MIME type: image/jpeg)\\n', 'Action selection', 'Action selection is a way of characterizing the most basic problem of intelligent systems: what to do next. In artificial intelligence and computational cognitive science, \"the action selection problem\" is typically associated with intelligent agents and animats—artificial systems that exhibit complex behaviour in an agent environment. The term is also sometimes used in ethology or animal behavior.\\n', 'Gradient descent', 'Gradient descent (also often called steepest descent) is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for finding a local minimum of a differentiable multivariate function\\n', 'Map (mathematics)', 'In mathematics, a map or mapping is a function in its general sense.[1]  These terms may have originated as from the process of making a geographical map: mapping the Earth surface to a sheet of paper.[2]\\n', 'Keras', 'Keras is an open-source library that provides a Python interface for artificial neural networks. Keras acts as an interface for the TensorFlow library.[citation needed]\\n', 'Nils John Nilsson', 'Nils John Nilsson (February 6, 1933 – April 23, 2019) was an American computer scientist. He was one of the founding researchers in the discipline of artificial intelligence.[2] He was the first Kumagai Professor of Engineering in computer science at Stanford University from 1991 until his retirement. He is particularly known for his contributions to search, planning, knowledge representation, and robotics.[2]\\n', 'Software engineering', 'Software engineering is an engineering-based approach to software development.[1][2][3]\\nA software engineer is a person who applies the engineering design process to design, develop, test, maintain, and evaluate computer software. The term programmer is sometimes used as a synonym, but may emphasize software implementation over design and can also lack connotations of engineering education or skills.[4]\\n', 'Stochastic gradient descent', 'Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.[1]\\n', 'Conditional random field', 'Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighbouring\" samples, a CRF can take context into account. To do so, the predictions are modelled as a graphical model, which represents the presence of dependencies between the predictions. What kind of graph is used depends on the application. For example, in natural language processing, \"linear chain\" CRFs are popular, for which each prediction is dependent only on its immediate neighbours. In image processing, the graph typically connects locations to nearby and/or similar locations to enforce that they receive similar predictions.\\n', 'Tensor Processing Unit', \"Tensor Processing Unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google for neural network machine learning, using Google's own TensorFlow software.[1] Google began using TPUs internally in 2015, and in 2018 made them available for third party use, both as part of its cloud infrastructure and by offering a smaller version of the chip for sale.\\n\", 'Cross-validation (statistics)', \"Cross-validation,[2][3][4] sometimes called rotation estimation[5][6][7] or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.\\nCross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.  In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set).[8][9] The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias[10] and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).\\n\", 'Programming tool', 'A programming tool or software development tool is a computer program that software developers use to create, debug, maintain, or otherwise support other programs and applications. The term usually refers to relatively simple programs, that can be combined to accomplish a task, much as one might use multiple hands to fix a physical object. The most basic tools are a source code editor and a compiler or interpreter, which are used ubiquitously and continuously. Other tools are used more or less depending on the language, development methodology, and individual engineer, often used for a discrete task, like a debugger or profiler. Tools may be discrete programs, executed separately – often from the command line – or may be parts of a single large program, called an integrated development environment (IDE). In many cases, particularly for simpler use, simple ad hoc techniques are used instead of a tool, such as print debugging instead of using a debugger, manual timing (of overall program or section of code) instead of a profiler, or tracking bugs in a  text file or spreadsheet instead of a bug tracking system.\\n', 'Restricted Boltzmann machine', 'A restricted Boltzmann machine (RBM) (also called a restricted Sherrington–Kirkpatrick model with external field or restricted stochastic Ising–Lenz–Little model) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.[1]\\n', 'Functional programming', 'In computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions. It is a declarative programming paradigm in which function definitions are trees of expressions that map values to other values, rather than a sequence of imperative statements which update the running state of the program.\\n', 'WaveNet', \"WaveNet is a deep neural network for generating raw audio. It was created by researchers at London-based AI firm DeepMind. The technique, outlined in a paper in September 2016,[1] is able to generate relatively realistic-sounding human-like voices by directly modelling waveforms using a neural network method trained with recordings of real speech. Tests with US English and Mandarin reportedly showed that the system outperforms Google's best existing text-to-speech (TTS) systems, although as of 2016 its text-to-speech synthesis still was less convincing than actual human speech.[2] WaveNet's ability to generate raw waveforms means that it can model any kind of audio, including music.[3]\\n\", 'Computability theory', '\\nComputability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability. In these areas, computability theory overlaps with proof theory and effective descriptive set theory.\\n', 'Distributed computing', 'A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another.[1][2] Distributed computing is a field of computer science that studies distributed systems. \\n', 'Software design', 'Software design is the process by which an agent creates a specification of a software artifact intended to accomplish goals, using a set of primitive components and subject to constraints.[1] The term is sometimes used broadly to refer to \"all the activity involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying\" the software, or more specifically \"the activity following requirements specification and before programming, as ... [in] a stylized software engineering process.\"[2]\\n', 'Rakesh Agrawal (computer scientist)', \"Rakesh Agrawal (हिन्दी - राकेश अग्रवाल) is a computer scientist who until recently was a Technical Fellow at the Microsoft Search Labs.[1] Rakesh is well known for developing fundamental data mining concepts and technologies and pioneering key concepts in data privacy, including Hippocratic Database, Sovereign Information Sharing, and Privacy-Preserving Data Mining. IBM's commercial data mining product, Intelligent Miner, grew out of his work. His research has been incorporated into other IBM products, including DB2 Mining Extender, DB2 OLAP Server and WebSphere Commerce Server, and has influenced several other commercial and academic products, prototypes and applications. His other technical contributions include Polyglot object-oriented type system, Alert active database system, Ode (Object database and environment), Alpha (extension of relational databases with generalized transitive closure), Nest distributed system, transaction management, and database machines.\\n\", 'Multithreading (computer architecture)', 'In computer architecture, multithreading is the ability of a central processing unit (CPU) (or a single core in a multi-core processor) to provide multiple threads of execution concurrently, supported by the operating system. This approach differs from multiprocessing. In a multithreaded application, the threads share the resources of a single or multiple cores, which include the computing units, the CPU caches, and the translation lookaside buffer (TLB).\\n', 'Flux (machine-learning framework)', 'Flux is an open-source machine-learning software library and ecosystem written in Julia.[1][6] Its current stable release is v0.14.5[4]\\xa0. It has a layer-stacking-based interface for simpler models, and has a strong support on interoperability with other Julia packages instead of a monolithic design.[7] For example, GPU support is implemented transparently by CuArrays.jl[8] This is in contrast to some other machine learning frameworks which are implemented in other languages with Julia bindings,\\xa0such as TensorFlow.jl, and thus are more limited by the functionality present in the underlying implementation, which is often in C or C++.[9] Flux joined NumFOCUS as an affiliated project in December of 2021.[10]\\n', 'SPSS Modeler', 'IBM SPSS Modeler is a data mining and text analytics software application from IBM. It is used to build predictive models and conduct other analytic tasks. It has a visual interface which allows users to leverage statistical and data mining algorithms without programming.\\n', 'Sparse dictionary learning', 'Sparse dictionary learning (also known as sparse coding or SDL) is a representation learning method which aims at finding a sparse representation of the input data in the form of a linear combination of basic elements as well as those basic elements themselves. These elements are called atoms and they compose a dictionary. Atoms in the dictionary are not required to be orthogonal, and they may be an over-complete spanning set. This problem setup also allows the dimensionality of the signals being represented to be higher than the one of the signals being observed. The above two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal but also provide an improvement in sparsity and flexibility of the representation.\\n', 'Autoregressive model', 'In statistics, econometrics, and signal processing, an autoregressive (AR) model is a representation of a type of random process; as such, it is used to describe certain time-varying processes in nature, economics, behavior, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term); thus the model is in the form of a stochastic difference equation (or recurrence relation) which should not be confused with a differential equation. Together with the moving-average (MA) model, it is a special case and key component of the more general autoregressive–moving-average (ARMA) and autoregressive integrated moving average (ARIMA) models of time series, which have a more complicated stochastic structure; it is also a special case of the vector autoregressive model (VAR), which consists of a system of more than one interlocking stochastic difference equation in more than one evolving random variable.\\n', 'KNIME', 'KNIME (/naɪm/), the Konstanz Information Miner,[2] is a free and open-source data analytics, reporting and integration platform. KNIME integrates various components for machine learning and data mining through its modular data pipelining \"Building Blocks of Analytics\" concept. A graphical user interface and use of JDBC allows assembly of nodes blending different data sources, including preprocessing (ETL: Extraction, Transformation, Loading), for modeling, data analysis and visualization without, or with only minimal, programming.\\n', 'Affinity analysis', 'Affinity analysis falls under the umbrella term of data mining which uncovers meaningful correlations between different entities according to their co-occurrence in a data set. In almost all systems and processes, the application of affinity analysis can extract significant knowledge about the unexpected trends[citation needed]. In fact, affinity analysis takes advantages of studying attributes that go together which helps uncover the hidden pattens in a big data through generating association rules. Association rules mining procedure is two-fold: first, it finds all frequent attributes in a data set and, then generates association rules satisfying some predefined criteria, support and confidence, to identify the most important relationships in the frequent itemset. The first step in the process is to count the co-occurrence of attributes in the data set. Next, a subset is created called the frequent itemset. The association rules mining takes the form of if a condition or feature (A) is present then another condition or feature (B) exists. The first condition or feature (A) is called antecedent and the latter (B) is known as consequent. This process is repeated until no additional frequent itemsets are found.\\xa0 There are two important metrics for performing the association rules mining technique: support and confidence. Also, a priori algorithm is used to reduce the search space for the problem.[1]\\n', 'Graphcore', 'Graphcore Limited is a British semiconductor company that develops accelerators for AI and machine learning. It aims to make a massively parallel Intelligence Processing Unit (IPU) that holds the complete machine learning model inside the processor.[2]\\n', 'Decision-making', 'In psychology, decision-making (also spelled decision making and decisionmaking) is regarded as the cognitive process resulting in the selection of a belief or a course of action among several possible alternative options. It could be either rational or irrational. The decision-making process is a reasoning process based on assumptions of values, preferences and beliefs of the decision-maker.[1] Every decision-making process produces a final choice, which may or may not prompt action.\\n', 'Fuzzy logic', '\\nFuzzy logic is a form of many-valued logic in which the truth value of variables may be any real number between 0 and 1. It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false.[1] By contrast, in Boolean logic, the truth values of variables may only be the integer values 0 or 1.\\n', 'Data cleansing', 'Data cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data.[1] Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting or a data quality firewall.\\n', 'Digital art', 'Digital art refers to any artistic work or practice that uses digital technology as part of the creative or presentation process. It can also refer to computational art that uses and engages with digital media.[2]\\n', 'Randomized algorithm', 'A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic or procedure. The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the \"average case\" over all possible choices of random determined by the random bits; thus either the running time, or the output (or both) are random variables.\\n', 'Heuristic', \"A heuristic (/hjʊˈrɪstɪk/; from Ancient Greek  εὑρίσκω (heurískō)\\xa0'to find, discover'), or heuristic technique, is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision.[1][2]\\n\", 'Computational chemistry', 'Computational chemistry is a branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into computer programs, to calculate the structures and properties of molecules, groups of molecules, and solids. It is essential because, apart from relatively recent results concerning the hydrogen molecular ion (dihydrogen cation, see references therein for more details), the quantum many-body problem cannot be solved analytically, much less in closed form.  While computational results normally complement the information obtained by chemical experiments, it can in some cases predict hitherto unobserved chemical phenomena. It is widely used in the design of new drugs and materials.[1]\\n', 'Computer architecture', 'In computer science and computer engineering, computer architecture is a description of the structure of a computer system made from component parts.[1] It can sometimes be a high-level description that ignores details of the implementation.[2] At a more detailed level, the description may include the instruction set architecture design, microarchitecture design, logic design, and implementation.[3]\\n', 'Information retrieval', 'Information retrieval (IR) in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources.  Searches can be based on full-text or other content-based indexing. Information retrieval is the science[1] of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.\\n', 'Caffe (software)', 'Caffe (Convolutional Architecture for Fast Feature Embedding) is a deep learning framework, originally developed at University of California, Berkeley. It is open source, under a BSD license.[4] It is written in C++, with a Python interface.[5]\\n', 'Neural circuit', 'A neural circuit (also known as a biological neural network BNNs) is a population of neurons interconnected by synapses to carry out a specific function when activated.[1] Multiple neural circuits interconnect with one another to form large scale brain networks.[2]\\n', 'Bayesian network', 'A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). While it is one of several forms of causal notation, causal networks are special cases of Bayesian networks. Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.\\n', 'Linear discriminant analysis', \"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\\n\", 'Mean shift', 'Mean shift is a non-parametric feature-space mathematical analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm.[1] Application domains include cluster analysis in computer vision and image processing.[2]\\n', 'Mehryar Mohri', 'Mehryar Mohri is a Professor and theoretical computer scientist[2] at the Courant Institute of Mathematical Sciences. He is also a Research Director \\nat Google Research where he heads the Learning Theory team.\\n', 'GPT-2', 'Generative Pre-trained Transformer 2 (GPT-2) is a large language model by OpenAI and the second in their foundational series of GPT models. GPT-2 was pre-trained on BookCorpus,[2] a dataset of over 7,000 self-published fiction books from various genres, and trained on a dataset of 8 million web pages.[3] It was partially released in February 2019, followed by full release of the 1.5-billion-parameter model on November 5, 2019.[4][5][6][7][8]\\n', 'David Rumelhart', 'David Everett Rumelhart (June 12, 1942 – March 13, 2011)[1] was an American psychologist who made many contributions to the formal analysis of human cognition, working primarily within the frameworks of mathematical psychology, symbolic artificial intelligence, and parallel distributed processing. He also admired formal linguistic approaches to cognition, and explored the possibility of formulating a formal grammar to capture the structure of stories.\\n', 'Computing platform', 'A computing platform, digital platform,[1] or software platform is an environment in which software is executed. It may be the hardware or the operating system (OS), a web browser and associated application programming interfaces, or other underlying software, as long as the program code is executed. Computing platforms have different abstraction levels, including a computer architecture, an OS, or runtime libraries.[2] A computing platform is the stage on which computer programs can run.\\n', 'Concurrency (computer science)', 'In computer science, concurrency is the ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or in partial order, without affecting the outcome.  This allows for parallel execution of the concurrent units, which can significantly improve overall speed of the execution in multi-processor and multi-core systems. In more technical terms, concurrency refers to the decomposability of a program, algorithm, or problem into order-independent or partially-ordered components or units of computation.[1]\\n', 'Activation function', 'Activation function of a node in an artificial neural network is a function that calculates the output of the node (based on its inputs and the weights on individual inputs). Nontrivial problems can be solved only using a nonlinear activation function.[1] Modern activation functions include the smooth version of the ReLU, the GELU, which was used in the 2018 BERT model,[2] the logistic (sigmoid) function used in the 2012 speech recognition model developed by Hinton et al,[3] the ReLU used in the 2012 AlexNet computer vision model and in the 2015 ResNet model. \\n', 'RapidMiner', \"RapidMiner is a data science platform that analyses the collective impact of an organization's data. It was acquired by Altair Engineering in September 2022.[1]\\n\", 'Uncertainty quantification', 'Uncertainty quantification (UQ) is the science of quantitative characterization and estimation of uncertainties in both computational and real world applications. It tries to determine how likely certain outcomes are if some aspects of the system are not exactly known. An example would be to predict the acceleration of a human body in a head-on crash with another car: even if the speed was exactly known, small differences in the manufacturing of individual cars, how tightly every bolt has been tightened, etc., will lead to different results that can only be predicted in a statistical sense.\\n', 'Algorithmic transparency', 'Algorithmic transparency is the principle that the factors that influence the decisions made by algorithms should be visible, or transparent, to the people who use, regulate, and are affected by systems that employ those algorithms. Although the phrase was coined in 2016 by Nicholas Diakopoulos and Michael Koliska about the role of algorithms in deciding the content of digital journalism services,[1] the underlying principle dates back to the 1970s and the rise of automated systems for scoring consumer credit.\\n', 'Category:Cybernetics', 'Cybernetics is a transdisciplinary approach for exploring regulatory systems with feedback, their structures, constraints, and possibilities. Cybernetics is relevant to the study of systems, such as mechanical, physical, biological, cognitive, and social.\\n', 'ACM Computing Classification System', 'The ACM Computing Classification System (CCS) is a subject classification system for computing devised by the Association for Computing Machinery (ACM). The system is comparable to the Mathematics Subject Classification (MSC) in scope, aims, and structure, being used by the various ACM journals to organize subjects by area.\\n', 'Mixed reality', 'Mixed reality (MR) is a term used to describe the merging of a real-world environment and a computer-generated one. Physical and virtual objects may co-exist in mixed reality environments and interact in real time.\\n', 'SpiNNaker', 'SpiNNaker (spiking neural network architecture) is a massively parallel, manycore supercomputer architecture designed by the Advanced Processor Technologies Research Group (APT) at the Department of Computer Science, University of Manchester.[2]  It is composed of 57,600 processing nodes, each with 18 ARM9 processors (specifically ARM968) and 128 MB of mobile DDR SDRAM, totalling 1,036,800 cores and over 7 TB of RAM.[3]  The computing platform is based on spiking neural networks, useful in simulating the human brain (see Human Brain Project).[4][5][6][7][8][9][10][11][12]\\n', 'Middleware', 'Middleware is a type of computer software programme that provides services to software applications beyond those available from the operating system. It can be described as \"software glue\".[1]\\n', 'Logistic regression', 'In statistics, the logistic model (or logit model) is a statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables. In regression analysis, logistic regression[1] (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination). Formally, in binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled \"0\" and \"1\", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \"1\" can vary between 0 (certainly the value \"0\") and 1 (certainly the value \"1\"), hence the labeling;[2] the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See §\\xa0Background and §\\xa0Definition for formal mathematics, and §\\xa0Example for a worked example.\\n', 'Physical neural network', 'A physical neural network is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse or a higher-order (dendritic) neuron model.[1] \"Physical\" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches. More generally the term is applicable to other artificial neural networks in which a memristor or other electrically adjustable resistance material is used to emulate a neural synapse.[2][3]\\n', 'Fuzzy clustering', 'Fuzzy clustering (also referred to as soft clustering or soft k-means) is a form of clustering in which each data point can belong to more than one cluster.\\n', 'Orange (software)', 'Orange is an open-source data visualization, machine learning and data mining toolkit. It features a visual programming front-end for explorative qualitative data analysis and interactive data visualization.\\n', 'Discrete mathematics', 'Discrete mathematics is the study of mathematical structures that can be considered \"discrete\" (in a way analogous to discrete variables, having a bijection with the set of natural numbers) rather than \"continuous\" (analogously to continuous functions). Objects studied in discrete mathematics include integers, graphs, and statements in logic.[1][2][3] By contrast, discrete mathematics excludes topics in \"continuous mathematics\" such as real numbers, calculus or Euclidean geometry. Discrete objects can often be enumerated by integers; more formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets[4] (finite sets or sets with the same cardinality as the natural numbers). However, there is no exact definition of the term \"discrete mathematics\".[5]\\n', 'Oracle Data Mining', 'Oracle Data Mining (ODM) is an option of Oracle Database Enterprise Edition. It contains several data mining and data analysis algorithms for classification, prediction, regression, associations, feature selection, anomaly detection, feature extraction, and specialized analytics. It provides means for the creation, management and operational deployment of data mining models inside the database environment.\\n', 'Wikipedia:WikiProject Computer science', \"Welcome to the WikiProject Computer science page. The goals of the project are to build a community of interest around computer science, and to provide a focal point for coordinating efforts to improve Wikipedia's computer science articles. The scope of the project includes all articles in the area of computer science, including computer programming and software engineering. \\n\", 'Information theory', 'Information theory is the mathematical study of the quantification, storage, and communication of information.[1] The field was originally established by the works of Harry Nyquist and Ralph Hartley, in the 1920s, and Claude Shannon in the 1940s.[2]:\\u200avii\\u200a The field, in  applied mathematics, is at the intersection of probability theory, statistics, computer science, statistical mechanics, information engineering, and electrical engineering.\\n', 'BLOOM (language model)', \"BigScience Large Open-science Open-access Multilingual Language Model (BLOOM[1]) is a transformer-based large language model. It was created by AI researchers to provide a free large language model for large-scale public access. Trained on around 366 billion tokens over March through July 2022, it is considered an alternative to OpenAI's GPT-3 with its 176 billion parameters. BLOOM uses a decoder-only transformer model architecture modified from Megatron-LM GPT-2.\\n\", 'Inference', 'Inferences are steps in reasoning, moving from premises to logical consequences; etymologically, the word infer means to \"carry forward\". Inference is theoretically traditionally divided into deduction and induction, a distinction that in Europe dates at least to Aristotle (300s BCE). Deduction is inference deriving logical conclusions from premises known or assumed to be true, with the laws of valid inference being studied in logic. Induction is inference from particular evidence to a universal conclusion. A third type of inference is sometimes distinguished, notably by Charles Sanders Peirce, contradistinguishing abduction from induction.\\n', 'Stochastic process', 'In probability theory and related fields, a stochastic (/stəˈkæstɪk/) or random process is a mathematical object usually defined as a sequence of random variables, where the index of the sequence has the interpretation of time. Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner. Examples include the growth of a bacterial population, an electrical current fluctuating due to thermal noise, or the movement of a gas molecule.[1][4][5] Stochastic processes have applications in many disciplines such as biology,[6] chemistry,[7] ecology,[8] neuroscience,[9] physics,[10] image processing, signal processing,[11] control theory,[12] information theory,[13] computer science,[14] and telecommunications.[15] Furthermore, seemingly random changes in financial markets have motivated the extensive use of stochastic processes in finance.[16][17][18]\\n', 'Independent component analysis', 'In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that at most one subcomponent is Gaussian and that the subcomponents are statistically independent from each other.[1] ICA is a special case of blind source separation. A common example application is the \"cocktail party problem\" of listening in on one person\\'s speech in a noisy room.[2]\\n', 'Ray Solomonoff', 'Ray Solomonoff (July 25, 1926 – December 7, 2009)[1][2] was the inventor of algorithmic probability,[3] his General Theory of Inductive Inference (also known as Universal Inductive Inference),[4] and was a founder of algorithmic information theory.[5] He was an originator of the branch of artificial intelligence based on machine learning, prediction and probability. He circulated the first report on non-semantic machine learning in 1956.[6]\\n', 'Michael I. Jordan', 'Michael Irwin Jordan ForMemRS[6] (born February 25, 1956) is an American scientist, professor at the University of California, Berkeley and researcher in machine learning, statistics, and artificial intelligence.[7][8][9]\\n', 'Expectation–maximization algorithm', 'In statistics, an expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables.[1] The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.\\n', 'SAS (software)', 'SAS (previously \"Statistical Analysis System\")[1] is a statistical software suite developed by SAS Institute for  data management, advanced analytics, multivariate analysis, business intelligence, criminal investigation,[2] and predictive analytics.\\n', 'Category:Computer science', 'This category has the following 25 subcategories, out of 25 total.\\n', 'Image compression', 'Image compression is a type of data compression applied to digital images, to reduce their cost for storage or transmission. Algorithms may take advantage of visual perception and the statistical properties of image data to provide superior results compared with generic data compression methods which are used for other digital data.[1]\\n', 'Vision processing unit', 'A vision processing unit (VPU) is (as of 2023) an emerging class of microprocessor; it is a specific type of AI accelerator, designed to accelerate machine vision tasks.[1][2]\\n', 'OpenAI Five', '\\nOpenAI Five is a computer program by OpenAI that plays the five-on-five video game Dota 2. Its first public appearance occurred in 2017, where it was demonstrated in a live one-on-one game against the professional player Dendi, who lost to it. The following year, the system had advanced to the point of performing as a full team of five, and began playing against and showing the capability to defeat professional teams.\\n', 'Residual neural network', 'A Residual Neural Network (a.k.a. Residual Network, ResNet)[1] is a deep learning model in which the weight layers learn residual functions with reference to the layer inputs. A Residual Network[1] is a network with skip connections that perform identity mappings, merged with the layer outputs by addition. It behaves like a Highway Network[2] whose gates are opened through strongly positive bias weights. This enables deep learning models with tens or hundreds of layers to train easily and approach better accuracy when going deeper. The identity skip connections, often referred to as \"residual connections\", are also used in the 1997 LSTM networks,[3] Transformer models (e.g., BERT, GPT models such as ChatGPT), the AlphaGo Zero system, the AlphaStar system, and the AlphaFold system.\\n', 'Anomaly detection', 'In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behaviour.[1] Such examples may arouse suspicions of being generated by a different mechanism,[2] or appear inconsistent with the remainder of that set of data.[3]\\n', 'XGBoost', 'XGBoost[2] (eXtreme Gradient Boosting) is an open-source software library which provides a regularizing gradient boosting framework for C++, Java, Python,[3] R,[4] Julia,[5] Perl,[6] and Scala. It works on Linux, Microsoft Windows,[7] and macOS.[8] From the project description, it aims to provide a \"Scalable, Portable and Distributed Gradient Boosting (GBM, GBRT, GBDT) Library\". It runs on a single machine, as well as the distributed processing frameworks Apache Hadoop, Apache Spark, Apache Flink, and Dask.[9][10]\\n', 'Information geometry', 'Information geometry is an interdisciplinary field that applies the techniques of differential geometry to study probability theory and statistics. [1]  It studies statistical manifolds, which are Riemannian manifolds whose points correspond to probability distributions.\\n', 'Alan Mackworth', 'Alan Mackworth is a professor emeritus in the Department of Computer Science at the University of British Columbia. He is known as \"The Founding Father\" of RoboCup. He is a former president of the Association for the Advancement of Artificial Intelligence (AAAI) and former Canada Research Chair in Artificial Intelligence from 2001–2014.\\n', 'IBM Watson Studio', 'Watson Studio, formerly Data Science Experience or DSX, is IBM’s software platform for data science. The platform consists of a workspace that includes multiple collaboration and open-source tools for use in data science.[1]\\n', 'Computer accessibility', 'Computer accessibility (also known as accessible computing) refers to the accessibility of a computer system to all people, regardless of disability type or severity of impairment. The term accessibility is most often used in reference to specialized hardware or software, or a combination of both, designed to enable the use of a computer by a person with a disability or impairment. Computer accessibility often has direct positive effects on people with disabilities.\\n', 'Computer-aided diagnosis', 'Computer-aided detection (CADe), also called computer-aided diagnosis (CADx), are systems that assist doctors in the interpretation of medical images. Imaging techniques in X-ray, MRI, Endoscopy, and ultrasound diagnostics yield a great deal of information that the radiologist or other medical professional has to analyze and evaluate comprehensively in a short time. CAD systems process digital images or videos for typical appearances and to highlight conspicuous sections, such as possible diseases, in order to offer input to support a decision taken by the professional.\\n', 'Apache Spark', \"Apache Spark  is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.\\n\", 'Simulation-based optimization', 'Simulation-based optimization (also known as simply simulation optimization) integrates optimization techniques into simulation modeling and analysis. Because of the complexity of the simulation, the objective function may become difficult and expensive to evaluate. Usually, the underlying simulation model is stochastic, so that the objective function must be estimated using statistical estimation techniques (called output analysis in simulation methodology).\\n', 'Hardware acceleration', 'Hardware acceleration is the use of computer hardware designed to perform specific functions more efficiently when compared to software running on a general-purpose central processing unit (CPU). Any transformation of data that can be calculated in software running on a generic CPU can also be calculated in custom-made hardware, or in some mix of both.\\n', 'LIONsolver', 'LIONsolver is an integrated software for data mining, business intelligence, analytics, and modeling and reactive business intelligence approach.[1] A non-profit version is also available as LIONoso.\\n', 'Concurrent computing', 'Concurrent computing is a form of computing in which several computations are executed concurrently—during overlapping time periods—instead of sequentially—with one completing before the next starts.\\n', 'Non-negative matrix factorization', 'Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.\\n', 'Project Debater', 'Project Debater is an IBM artificial intelligence project, designed to participate in a full live debate with expert human debaters.[1][2][3][4] It follows on from the Watson project which played Jeopardy![5]\\n', 'Gaussian process', 'In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.\\n', 'k-SVD', 'In applied mathematics, k-SVD is a dictionary learning algorithm for creating a dictionary for sparse representations, via a singular value decomposition approach. k-SVD is a generalization of the k-means clustering method, and it works by iteratively alternating between sparse coding the input data based on the current dictionary, and updating the atoms in the dictionary to better fit the data. It is structurally related to the expectation maximization (EM) algorithm.[1][2] k-SVD can be found widely in use in applications such as image processing, audio processing, biology, and document analysis.\\n', 'Weighting', 'The process of weighting involves emphasizing the contribution of particular aspects of a phenomenon (or of a set of data) over others to an outcome or result; thereby highlighting those aspects in comparison to others in the analysis. That is, rather than each variable in the data set contributing equally to the final result, some of the data is adjusted to make a greater contribution than others. This is analogous to the practice of adding (extra) weight to one side of a pair of scales in order to favour either the buyer or seller.\\n', 'Word processor', 'A word processor (WP)[1][2] is a device or computer program that provides for input, editing, formatting, and output of text, often with some additional features.\\n', 'Statistica', 'Statistica is an advanced analytics software package originally developed by StatSoft and currently maintained by TIBCO Software Inc.[1]\\nStatistica provides data analysis, data management, statistics, data mining, machine learning, text analytics and data visualization procedures.\\n', 'Software suite', 'A software suite[1] (also known as an application suite) is a collection of computer programs (application software, or programming software) of related functionality, sharing a similar user interface and the ability to easily exchange data with each other.\\n', 'Exploratory data analysis', 'In statistics, exploratory data analysis (EDA) is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling and thereby contrasts traditional hypothesis testing. Exploratory data analysis has been promoted by John Tukey since 1970 to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA),[1][2] which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.\\n', 'Principal component analysis', 'Principal component analysis (PCA) is a popular technique for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data. Formally, PCA is a statistical technique for reducing the dimensionality of a dataset. This is accomplished by linearly transforming the data into a new coordinate system where (most of) the variation in the data can be described with fewer dimensions than the initial data. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points. Principal component analysis has applications in many fields such as population genetics, microbiome studies, and atmospheric science.[1]', 'Gated recurrent unit', \"Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al.[1] The GRU is like a long short-term memory (LSTM) with a gating mechanism to input or forget certain features,[2] but lacks a context vector or output gate, resulting in fewer parameters than LSTM.[3] \\nGRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM.[4][5] GRUs showed that gating is indeed helpful in general, and Bengio's team came to no concrete conclusion on which of the two gating units was better.[6][7]\\n\", 'Bootstrapping (statistics)', 'Bootstrapping is any test or metric that uses random sampling with replacement (e.g. mimicking the sampling process), and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates.[1][2] This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.[3][4]\\n', 'Netflix Prize', 'The Netflix Prize was an open competition for the best collaborative filtering algorithm to predict user ratings for films, based on previous ratings without any other information about the users or films, i.e. without the users being identified except by numbers assigned for the contest.\\n', 'Hierarchical clustering', 'In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis that seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two categories:\\n', 'Ubiquitous computing', ' Ubiquitous computing (or \"ubicomp\") is a concept in software engineering, hardware engineering and computer science where computing is made to appear anytime and everywhere. In contrast to desktop computing, ubiquitous computing can occur using any device, in any location, and in any format. A user interacts with the computer, which can exist in many different forms, including laptop computers, tablets, smart phones and terminals in everyday objects such as a refrigerator or a pair of glasses. The underlying technologies to support ubiquitous computing include Internet, advanced middleware, operating system, mobile code, sensors, microprocessors, new I/O and user interfaces, computer networks, mobile protocols, location and positioning, and new materials.\\n', 'Probability theory', 'Probability theory or probability calculus is the branch of mathematics concerned with probability. Although there are several different probability interpretations, probability theory treats the concept in a rigorous mathematical manner by expressing it through a set of axioms. Typically these axioms formalise probability in terms of a probability space, which assigns a measure taking values between 0 and 1, termed the probability measure, to a set of outcomes called the sample space. Any specified subset of the sample space is called an event.\\n', 'k-means clustering', 'k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.\\n', 'Naomi Altman', 'Naomi Altman is a statistician known for her work on kernel smoothing[KS] and kernel regression,[KR]\\nand interested in applications of statistics to gene expression and genomics. She is a professor of statistics at Pennsylvania State University,[1] and a regular columnist for the \"Points of Significance\" column in Nature Methods.[2]\\n', 'Template talk:Differentiable computing', \"I don't want to remove the group because I didn't contribute anything to the template and removing stuff added by other users seems rude to me. But now using JS or GO for programming gradient descent based algorithm is not that uncommon, does it implies that wikipedia should also add JS/GO to the group. To put it simply, I don't think having a group for popular programming languages used for Differentiable computing helps anyone but rather might push the idea that Python is the language for AI ignoring many other important stuff, for example the C++ core of the python interface. I don't hate python/julia and not a Go/JS fanboy. -- 1e100 (talk) 11:16, 18 December 2021 (UTC)Reply[reply]\\n\", 'U-Net', 'U-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg.[1] The network is based on a fully convolutional neural network[2] whose architecture was modified and extended to work with fewer training images and to yield more precise segmentation. Segmentation of a 512\\xa0×\\xa0512 image takes less than a second on a modern GPU.\\n', 'Control flow', 'In computer science, control flow (or flow of control) is the order in which individual statements, instructions or function calls of an imperative program are executed or evaluated. The emphasis on explicit control flow distinguishes an imperative programming language from a declarative programming language.\\n', 'Matrix (mathematics)', 'In mathematics, a matrix (pl.: matrices) is a rectangular array or table of numbers, symbols, or expressions, arranged in rows and columns, which is used to represent a mathematical object or a property of such an object.\\n', 'Software construction', 'Software construction is a software engineering discipline. It is the detailed creation of working meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging. It is linked to all the other software engineering disciplines, most strongly to software design and software testing.[1]\\n', 'Bayesian optimization', 'Bayesian optimization is a sequential design strategy for global optimization of black-box functions[1][2][3]  that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions.\\n', 'Ordinary least squares', 'In statistics, ordinary least squares (OLS) is a type of linear least squares method for choosing the unknown parameters in a linear regression model (with fixed level-one effects of a linear function of a set of explanatory variables) by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the input dataset and the output of the (linear) function of the independent variable.\\n', 'Database', 'In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.\\n', 'Software maintenance', 'Software maintenance in software engineering is the modification of a software product after delivery to correct faults, to improve performance or other attributes.[1][2]\\n', 'GPT-J', 'GPT-J or GPT-J-6B is an open-source large language model (LLM) developed by EleutherAI in 2021.[1] As the name suggests, it is a generative pre-trained transformer model designed to produce human-like text that continues from a prompt. The optional \"6B\" in the name refers to the fact that it has 6 billion parameters.[2]\\n', 'Vision transformer', 'A vision transformer (ViT) is a transformer designed for computer vision. Transformers were introduced in 2017,[1] and have found widespread use in natural language processing. In 2020, they were adapted for computer vision, yielding ViT.[2] The basic structure is to break down input images as a series of patches, then tokenized, before applying the tokens to a standard Transformer architecture.\\n', 'Special Interest Group on Knowledge Discovery and Data Mining', \"SIGKDD, representing the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining, hosts an influential annual conference.\\n\", 'Trevor Hastie', 'Trevor John Hastie (born 27 June 1953) is an American statistician and computer scientist. He is currently serving as the John A. Overdeck Professor of Mathematical Sciences and Professor of Statistics at Stanford University.[1] Hastie is known for his contributions to applied statistics, especially in the field of machine learning, data mining, and bioinformatics. He has authored several popular books in statistical learning, including The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Hastie has been listed as an ISI Highly Cited Author in Mathematics by the ISI Web of Knowledge.\\n', 'Polynomial regression', 'In statistics, polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y\\xa0|x). Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function E(y\\xa0|\\xa0x) is linear in the unknown parameters that are estimated from the data.  For this reason, polynomial regression is considered to be a special case of multiple linear regression.\\n', 'Dimensionality reduction', 'Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with). Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics.[1]\\n', 'ELKI', 'ELKI (Environment for Developing KDD-Applications Supported by Index-Structures) is a data mining (KDD, knowledge discovery in databases) software framework developed for use in research and teaching. It was originally at the database systems research unit of Professor Hans-Peter Kriegel at the Ludwig Maximilian University of Munich, Germany, and now continued at the Technical University of Dortmund, Germany. It aims at allowing the development and evaluation of advanced data mining algorithms and their interaction with database index structures.\\n', 'Information system', 'An information system (IS) is a formal, sociotechnical, organizational system designed to collect, process, store, and distribute information.[1] From a sociotechnical perspective, information systems are composed by four components: task, people, structure (or roles), and technology.[2] Information systems can be defined as an integration of components for collection, storage and processing of data of which the data is used to provide information, contribute to knowledge as well as digital products that facilitate decision making.[3]\\n', 'State–action–reward–state–action', 'State–action–reward–state–action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was proposed by Rummery and Niranjan in a technical note[1] with the name \"Modified Connectionist Q-Learning\" (MCQ-L). The alternative name SARSA, proposed by Rich Sutton, was only mentioned as a footnote.\\n', 'International Conference on Intelligent Robots and Systems', \"IROS, the IEEE/RSJ International Conference on Intelligent Robots and Systems,[1] is an annual academic conference covering advances in robotics.[2] It is one of the premier conferences of its field (alongside ICRA, International Conference on Robotics and Automation) with an 'A' rating from the Australian Ranking of ICT Conferences obtained in 2010 and an 'A1' rating from the Brazilian ministry of education in 2012.[3][4]\\n\", 'Time complexity', 'In theoretical computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a constant factor.\\n', 'Matrix decomposition', 'In the mathematical discipline of linear algebra, a matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. There are many different matrix decompositions; each finds use among a particular class of problems.\\n', 'Geographic information system', 'A geographic information system (GIS) consists of integrated computer hardware and software that store, manage, analyze, edit, output, and visualize geographic data.[1][2] Much of this often happens within a spatial database, however, this is not essential to meet the definition of a GIS.[1] In a broader sense, one may consider such a system also to include human users and support staff, procedures and workflows, the body of knowledge of relevant concepts and methods, and institutional organizations.\\n', 'Tree (data structure)', 'In computer science, a tree is a widely used abstract data type that represents a hierarchical tree structure with a set of connected nodes. Each node in the tree can be connected to many children (depending on the type of tree), but must be connected to exactly one parent,[1] except for the root node, which has no parent (i.e., the root node as the top-most node in the tree hierarchy). These constraints mean there are no cycles or \"loops\" (no node can be its own ancestor), and also that each child can be treated like the root node of its own subtree, making recursion a useful technique for tree traversal. In contrast to linear data structures, many trees cannot be represented by relationships between neighboring nodes (parent and children nodes of a node under consideration if they exists) in a single straight line (called edge or link between two adjacent nodes).\\n', 'Imprecise probability', 'Imprecise probability generalizes probability theory to allow for partial probability specifications, and is applicable when information is scarce, vague, or conflicting, in which case a unique probability distribution may be hard to identify. Thereby, the theory aims to represent the available knowledge more accurately.  Imprecision is useful for dealing with expert elicitation, because:\\n', 'Data collection', 'Data collection or data gathering is the process of gathering and measuring information on targeted variables in an established system, which then enables one to answer relevant questions and evaluate outcomes. Data collection is a research component in all study fields, including physical and social sciences, humanities,[2] and business. While methods vary by discipline, the emphasis on ensuring accurate and honest collection remains the same. The goal for all data collection is to capture evidence that allows data analysis to lead to the formulation of credible answers to the questions that have been posed.\\n', 'Tensor', 'In mathematics, a tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects related to a vector space. Tensors may map between different objects such as vectors, scalars, and even other tensors. There are many types of tensors, including scalars and vectors (which are the simplest tensors), dual vectors, multilinear maps between vector spaces, and even some operations such as the dot product. Tensors are defined independent of any basis, although they are often referred to by their components in a basis related to a particular coordinate system; those components form an array, which can be thought of as a high-dimensional matrix. \\n', 'Enterprise software', 'Enterprise software, also known as enterprise application software (EAS), is computer software used to satisfy the needs of an organization rather than its individual users. Enterprise software is an integral part of a computer-based information system, handling a number of business operations, for example to enhance business and management reporting tasks, or support production operations and back office functions. Enterprise systems must process information at a relatively high speed.[1]\\n', 'Network service', 'In computer networking, a network service is an application running at the network application layer and above, that provides data storage, manipulation, presentation, communication or other capability which is often implemented using a client–server or peer-to-peer architecture based on application layer network protocols.[1]\\n', 'Sampling (statistics)', 'In statistics, quality assurance, and survey methodology, sampling is the selection of a subset or a statistical sample (termed sample for short) of individuals from within a statistical population to estimate characteristics of the whole population. Statisticians attempt to collect samples that are representative of the population. Sampling has lower costs and faster data collection compared to recording data from the entire population, and thus, it can provide insights in cases where it is infeasible to measure an entire population. \\n', 'Software configuration management', 'In software engineering, software configuration management (SCM or S/W CM; also expanded as source configuration management process and software change and configuration management[1]) is the task of tracking and controlling changes in the software, part of the larger cross-disciplinary field of configuration management.[2]  SCM practices include revision control and the establishment of baselines.  If something goes wrong, SCM can determine the \"what, when, why and who\" of the change.  If a configuration is working well, SCM can determine how to replicate it across many hosts.\\n', 'Force control', 'Force control is the control of the force with which a machine or the manipulator of a robot acts on an object or its environment. By controlling the contact force, damage to the machine as well as to the objects to be processed and injuries when handling people can be prevented. In manufacturing tasks, it can compensate for errors and reduce wear by maintaining a uniform contact force. Force control achieves more consistent results than position control, which is also used in machine control. Force control can be used as an alternative to the usual motion control, but is usually used in a complementary way, in the form of hybrid control concepts. The acting force for control is usually measured via force transducers or estimated via the motor current.\\n', 'Very Large Scale Integration', 'Very-large-scale integration (VLSI) is the process of creating an integrated circuit (IC) by combining millions or billions of MOS transistors onto a single chip. VLSI began in the 1970s when MOS integrated circuit (Metal Oxide Semiconductor) chips were developed and then widely adopted, enabling complex semiconductor and telecommunication technologies. The microprocessor and memory chips are VLSI devices.\\n', 'Syntactic pattern recognition', 'Syntactic pattern recognition or structural pattern recognition is a form of pattern recognition, in which each object can be represented by a variable-cardinality set of symbolic, nominal features. This allows for representing pattern structures, taking into account more complex interrelationships between attributes than is possible in the case of flat, numerical feature vectors of fixed dimensionality, that are used in statistical classification.\\n', 'Real-time computing', 'Real-time computing (RTC) is the computer science term for hardware and software systems subject to a \"real-time constraint\", for example from event to system response.[1] Real-time programs must guarantee response within specified time constraints, often referred to as \"deadlines\".[2]\\n', 'Network architecture', \"Network architecture is the design of a computer network. It is a framework for the specification of a network's physical components and their functional organization and configuration, its operational principles and procedures, as well as communication protocols used.\\n\", 'Dynamic Bayesian network', 'A dynamic Bayesian network (DBN) is a Bayesian network (BN) which relates variables to each other over adjacent time steps. \\n', 'Adaptive website', \"An adaptive website is a website that builds a model of user activity and modifies the information and/or presentation of information to the user in order to better address the user's needs.[1]\\n\", 'Loss function', 'In mathematical optimization and decision theory, a loss function or cost function (sometimes also called an error function) [1] is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its opposite (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. The loss function could include terms from several levels of the hierarchy.\\n', 'Random variable', \"A random variable (also called random quantity, aleatory variable, or stochastic variable) is a mathematical formalization of a quantity or object which depends on random events.[1] The term 'random variable' can be misleading as its mathematical definition is not actually random nor a variable,[2] but rather it is a function from possible outcomes (e.g., the possible upper sides of a flipped coin such as heads \\n\\n\\n\\nH\\n\\n\\n{\\\\displaystyle H}\\n\\n and tails \\n\\n\\n\\nT\\n\\n\\n{\\\\displaystyle T}\\n\\n) in a sample space (e.g., the set \\n\\n\\n\\n{\\nH\\n,\\nT\\n}\\n\\n\\n{\\\\displaystyle \\\\{H,T\\\\}}\\n\\n) to a measurable space (e.g., \\n\\n\\n\\n{\\n−\\n1\\n,\\n1\\n}\\n\\n\\n{\\\\displaystyle \\\\{-1,1\\\\}}\\n\\n in which 1 is corresponding to \\n\\n\\n\\nH\\n\\n\\n{\\\\displaystyle H}\\n\\n and −1 is corresponding to \\n\\n\\n\\nT\\n\\n\\n{\\\\displaystyle T}\\n\\n, respectively), often to the real numbers.\\n\", 'Echo state network', 'An echo state network (ESN)[1][2] is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can produce or reproduce specific temporal patterns. The main interest of this network is that although its behavior is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.\\n', 'Social software', 'Social software, also known as social apps or social platform includes communications and interactive tools that are often based on the Internet. Communication tools typically handle capturing, storing and presenting communication, usually written but increasingly including audio and video as well. Interactive tools handle mediated interactions between a pair or group of users. They focus on establishing and maintaining a connection among users, facilitating the mechanics of conversation and talk.[1] Social software generally refers to software that makes collaborative behaviour, the organisation and moulding of communities, self-expression, social interaction and feedback possible for individuals. Another element of the existing definition of social software is that it allows for the structured mediation of opinion between people, in a centralized or self-regulating manner. The most improved area for social software is that Web 2.0 applications can all promote co-operation between people and the creation of online communities more than ever before. The opportunities offered by social software are instant connections and opportunities to learn.[2]An additional defining feature of social software is that apart from interaction and collaboration, it aggregates the collective behaviour of its users, allowing not only crowds to learn from an individual but individuals to learn from the crowds as well.[3] Hence, the interactions enabled by social software can be one-to-one, one-to-many, or many-to-many.[2]\\n', 'Industrial process control', 'An industrial process control or simply process control in continuous production processes is a discipline that uses industrial control systems and control theory to achieve a production level of consistency, economy and safety which could not be achieved purely by human manual control. It is implemented widely in industries such as automotive, mining, dredging, oil refining, pulp and paper manufacturing, chemical processing and power generating plants.[1]\\n', 'Mutation (genetic algorithm)', 'Mutation is a genetic operator used to maintain genetic diversity of the chromosomes of a population of a genetic or, more generally, an evolutionary algorithm (EA). It is analogous to biological mutation.\\n', 'Medical diagnosis', \"Medical diagnosis (abbreviated Dx,[1] Dx, or Ds) is the process of determining which disease or condition explains a person's symptoms and signs. It is most often referred to as diagnosis with the medical context being implicit. The information required for diagnosis is typically collected from a history and physical examination of the person seeking medical care. Often, one or more diagnostic procedures, such as medical tests, are also done during the process. Sometimes the posthumous diagnosis is considered a kind of medical diagnosis.\\n\", 'SequenceL', 'SequenceL is a general purpose functional programming language and auto-parallelizing (Parallel computing) compiler and tool set, whose primary design objectives are performance on multi-core processor hardware, ease of programming, platform portability/optimization, and code clarity and readability.  Its main advantage is that it can be used to write straightforward code that automatically takes full advantage of all the processing power available, without programmers needing to be concerned with identifying parallelisms, specifying vectorization, avoiding race conditions, and other challenges of manual directive-based programming approaches such as OpenMP.\\n', 'Climatology', 'Climatology (from Greek κλίμα, klima, \"slope\"; and -λογία, -logia) or climate science is the scientific study of Earth\\'s climate, typically defined as weather conditions averaged over a period of at least 30 years.[1] Climate concerns the atmospheric condition during an extended to indefinite period of time; weather is the condition of the atmosphere during a relative brief period of time. The main topics of research are the study of climate variability, mechanisms of climate changes and modern climate change.[2][3] This topic of study is regarded as part of the atmospheric sciences and a subdivision of physical geography, which is one of the Earth sciences. Climatology includes some aspects of oceanography and biogeochemistry.\\n', 'Convolution', 'In mathematics (in particular, functional analysis), convolution is a mathematical operation on two functions (f and g) that produces a third function (\\n\\n\\n\\nf\\n∗\\ng\\n\\n\\n{\\\\displaystyle f*g}\\n\\n) that expresses how the shape of one is modified by the other. The term convolution refers to both the result function and to the process of computing it. It is defined as the integral of the product of the two functions after one is reflected about the y-axis and shifted. The choice of which function is reflected and shifted before the integral does not change the integral result (see commutativity). The integral is evaluated for all values of shift, producing the convolution function.\\n', 'Application security', 'Application security (short AppSec) includes all tasks that introduce a secure software development life cycle to development teams. Its final goal is to improve security practices and, through that, to find, fix and preferably prevent security issues within applications. It encompasses the whole application life cycle from requirements analysis, design, implementation, verification as well as maintenance.[1]\\n', 'Nature Methods', 'Nature Methods is a monthly peer-reviewed scientific journal covering new scientific techniques. It was established in 2004 and is published by Springer Nature under the Nature Portfolio. Like other Nature journals, there is no external editorial board and editorial decisions are made by an in-house team, although peer review by external experts forms a part of the review process.[1] The editor-in-chief is Allison Doerr.[2]\\n', 'Template talk:Computer science', 'This part is a bit short. But does OLAP really belong into here? is it really \"major\"?\\n', 'Anthropic', 'Anthropic PBC is an American artificial intelligence (AI) startup company, founded by former members of OpenAI.[3][4] Anthropic develops general AI systems and large language models.[5] It is a public-benefit corporation, and has been connected to the effective altruism movement.\\n', 'Ridge regression', 'Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated.[1] It has been used in many fields including econometrics, chemistry, and engineering.[2] Also known as Tikhonov regularization, named for Andrey Tikhonov, it is a method of regularization of ill-posed problems.[a] It is particularly useful to mitigate the problem of multicollinearity in linear regression, which commonly occurs in models with large numbers of parameters.[3] In general, the method provides improved efficiency in parameter estimation problems in exchange for a tolerable amount of bias (see bias–variance tradeoff).[4]\\n', 'Health informatics', '  \\n  \\n  \\n', 'Tomasz Imieliński', 'Tomasz Imieliński (born July 11, 1954, in Toruń, Poland) is a Polish-American computer scientist, most known in the areas of data mining, mobile computing, data extraction, and search engine technology. He is currently a professor of computer science at Rutgers University in New Jersey, United States.\\n', 'False positives and false negatives', 'A false positive is an error in binary classification in which a test result incorrectly indicates the presence of a condition (such as a disease when the disease is not present), while a false negative is the opposite error, where the test result incorrectly indicates the absence of a condition when it is actually present. These are the two kinds of errors in a binary test, in contrast to the two kinds of correct result (a true positive and a true negative). They are also known in medicine as a false positive (or false negative) diagnosis, and in statistical classification as a false positive (or false negative) error.[1]\\n', 'User behavior analytics', \"User behavior analytics (UBA) or user and entity behavior analytics (UEBA),[1] is the concept of analyzing the behavior of users, subjects, visitors, etc. for a specific purpose.[2] It allows cybersecurity tools to build a profile of each individual's normal activity, by looking at patterns of human behavior, and then highlighting deviations from that profile (or anomalies) that may indicate a potential compromise.[3][4][5]\\n\", 'Factor analysis', 'Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors plus \"error\" terms, hence factor analysis can be thought of as a special case of errors-in-variables models.[1]\\n', 'ACM Computing Surveys', 'ACM Computing Surveys is peer-reviewed quarterly scientific journal and is published by the Association for Computing Machinery. It publishes survey articles and tutorials related to computer science and computing. The journal was established in 1969 with William S. Dorn as founding editor-in-chief.[1]\\n', 'Statistical manifold', 'In mathematics, a statistical manifold is a Riemannian manifold, each of whose points is a probability distribution.  Statistical manifolds provide a setting for the field of information geometry.  The Fisher information metric provides a metric on these manifolds. Following this definition, the log-likelihood function is a differentiable map and the score is an inclusion.[1]\\n', 'File:Neural network with dark background.png', 'Original file \\u200e(1,280 × 1,039 pixels, file size: 837 KB, MIME type: image/png)\\n', 'Rectifier (neural networks)', 'In the context of artificial neural networks, the rectifier or ReLU (rectified linear unit) activation function[1][2] is an activation function defined as the positive part of its argument:\\n', 'Document management system', '\\nA document management system (DMS) is usually a computerized system used to store, share, track and manage files or documents. Some systems include history tracking where a log of the various versions created and modified by different users is recorded. The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems.\\n', 'Operations research', '\\nOperations research (British English: operational research) (U.S. Air Force Specialty Code: Operations Analysis), often shortened to the initialism OR, is a discipline that deals with the development and application of analytical methods to improve decision-making.[1] The term management science is occasionally used as a synonym.[2]\\n', 'Ehud Shapiro', \"Ehud Shapiro (Hebrew: אהוד שפירא; born 1955) is an Israeli scientist, artist, and entrepreneur, who is Professor of Computer Science and Biology at the Weizmann Institute of Science.[2] With international reputation, he made fundamental contributions to many scientific disciplines,[3] laying in each a long-term research agenda by asking a novel basic question and offering a first step towards answering it, including how to computerize the process of scientific discovery, by providing an algorithmic interpretation to Karl Popper's methodology of conjectures and refutations;[4][5] how to automate program debugging, by algorithms for fault localization;[6] how to unify parallel, distributed, and systems programming with a high-level logic-based programming language;[7] how to use the metaverse as a foundation for social networking;[8] how to devise molecular computers that can function as smart programmable drugs;[9][10] how to uncover the human cell lineage tree, via single-cell genomics;[11][12] how to support digital democracy, by devising an alternative architecture to the digital realm.[13][14]\\n\", 'Diffusion process', 'In probability theory and statistics, diffusion processes are a class of continuous-time Markov process with almost surely continuous sample paths. Diffusion process is stochastic in nature and hence is used to model many real-life stochastic systems. Brownian motion, reflected Brownian motion and Ornstein–Uhlenbeck processes are examples of diffusion processes. It is used heavily in statistical physics, statistical analysis, information theory, data science, neural networks, finance and marketing.\\n', 'Microcontroller', 'A microcontroller (MC, UC, or μC) or microcontroller unit (MCU) is a small computer on a single integrated circuit. A microcontroller contains one or more CPUs (processor cores) along with memory and programmable input/output peripherals. Program memory in the form of ferroelectric RAM, NOR flash or OTP ROM is also often included on chip, as well as a small amount of RAM. Microcontrollers are designed for embedded applications, in contrast to the microprocessors used in personal computers or other general purpose applications consisting of various discrete chips.\\n', 't-distributed stochastic neighbor embedding', 't-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map. It is based on Stochastic Neighbor Embedding originally developed by Geoffrey Hinton and Sam Roweis,[1] where Laurens van der Maaten proposed the t-distributed variant.[2] It is a nonlinear dimensionality reduction technique for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\\n', 'Hardware security', 'Hardware security is a discipline originated from the cryptographic engineering and involves hardware design, access control, secure multi-party computation, secure key storage, ensuring code authenticity, measures to ensure that the supply chain that built the product is secure among other things.[1][2][3][4]\\n', 'Crossover (genetic algorithm)', 'In genetic algorithms and evolutionary computation, crossover, also called recombination, is a genetic operator used to combine the genetic information of two parents to generate new offspring. It is one way to stochastically generate new solutions from an existing population, and is analogous to the crossover that happens during sexual reproduction in biology. Solutions can also be generated by cloning an existing solution, which is analogous to asexual reproduction. Newly generated solutions may be mutated before being added to the population.\\n', 'CiteSeerX', 'CiteSeerX (formerly called CiteSeer) is a public search engine and digital library for scientific and academic papers, primarily in the fields of computer and information science.\\n', 'Tensor calculus', 'In mathematics, tensor calculus, tensor analysis, or Ricci calculus is an extension of vector calculus to tensor fields (tensors that may vary over a manifold, e.g. in spacetime).\\n', 'Intrusion detection system', 'An intrusion detection system (IDS; also intrusion prevention system or IPS) is a device or software application that monitors a network or systems for malicious activity or policy violations.[1] Any intrusion activity or violation is typically reported either to an administrator or collected centrally using a security information and event management (SIEM) system. A SIEM system combines outputs from multiple sources and uses alarm filtering techniques to distinguish malicious activity from false alarms.[2]\\n', 'Digital library', 'A digital library, also called an online library, an internet library, a digital repository,  a library without walls, or a digital collection, is an online database of digital objects that can include text, still images, audio, video, digital documents, or other digital media formats or a library accessible through the internet. Objects can consist of digitized content like print or photographs, as well as originally produced digital content like word processor files or social media posts. In addition to storing content, digital libraries provide means for organizing, searching, and retrieving the content contained in the collection. Digital libraries can vary immensely in size and scope, and can be maintained by individuals or organizations.[1] The digital content may be stored locally, or accessed remotely via computer networks. These information retrieval systems are able to exchange information with each other through interoperability and sustainability.[2]\\n', 'Software quality', 'In the context of software engineering, software quality refers to two related but distinct notions:[citation needed]\\n', 'File:Decision Tree.jpg', 'Decision_Tree.jpg \\u200e(457 × 473 pixels, file size: 17 KB, MIME type: image/jpeg)\\n', 'Possibility theory', 'Possibility theory is a mathematical theory for dealing with certain types of uncertainty and is an alternative to probability theory. It uses measures of possibility and necessity between 0 and 1, ranging from impossible to possible and unnecessary to necessary, respectively. Professor Lotfi Zadeh first introduced possibility theory in 1978 as an extension of his theory of fuzzy sets and fuzzy logic. Didier Dubois and Henri Prade further contributed to its development. Earlier, in the 1950s, economist G. L. S. Shackle proposed the min/max algebra to describe degrees of potential surprise.\\n', 'BIRCH', 'BIRCH (balanced iterative reducing and clustering using hierarchies) is an unsupervised data mining algorithm used to perform hierarchical clustering over particularly large data-sets.[1] With modifications it can also be used to accelerate k-means clustering and Gaussian mixture modeling with the expectation–maximization algorithm.[2] An advantage of BIRCH is its ability to incrementally and dynamically cluster incoming, multi-dimensional metric data points in an attempt to produce the best quality clustering for a given set of resources (memory and time constraints). In most cases, BIRCH only requires a single scan of the database.\\n', 'Networking hardware', '\\nNetworking hardware, also known as network equipment or computer networking devices, are electronic devices that are required for communication and interaction between devices on a computer network. Specifically, they mediate data transmission in a computer network.[1] Units which are the last receiver or generate data are called hosts, end systems or data terminal equipment.\\n', 'Network security', 'Network security consists of the policies, processes and practices adopted to prevent, detect and monitor unauthorized access, misuse, modification, or denial of a computer network and network-accessible resources.[1] Network security involves the authorization of access to data in a network, which is controlled by the network administrator. Users choose or are assigned an ID and password or other authenticating information that allows them access to information and programs within their authority. Network security covers a variety of computer networks, both public and private, that are used in everyday jobs: conducting transactions and communications among businesses, government agencies and individuals. Networks can be private, such as within a company, and others which might be open to public access. Network security is involved in organizations, enterprises, and other types of institutions. It does as its title explains: it secures the network, as well as protecting and overseeing operations being done. The most common and simple way of protecting a network resource is by assigning it a unique name and a corresponding password.\\n', 'Data quality', 'Data quality refers to the state of qualitative or quantitative pieces of information. There are many definitions of data quality, but data is generally considered high quality if it is \"fit for [its] intended uses in operations, decision making and planning\".[1][2][3] Moreover, data is deemed of high quality if it correctly represents the real-world construct to which it refers. Furthermore, apart from these definitions, as the number of data sources increases, the question of internal data consistency becomes significant, regardless of fitness for use for any particular external purpose. People\\'s views on data quality can often be in disagreement, even when discussing the same set of data used for the same purpose. When this is the case, data governance is used to form agreed upon definitions and standards for data quality. In such cases, data cleansing, including standardization, may be required in order to ensure data quality.[4]\\n', 'Software repository', 'A software repository, or repo for short, is a storage location for software packages. Often a table of contents is also stored, along with metadata. A software repository is typically managed by source or version control, or repository managers. Package managers allow automatically installing and updating repositories, sometimes called \"packages\".\\n', 'Sensor', 'A sensor is a device that produces an output signal for the purpose of sensing a physical phenomenon.\\n', 'Isolation forest', 'Isolation Forest is an algorithm for data anomaly detection initially developed by Fei Tony Liu in 2008.[1] Isolation Forest detects anomalies using binary trees. The algorithm has a linear time complexity and a low memory requirement, which works well with high-volume data.[2][3]\\nIn essence, the algorithm relies upon the characteristics of anomalies, i.e., being few and different, in order to detect anomalies. No density estimation is performed in the algorithm. The algorithm is different from decision tree algorithms in that only the path-length measure or approximation is being used to generate the anomaly score, no leaf node statistics on class distribution or target value is needed.\\n', 'Dempster–Shafer theory', 'The theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory (DST), is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. First introduced by Arthur P. Dempster[1] in the context of statistical inference, the theory was later developed by Glenn Shafer into a general framework for modeling epistemic uncertainty—a mathematical theory of evidence.[2][3] The theory allows one to combine evidence from different sources and arrive at a degree of belief (represented by a mathematical object called belief function) that takes into account all the available evidence.\\n', 'Ranking', 'A ranking is a relationship between a set of items such that, for any two items, the first is either \"ranked higher than\", \"ranked lower than\", or \"ranked equal to\" the second.[1] In mathematics, this is known as a weak order or total preorder of objects. It is not necessarily a total order of objects because two different objects can have the same ranking. The rankings themselves are totally ordered. For example, materials are totally preordered by hardness, while degrees of hardness are totally ordered. If two items are the same in rank it is considered a tie.\\n', 'Dependability', \"In systems engineering, dependability is a measure of a system's availability, reliability, maintainability, and in some cases, other characteristics such as durability, safety and security.[1]  In real-time computing, dependability is the ability to provide services that can be trusted within a time-period.[2] The service guarantees must hold even when the system is subject to attacks or natural failures. \\n\", 'Operational definition', 'An operational definition specifies concrete, replicable procedures designed to represent a construct. In the words of American psychologist S.S. Stevens (1935), \"An operation is the performance which we execute in order to make known a concept.\"[1][2] For example, an operational definition of \"fear\" (the construct) often includes measurable physiologic responses that occur in response to a perceived threat. Thus, \"fear\" might be operationally defined as specified changes in heart rate, galvanic skin response, pupil dilation, and blood pressure.[3]\\n', 'Differentiable function', 'In mathematics, a differentiable function of one real variable is a function whose derivative exists at each point in its domain. In other words, the graph of a differentiable function has a non-vertical tangent line at each interior point in its domain. A differentiable function is smooth (the function is locally well approximated as a linear function at each interior point) and does not contain any break, angle, or cusp.\\n', 'CURE algorithm', 'CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases[citation needed]. Compared with K-means clustering it is more robust to outliers and able to identify clusters having non-spherical shapes and size variances.\\n', 'Multimedia database', 'A Multimedia database (MMDB) is a collection of related for multimedia data.[1] The multimedia data include one or more primary media data types such as text, images, graphic objects (including drawings, sketches and illustrations) animation sequences, audio and video.\\n', 'Multivariate normal distribution', 'In probability theory and statistics, the multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions.  One definition is that a random vector is said to be k-variate normally distributed if every linear combination of its k components has a univariate normal distribution. Its importance derives mainly from the multivariate central limit theorem. The multivariate normal distribution is often used to describe, at least approximately, any set of (possibly) correlated real-valued random variables each of which clusters around a mean value.\\n', 'Electrochemical RAM', 'Electrochemical Random-Access Memory (ECRAM) is a type of non-volatile memory (NVM) with multiple levels per cell (MLC) designed for deep learning analog acceleration.[1][2][3] An ECRAM cell is a three-terminal device composed of a conductive channel, an insulating electrolyte, an ionic reservoir, and metal contacts. The resistance of the channel is modulated by ionic exchange at the interface between the channel and the electrolyte upon application of an electric field. The charge-transfer process allows both for state retention in the absence of applied power, and for programming of multiple distinct levels, both differentiating ECRAM operation from that of a field-effect transistor (FET). The write operation is deterministic and can result in symmetrical potentiation and depression, making ECRAM arrays attractive for acting as artificial synaptic weights in physical implementations of artificial neural networks (ANN). The technological challenges include open circuit potential (OCP) and semiconductor foundry compatibility associated with energy materials. Universities, government laboratories, and corporate research teams have contributed to the development of ECRAM for analog computing. Notably, Sandia National Laboratories designed a lithium-based cell inspired by solid-state battery materials,[4] Stanford University built an organic proton-based cell,[5] and International Business Machines (IBM) demonstrated in-memory selector-free parallel programming for a logistic regression task in an array of metal-oxide ECRAM designed for insertion in the back end of line (BEOL).[6] In 2022, researchers at Massachusetts Institute of Technology built an inorganic, CMOS-compatible protonic technology that achieved near-ideal modulation characteristics using nanosecond fast pulses [7]\\n', 'Softmax function', \"The softmax function, also known as softargmax[1]:\\u200a184\\u200a or normalized exponential function,[2]:\\u200a198\\u200a converts a vector of K real numbers into a probability distribution of K possible outcomes. It is a generalization of the logistic function to multiple dimensions, and used in multinomial logistic regression. The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes, based on Luce's choice axiom.\\n\", 'White-box testing', 'White-box testing (also known as clear box testing, glass box testing, transparent box testing, and structural testing) is a method of software testing that tests internal structures or workings of an application, as opposed to its functionality (i.e. black-box testing). In white-box testing, an internal perspective of the system is used to design test cases. The tester chooses inputs to exercise paths through the code and determine the expected outputs. This is analogous to testing nodes in a circuit, e.g. in-circuit testing (ICT).\\nWhite-box testing can be applied at the unit, integration and system levels of the software testing process. Although traditional testers tended to think of white-box testing as being done at the unit level, it is used for integration and system testing more frequently today. It can test paths within a unit, paths between units during integration, and between subsystems during a system–level test. Though this method of test design can uncover many errors or problems, it has the potential to miss unimplemented parts of the specification or missing requirements. Where white-box testing is design-driven,[1] that is, driven exclusively by agreed specifications of how each component of software is required to behave (as in DO-178C and ISO 26262 processes), white-box test techniques can accomplish assessment for unimplemented or missing requirements.\\n', 'ROOT', 'ROOT is an object-oriented computer program and library developed by CERN. It was originally designed for particle physics data analysis and contains several features specific to the field, but it is also used in other applications such as astronomy and data mining.  The latest minor release is 6.28, as of 2023-02-03.[3]\\n', 'Jerome H. Friedman', 'Jerome Harold Friedman (born December 29, 1939) is an American statistician, consultant and Professor of Statistics at Stanford University, known for his contributions in the field of statistics and data mining.[2][3]\\n', 'Neuron', 'Within a nervous system, a neuron, neurone, or nerve cell is an electrically excitable cell that fires electric signals called action potentials across a neural network. Neurons communicate with other cells via synapses, which are specialized connections that commonly use minute amounts of chemical neurotransmitters to pass the electric signal from the presynaptic neuron to the target cell through the synaptic gap. \\n', 'Logical conjunction', 'In logic, mathematics and linguistics, and (\\n\\n\\n\\n∧\\n\\n\\n{\\\\displaystyle \\\\wedge }\\n\\n) is the truth-functional operator of conjunction or logical conjunction. The logical connective of this operator is typically represented as \\n\\n\\n\\n∧\\n\\n\\n{\\\\displaystyle \\\\wedge }\\n\\n[1] or \\n\\n\\n\\n&\\n\\n\\n{\\\\displaystyle \\\\&}\\n\\n or \\n\\n\\n\\nK\\n\\n\\n{\\\\displaystyle K}\\n\\n (prefix) or \\n\\n\\n\\n×\\n\\n\\n{\\\\displaystyle \\\\times }\\n\\n or \\n\\n\\n\\n⋅\\n\\n\\n{\\\\displaystyle \\\\cdot }\\n\\n[2] in which \\n\\n\\n\\n∧\\n\\n\\n{\\\\displaystyle \\\\wedge }\\n\\n is the most modern and widely used.\\n', 'Continuous production', 'Continuous production is a flow production method used to manufacture, produce, or process materials without interruption.  Continuous production is called a continuous process or a continuous flow process because the materials, either dry bulk or fluids that are being processed are continuously in motion, undergoing chemical reactions or subject to mechanical or heat treatment.  Continuous processing is contrasted with batch production.\\n', 'Tomographic reconstruction', 'Tomographic reconstruction is a type of multidimensional inverse problem where the challenge is to yield an estimate of a specific system from a finite number of projections. The mathematical basis for tomographic imaging was laid down by Johann Radon. A notable example of applications is the reconstruction of computed tomography (CT) where cross-sectional images of patients are obtained in non-invasive manner. Recent developments have seen the Radon transform and its inverse used for tasks related to realistic object insertion required for testing and evaluating computed tomography use in airport security.[1]\\n', 'Backdoor (computing)', 'A backdoor is a typically covert method of bypassing normal authentication or encryption in a computer, product, embedded device (e.g. a home router), or its embodiment (e.g. part of a cryptosystem, algorithm, chipset, or even a \"homunculus computer\"—a tiny computer-within-a-computer such as that found in Intel\\'s AMT technology).[1][2] Backdoors are most often used for securing remote access to a computer, or obtaining access to plaintext in cryptosystems. From there it may be used to gain access to privileged information like passwords, corrupt or delete data on hard drives, or transfer information within autoschediastic networks.\\n', 'Chromosome (genetic algorithm)', 'In genetic algorithms (GA), or more general, evolutionary algorithms (EA), a chromosome (also sometimes called a genotype) is a set of parameters which define a proposed solution of the problem that the evolutionary algorithm is trying to solve. The set of all solutions, also called individuals according to the biological model, is known as the population.[1][2] The genome of an individual consists of one, more rarely of several,[3][4] chromosomes and corresponds to the genetic representation of the task to be solved. A chromosome is composed of a set of genes, where a gene consists of one or more semantically connected parameters, which are often also called decision variables. They determine one or more phenotypic characteristics of the individual or at least have an influence on them.[2]  In the basic form of genetic algorithms, the chromosome is represented as a binary string,[5] while in later variants[6][7] and in EAs in general, a wide variety of other data structures are used.[8][9][10]\\n', 'File:Colored neural network.svg', 'Original file \\u200e(SVG file, nominally 296 × 356 pixels, file size: 206 KB)\\n', 'Influence diagram', 'An influence diagram (ID) (also called a relevance diagram, decision diagram or a decision network) is a compact graphical and mathematical representation of a decision situation.  It is a generalization of a Bayesian network, in which not only probabilistic inference problems but also decision making problems (following the maximum expected utility criterion) can be modeled and solved.\\n', 'Logical consequence', 'Logical consequence (also entailment) is a fundamental concept in logic which describes the relationship between statements that hold true when one statement logically follows from one or more statements. A valid logical argument is one in which the conclusion is entailed by the premises, because the conclusion is the consequence of the premises. The philosophical analysis of logical consequence involves the questions: In what sense does a conclusion follow from its premises? and What does it mean for a conclusion to be a consequence of premises?[1] All of philosophical logic is meant to provide accounts of the nature of logical consequence and the nature of logical truth.[2]\\n', 'Sparse matrix', 'In numerical analysis and scientific computing, a sparse matrix or sparse array is a matrix in which most of the elements are zero.[1] There is no strict definition regarding the proportion of zero-value elements for a matrix to qualify as sparse but a common criterion is that the number of non-zero elements is roughly equal to the number of rows or columns. By contrast, if most of the elements are non-zero, the matrix is considered dense.[1] The number of zero-valued elements divided by the total number of elements (e.g., m × n for an m × n matrix) is sometimes referred to as the sparsity of the matrix.\\n', 'Leo Breiman', 'Leo Breiman (January 27, 1928 – July 5, 2005) was a distinguished statistician at the University of California, Berkeley. He was the recipient of numerous honors and awards,[citation needed] and was a member of the United States National Academy of Sciences.\\n', 'Enterprise information system', 'An Enterprise Information System (EIS) is any kind of information system which improves the functions of enterprise business processes by integration. This means typically offering high quality of service, dealing with large volumes of data and capable of supporting some large and possibly complex organization or enterprise. An EIS must be able to be used by all parts and all levels of an enterprise.[1]\\n', 'Information security', '\\nInformation security, sometimes shortened to InfoSec,[1] is the practice of protecting information by mitigating information risks. It is part of information risk management.[2][3] It typically involves preventing or reducing the probability of unauthorized or inappropriate access to data or the unlawful use, disclosure, disruption, deletion, corruption, modification, inspection, recording, or devaluation of information.[citation needed] It also involves actions intended to reduce the adverse impacts of such incidents. Protected information may take any form, e.g., electronic or physical, tangible (e.g., paperwork), or intangible (e.g., knowledge).[4][5] Information security\\'s primary focus is the balanced protection of data confidentiality, integrity, and availability (also known as the \"CIA\" triad) while maintaining a focus on efficient policy implementation, all without hampering organization productivity.[6] This is largely achieved through a structured risk management process that involves: \\n', 'PubMed Central', \"PubMed Central (PMC) is a free digital repository that archives open access full-text scholarly articles that have been published in biomedical and life sciences journals. As one of the major research databases developed by the National Center for Biotechnology Information (NCBI), PubMed Central is more than a document repository. Submissions to PMC are indexed and formatted for enhanced metadata, medical ontology, and unique identifiers which enrich the XML structured data for each article.[1] Content within PMC can be linked to other NCBI databases and accessed via Entrez search and retrieval systems, further enhancing the public's ability to discover, read and build upon its biomedical knowledge.[2]\\n\", 'Synapse', 'In the nervous system, a synapse[1] is a structure that permits a neuron (or nerve cell) to pass an electrical or chemical signal to another neuron or to the target effector cell.\\n', 'Springer Science+Business Media', 'Springer Science+Business Media, commonly known as Springer, is a German multinational publishing company of books, e-books and peer-reviewed journals in science, humanities, technical and medical (STM) publishing.[1]\\n', 'Strong NP-completeness', 'In computational complexity, strong NP-completeness is a property of computational problems that is a special case of NP-completeness. A general computational problem may have numerical parameters.  For example, the input to the bin packing problem is a list of objects of specific sizes and a size for the bins that must contain the objects—these object sizes and bin size are numerical parameters.\\n', 'Internet fraud', 'Internet fraud is a type of cybercrime fraud or deception which makes use of the Internet and could involve hiding of information or providing incorrect information for the purpose of tricking victims out of money, property, and inheritance.[1] Internet fraud is not considered a single, distinctive crime but covers a range of illegal and illicit actions that are committed in cyberspace.[1] It is, however, differentiated from theft since, in this case, the victim voluntarily and knowingly provides the information, money or property to the perpetrator.[2] It is also distinguished by the way it involves temporally and spatially separated offenders.[3]\\n', 'Density estimation', 'In statistics, probability density estimation or simply density estimation is the construction of an estimate, based on observed data, of an unobservable underlying probability density function.  The unobservable density function is thought of as the density according to which a large population is distributed; the data are usually thought of as a random sample from that population.[1]\\n', 'Sensitivity and specificity', 'In medicine and statistics, sensitivity and specificity mathematically describe the accuracy of a test that reports the presence or absence of a medical condition. If individuals who have the condition are considered \"positive\" and those who do not are considered \"negative\", then sensitivity is a measure of how well a test can identify true positives and specificity is a measure of how well a test can identify true negatives:\\n', 'Manifold', 'In mathematics, a manifold is a topological space that locally resembles Euclidean space near each point. More precisely, an \\n\\n\\n\\nn\\n\\n\\n{\\\\displaystyle n}\\n\\n-dimensional manifold, or \\n\\n\\n\\nn\\n\\n\\n{\\\\displaystyle n}\\n\\n-manifold for short, is a topological space with the property that each point has a neighborhood that is homeomorphic to an open subset of \\n\\n\\n\\nn\\n\\n\\n{\\\\displaystyle n}\\n\\n-dimensional Euclidean space.\\n', 'Conditional independence', 'In probability theory, conditional independence describes situations wherein an observation is irrelevant or redundant when evaluating the certainty of a hypothesis. Conditional independence is usually formulated in terms of conditional probability, as a special case where the probability of the hypothesis given the uninformative observation is equal to the probability without. If \\n\\n\\n\\nA\\n\\n\\n{\\\\displaystyle A}\\n\\n is the hypothesis, and \\n\\n\\n\\nB\\n\\n\\n{\\\\displaystyle B}\\n\\n and \\n\\n\\n\\nC\\n\\n\\n{\\\\displaystyle C}\\n\\n are observations, conditional independence can be stated as an equality:\\n', 'File:Svm max sep hyperplane with margin.png', 'Original file \\u200e(800 × 862 pixels, file size: 78 KB, MIME type: image/png)\\n', 'Robert Tibshirani', 'Robert Tibshirani FRS FRSC (born July 10, 1956) is a professor in the Departments of Statistics and Biomedical Data Science at Stanford University. He was a professor at the University of Toronto from 1985 to 1998. In his work, he develops statistical tools for the analysis of complex datasets, most recently in genomics and proteomics.\\n', 'Memtransistor', 'The memtransistor (a blend word from Memory Transfer Resistor) is an experimental multi-terminal passive electronic component that might be used in the construction of artificial neural networks.[1] It is a combination of the memristor and transistor technology.[2] This technology is different from the 1T-1R approach since the devices are merged into one single entity. Multiple memristers can be embedded with a single transistor, enabling it to more accurately model a neuron with its multiple synaptic connections. A neural network produced from these would provide hardware-based artificial intelligence with a good foundation.[1][3]\\n', 'Local outlier factor', 'In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.[1]\\n', 'DBSCAN', 'Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996.[1]\\nIt is a density-based clustering non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away).\\nDBSCAN is one of the most common, and most commonly cited, clustering algorithms.[2]\\n', 'Wikipedia:Contents', '\\n', 'Category:Articles with short description', 'This category is for articles with short descriptions defined on Wikipedia by {{short description}} (either within the page itself or via another template).\\n', 'Random sample consensus', 'Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method.[1] It is a non-deterministic algorithm in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are allowed. The algorithm was first published by Fischler and Bolles at SRI International in 1981. They used RANSAC to solve the Location Determination Problem (LDP), where the goal is to determine the points in the space that project onto an image into a set of landmarks with known locations.\\n', 'Social network', 'A social network is a social structure made up of a set of social actors (such as individuals or organizations), sets of dyadic ties, and other social interactions between actors. The social network perspective provides a set of methods for analyzing the structure of whole social entities as well as a variety of theories explaining the patterns observed in these structures.[1] The study of these structures uses social network analysis to identify local and global patterns, locate influential entities, and examine network dynamics.\\n', 'Interaction design', '\\nInteraction design, often abbreviated as IxD, is \"the practice of designing interactive digital products, environments, systems, and services.\"[1]:\\u200axxvii,\\u200a30\\u200a While interaction design has an interest in form (similar to other design fields), its main area of focus rests on behavior.[1]:\\u200axxvii,\\u200a30\\u200a Rather than analyzing how things are, interaction design synthesizes and imagines things as they could be. This element of interaction design is what characterizes IxD as a design field, as opposed to a science or engineering field.[1]\\n', 'Real number', 'In mathematics, a real number is a number that can be used to measure a continuous one-dimensional quantity such as a distance, duration or temperature. Here, continuous means that pairs of values can have arbitrarily small differences.[a] Every real number can be almost uniquely represented by an infinite decimal expansion.[b][1]\\n', 'Category:Articles with unsourced statements from May 2022', 'This category combines all articles with unsourced statements from May 2022 (2022-05) to enable us to work through the backlog more systematically. It is a member of Category:Articles with unsourced statements.\\nTo add an article to this category add     {{Citation needed|date=May 2022}} to the article. If you omit the date a bot will add it for you at some point.\\n', 'Oracle Cloud', 'Oracle Cloud is a cloud computing\\xa0service offered by\\xa0Oracle Corporation\\xa0providing servers, storage, network, applications and services through a global network of Oracle Corporation managed\\xa0data centers. The company allows these services to be provisioned\\xa0on demand over the\\xa0Internet.\\n', 'Canonical correlation', 'In statistics, canonical-correlation analysis (CCA), also called canonical variates analysis, is a way of inferring information from cross-covariance matrices. If we have two vectors X\\xa0=\\xa0(X1,\\xa0...,\\xa0Xn) and Y\\xa0=\\xa0(Y1,\\xa0...,\\xa0Ym)  of random variables, and there are correlations among the variables, then canonical-correlation analysis will find linear combinations of X and Y which have maximum correlation with each other.[1] T. R. Knapp notes that \"virtually all of the commonly encountered parametric tests of significance can be treated as special cases of canonical-correlation analysis, which is the general procedure for investigating the relationships between two sets of variables.\"[2] The method was first introduced by Harold Hotelling in 1936,[3] although in the context of angles between flats the mathematical concept was published by Jordan in 1875.[4]\\n', 'Network scheduler', 'A network scheduler, also called packet scheduler, queueing discipline (qdisc) or queueing algorithm, is an arbiter on a node in a packet switching communication network. It manages the sequence of network packets in the transmit and receive queues of the protocol stack and network interface controller. There are several network schedulers available for the different operating systems, that implement many of the existing network scheduling algorithms.\\n', 'Chemical synapse', \"Chemical synapses are biological junctions through which neurons' signals can be sent to each other and to non-neuronal cells such as those in muscles or glands. Chemical synapses allow neurons to form circuits within the central nervous system. They are crucial to the biological computations that underlie perception and thought. They allow the nervous system to connect to and control other systems of the body.\\n\", 'RCASE', 'Root Cause Analysis Solver Engine (informally RCASE) is a proprietary algorithm developed from research originally at the Warwick Manufacturing Group (WMG) at Warwick University.[1][2] RCASE development commenced in 2003 to provide an automated version of root cause analysis, the method of problem solving that tries to identify the root causes of faults or problems.[3] RCASE is now owned by the spin-out company Warwick Analytics where it is being applied to automated predictive analytics software.\\n', 'Paraphrase', \"A paraphrase (/ˈpærəˌfreɪz/) is a restatement of the meaning of a text or passage using other words. The term itself is derived via Latin paraphrasis, from Ancient Greek  παράφρασις (paráphrasis)\\xa0'additional manner of expression'. The act of paraphrasing is also called paraphrasis.\\n\", 'File:AI hierarchy.svg', 'Original file \\u200e(SVG file, nominally 399 × 399 pixels, file size: 8 KB)\\n', 'The New England Journal of Medicine', 'The New England Journal of Medicine (NEJM) is a weekly medical journal published by the Massachusetts Medical Society. It is among the most prestigious peer-reviewed medical journals[1][2] as well as the oldest continuously published one.[1]\\n', 'Errors and residuals', 'In statistics and optimization, errors and residuals are two closely related and easily confused measures of the deviation of an observed value of an element of a statistical sample from its \"true value\" (not necessarily observable). The error of an observation is the deviation of the observed value from the true value of a quantity of interest (for example, a population mean). The residual is the difference between the observed value and the estimated value of the quantity of interest (for example, a sample mean). The distinction is most important in regression analysis, where the concepts are sometimes called the regression errors and regression residuals and where they lead to the concept of studentized residuals.\\nIn econometrics, \"errors\" are also called disturbances.[1][2][3]\\n', 'DeepSpeed', 'DeepSpeed is an open source deep learning optimization library for PyTorch.[1] The library is designed to reduce computing power and memory use and to train large distributed models with better parallelism on existing computer hardware.[2][3] DeepSpeed is optimized for low latency, high throughput training. It includes the Zero Redundancy Optimizer (ZeRO) for training models with 1 trillion or more parameters.[4] Features include mixed precision training, single-GPU, multi-GPU, and multi-node training as well as custom model parallelism. The DeepSpeed source code is licensed under MIT License and available on GitHub.[5]\\n', 'Pricing', \"Pricing is the process whereby a business sets the price at which it will sell its products and services, and may be part of the business's marketing plan. In setting prices, the business will take into account the price at which it could acquire the goods, the manufacturing cost, the marketplace, competition, market condition, brand, and quality of product.\\n\", 'Piecewise', 'In mathematics, a piecewise-defined function (also called a piecewise function, a hybrid function, or definition by cases) is a function defined by multiple sub-functions, where each sub-function applies to a different interval in the domain.[1][2][3] Piecewise definition is actually a way of expressing the function, rather than a characteristic of the function itself.\\n', 'Peripheral', 'A peripheral  device, or simply peripheral, is an auxiliary hardware device used to transfer information into and out of a computer.[1] The term peripheral device refers to all hardware components that are attached to a computer and are controlled by the computer system, but they are not the core components of the computer.\\n', 'Network performance', 'Network performance refers to measures of service quality of a network as seen by the customer.\\n', 'Basis function', 'In mathematics,  a basis function is an element of a particular basis for a function space. Every function in the function space can be represented as a linear combination of basis functions, just as every vector in a vector space can be represented as a linear combination of basis vectors.\\n', 'Indel', 'Indel (insertion-deletion) is a molecular biology term for an insertion or deletion of bases in the genome of an organism. Indels ≥ 50 bases in length are classified as structural variants.[1][2]\\n', 'Product placement', 'Product placement, also known as embedded marketing,[1][2][3][4] is a marketing technique where references to specific brands or products are incorporated into another work, such as a film or television program, with specific promotional intent.  Much of this is done by loaning products, especially when expensive items, such as vehicles, are involved.[5]  In 2021, the agreements between brand owners and films and television programs were worth more than US$20 billion.[5]\\n', 'Security service (telecommunication)', 'Security service is a service, provided by a layer of communicating open systems, which ensures adequate security of the systems or of data transfers[1] as defined by ITU-T X.800 Recommendation. \\nX.800 and ISO 7498-2 (Information processing systems – Open systems interconnection – Basic Reference Model – Part 2: Security architecture)[2]  are technically aligned. This model is widely recognized [3]\\n[4]\\n', 'Template:Computer science', \"This template's initial visibility currently defaults to autocollapse, meaning that if there is another collapsible item on the page (a navbox, sidebar, or table with the collapsible attribute), it is hidden apart from its title bar; if not, it is fully visible.\\n\", 'Category:Articles with GND identifiers', 'This category has only the following subcategory.\\n', 'Pan-genome', 'In the fields of molecular biology and genetics, a pan-genome (pangenome or supragenome) is the entire set of genes from all strains within a clade. More generally, it is the union of all the genomes of a clade.[2][3][4][5] The pan-genome can be broken down into a \"core pangenome\" that contains genes present in all individuals, a \"shell pangenome\" that contains genes present in two or more strains, and a \"cloud pangenome\" that contains genes only found in a single strain.[3][4][6] Some authors also refer to the cloud genome as \"accessory genome\" containing \\'dispensable\\' genes present in a subset of the strains and strain-specific genes.[2][3][4] Note that the use of the term \\'dispensable\\' has been questioned, at least in plant genomes, as accessory genes play \"an important role in genome evolution and in the complex interplay between the genome and the environment\".[5] The field of study of pangenomes is called pangenomics.[2]\\n', 'Chartered Financial Analyst', 'The Chartered Financial Analyst (CFA) program is a postgraduate professional certification offered internationally by the America based CFA Institute (formerly the Association for Investment Management and Research, or AIMR) to investment and financial professionals. The program teaches a wide range of subjects relating to advanced investment analysis—including security analysis, statistics, probability theory, fixed income, derivatives, economics, financial analysis, corporate finance, alternative investments, portfolio management—and provides a generalist knowledge of other areas of finance.\\n', 'Help:Contents', '\\n\\n', 'Financial market', 'A financial market is a market in which people trade financial securities and derivatives at low transaction costs. Some of the securities include stocks and bonds, raw materials and precious metals, which are known in the financial markets as commodities.\\n', 'File:Overfitted Data.png', 'Overfitted_Data.png \\u200e(377 × 256 pixels, file size: 14 KB, MIME type: image/png)\\n', 'False positive rate', 'In statistics, when performing multiple comparisons, a false positive ratio (also known as fall-out or false alarm ratio) is the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positives) and the total number of actual negative events (regardless of classification).\\n', 'Category:All articles with unsourced statements', '\\nThis is a category to help keep count of the total number of articles with the {{citation needed}} template.  They should all be in one of the dated categories, which can be found at Category:Articles with unsourced statements.\\n', 'File:Linear regression.svg', 'Original file \\u200e(SVG file, nominally 438 × 289 pixels, file size: 71 KB)\\n', 'Julia Angwin', 'Julia Angwin is a Pulitzer Prize-winning[1] American investigative journalist,[2] New York Times bestselling author, and entrepreneur. She was a co-founder and editor-in-chief of The Markup, a nonprofit newsroom that investigates the impact of technology on society. She was a senior reporter at ProPublica from 2014 to April 2018[3] and staff reporter at the New York bureau of The Wall Street Journal from 2000 to 2013. Angwin is author of non-fiction books, Stealing MySpace: The Battle to Control the Most Popular Website in America (2009) and Dragnet Nation (2014).[4] She is a winner and two-time finalist for the Pulitzer Prize in journalism.[5]\\n', 'Special pages', 'This page contains a list of special pages. Most of the content of these pages is automatically generated and cannot be edited. To suggest a change to the parts that can be edited, find the appropriate text on Special:AllMessages and then request your change on the talk page of the message (using {{editprotected}} to draw the attention of administrators).\\n', 'Edge device', 'In computer networking, an edge device is a device that provides an entry point into enterprise or service provider core networks. Examples include routers, routing switches, integrated access devices (IADs), multiplexers, and a variety of metropolitan area network (MAN) and wide area network (WAN) access devices.  Edge devices also provide connections into carrier and service provider networks. An edge device that connects a local area network to a high speed switch or backbone (such as an ATM switch) may be called an edge concentrator.\\n', 'Goof', 'A goof is a mistake. The term is also used in a number of specific senses: in cinema, it is an error or oversight during production that is visible in the released version of the film.\\n', 'Structural health monitoring', 'Structural health monitoring (SHM) involves the observation and analysis of a system over time using periodically sampled response measurements to monitor changes to the material and geometric properties of engineering structures such as bridges and buildings.\\n', 'Covariance function', 'In probability theory and statistics, the covariance function describes how much two random variables change together (their covariance) with varying spatial or temporal separation. For a random field or stochastic process Z(x) on a domain D, a covariance function C(x,\\xa0y) gives the covariance of the values of the random field at the two locations x and y:\\n', 'Springer Nature', \"Springer Nature or the Springer Nature Group[1][2] is a German-British academic publishing company created by the May 2015 merger of Springer Science+Business Media and Holtzbrinck Publishing Group's Nature Publishing Group, Palgrave Macmillan, and Macmillan Education.[3]\\n\", 'Wikipedia:File upload wizard', 'Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand copyright and the image use policy before proceeding.\\n', 'IEEE Spectrum', 'IEEE Spectrum is a magazine edited by the Institute of Electrical and Electronics Engineers.\\n', 'Bank fraud', 'Bank fraud is the use of potentially illegal means to obtain money, assets, or other property owned or held by a financial institution, or to obtain money from depositors by fraudulently posing as a bank or other financial institution.[1] In many instances, bank fraud is a criminal offence. While the specific elements of particular banking fraud laws vary depending on jurisdictions, the term bank fraud applies to actions that employ a scheme or artifice, as opposed to bank robbery or theft. For this reason, bank fraud is sometimes considered a white-collar crime.[2]\\n', 'Category:Short description is different from Wikidata', 'This category contains articles with short descriptions that do not match the description field on Wikidata. \\n', 'Recent changes', 'This is a list of recent changes to Wikipedia.\\n', 'Total operating characteristic', 'The total operating characteristic (TOC)  is a statistical method to compare a Boolean variable versus a rank variable. TOC can measure the ability of an index variable to diagnose either presence or absence of a characteristic. The diagnosis of presence or absence depends on whether the value of the index is above a threshold. TOC considers multiple possible thresholds. Each threshold generates a two-by-two contingency table, which contains four entries: hits, misses, false alarms, and correct rejections.[1]\\n', 'Handle System', 'The Handle System is the Corporation for National Research Initiatives\\'s proprietary registry assigning persistent identifiers, or handles, to information resources, and for resolving \"those handles into the information necessary to locate, access, and otherwise make use of the resources\".[1]\\n', 'Angoss', \"Angoss Software Corporation, headquartered in Toronto, Ontario, Canada, with offices in the United States and UK, acquired by Datawatch and now owned by Altair, was a provider of predictive analytics systems through software licensing and services. Angoss' customers represent industries including finance, insurance, mutual funds, retail, health sciences, telecom and technology. The company was founded in 1984, and publicly traded on the TSX Venture Exchange from 2008-2013 under the ticker symbol ANC.[citation needed]\\n\", 'File:SimpleBayesNetNodes.svg', 'Original file \\u200e(SVG file, nominally 246 × 128 pixels, file size: 5 KB)\\n', 'Peter E. Hart', 'Peter E. Hart (born 1941[2]) is an American computer scientist and entrepreneur. He was chairman and president of Ricoh Innovations, which he founded in 1997. He made significant contributions in the field of computer science in a series of widely cited publications from the years 1967 to 1975 while associated with the Artificial Intelligence Center of SRI International, a laboratory where he also served as director.\\n', 'Category:Webarchive template wayback links', 'The following 200 pages are in this category, out of approximately 507,488 total. This list may not reflect recent changes.\\n', 'Book sources', 'This page allows users to search multiple sources for a book given a 10- or 13-digit International Standard Book Number. Spaces and dashes in the ISBN do not matter.\\n', 'Portal:Current events', 'Edit instructions\\n', 'Proper generalized decomposition', \"The proper generalized decomposition (PGD) is an iterative numerical method for solving boundary value problems (BVPs), that is, partial differential equations constrained by a set of boundary conditions, such as the Poisson's equation or the Laplace's equation.\\n\", 'Haplotype', 'A haplotype (haploid genotype) is a group of alleles in an organism that are inherited together from a single parent.[1][2]\\n', 'Point of sale', 'The point of sale (POS) or point of purchase (POP) is the time and place at which a retail transaction is completed.  At the point of sale, the merchant calculates the amount owed by the customer, indicates that amount, may prepare an invoice for the customer (which may be a cash register printout), and indicates the options for the customer to make payment.  It is also the point at which a customer makes a payment to the merchant in exchange for goods or after provision of a service.  After receiving payment, the merchant may issue a receipt for the transaction, which is usually printed but can also be dispensed with or sent electronically.[1][2][3]\\n', 'OPTICS algorithm', \"Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based[1] clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and Jörg Sander.[2]\\nIts basic idea is similar to DBSCAN,[3] but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. To do so, the points of the database are (linearly) ordered such that spatially closest points become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster. This is represented as a dendrogram.\\n\", 'ISSN', 'An International Standard Serial Number (ISSN) is an eight-digit serial number used to uniquely identify a serial publication, such as a magazine.[1] The ISSN is especially helpful in distinguishing between serials with the same title. ISSNs are used in ordering, cataloging, interlibrary loans, and other practices in connection with serial literature.[2]\\n', 'Bibcode', 'The bibcode (also known as the refcode) is a compact identifier used by several astronomical data systems to uniquely specify literature references.\\n', 'File:CLIPS.jpg', 'Original file \\u200e(3,866 × 921 pixels, file size: 328 KB, MIME type: image/jpeg)\\n', 'Raytheon', \"The Raytheon Company was a major U.S. defense contractor and industrial corporation with manufacturing concentrations in weapons and military and commercial electronics. It was previously involved in corporate and special-mission aircraft until early 2007. Raytheon was the world's largest producer of guided missiles.[3] In April 2020, the company merged with United Technologies Corporation to form Raytheon Technologies,[4] which, since July 2023, is known as RTX Corporation.\\n\", 'Related changes', 'Enter a page name to see changes on pages linked to or from that page. (To see members of a category, enter Category:Name of category). Changes to pages on your Watchlist are shown in bold with a green bullet. See more at Help:Related changes.\\n', 'Protein primary structure', 'Protein primary structure is the linear sequence of amino acids in a peptide or protein.[1] By convention, the primary structure of a protein is reported starting from the amino-terminal (N) end to the carboxyl-terminal (C) end. Protein biosynthesis is most commonly performed by ribosomes in cells. Peptides can also be synthesized in the laboratory. Protein primary structures can be directly sequenced, or inferred from DNA sequences.\\n', 'Inger Wikström', 'Inger Wikstrom (born 11 December 1939) is a Swedish pianist, composer and conductor.\\n', 'Receiver operating characteristic', 'Sources: Fawcett (2006),[1] Piryonesi and El-Diraby (2020),[2]\\nPowers (2011),[3] Ting (2011),[4] CAWCR,[5] D. Chicco & G. Jurman (2020, 2021, 2023),[6][7][8]  Tharwat (2018).[9] Balayla (2020)[10]\\n', 'File:Computer Retro.svg', 'Original file \\u200e(SVG file, nominally 512 × 512 pixels, file size: 499 KB)\\n', 'File:Wikiquote-logo.svg', 'Original file \\u200e(SVG file, nominally 300 × 355 pixels, file size: 1,012 bytes)\\n', 'User contributions for 2001:861:4441:AB40:B52C:4689:9091:A7BF', 'No changes were found matching these criteria.\\n', 'File:Regressions sine demo.svg', 'Original file \\u200e(SVG file, nominally 900 × 450 pixels, file size: 582 KB)\\n', 'Main Page', 'Florence Petty (1\\xa0December 1870\\xa0– 18\\xa0November 1948) was a Scottish social worker, cookery writer and broadcaster. During the 1900s she undertook social work in the deprived area of Somers Town in North London, demonstrating for working-class women how to cook inexpensive and nutritious foods. Much of the instruction was done in their homes. She published cookery-related works aimed at those also involved in social work, and a cookery book and pamphlet aimed at the public. From 1914 until the mid-1940s she toured Britain giving lecture-demonstrations of cost-efficient and nutritious ways to cook, including dealing with food shortages during the First World War. In the late 1920s and early 1930s, she was a BBC broadcaster on food and budgeting. Petty worked until she was in her seventies. She is considered to be a pioneer of social work innovations. Her approach to teaching the use of cheap nutritious food was a precursor to the method adopted by the Ministry of Food during the Second World War. (Full\\xa0article...)\\n', 'User talk:2001:861:4441:AB40:B52C:4689:9091:A7BF', 'People on Wikipedia can use this talk page to post a public message about edits made from the IP address you are currently using.\\n', 'File:Symbol portal class.svg', 'Original file \\u200e(SVG file, nominally 180 × 185 pixels, file size: 12 KB)\\n']\n"
     ]
    }
   ],
   "source": [
    "# Create the corpus by concatenating the title and the paragraph of each article\n",
    "corpus = []\n",
    "for wiki in wiki_list:\n",
    "    corpus.append(wiki['title'])\n",
    "    corpus.append(wiki['paragraph'])\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1152x5335 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19867 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the vectorizer to the corpus\n",
    "vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector for the main article\n",
    "wiki_main['vector'] = vectorizer.transform([wiki_main['title'] + wiki_main['paragraph']]).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance between the main article and each article in wiki_list using the dot product of their vectors\n",
    "for wiki in wiki_list:\n",
    "    wiki['vector'] = vectorizer.transform([wiki['title'] + wiki['paragraph']]).toarray()[0]\n",
    "    wiki['distance_vector'] = np.dot(wiki_main['vector'], wiki['vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the wiki_list by distance to the main article\n",
    "wiki_list.sort(key=lambda x: x['distance_vector'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Online_machine_learning\n",
      "In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Meta-learning_(computer_science)\n",
      "Meta learning[1][2]\n",
      "is a subfield of machine learning where automatic learning algorithms are applied to metadata about machine learning experiments. As of 2017, the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to learn.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Manifold_regularization\n",
      "In machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is smooth: data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence\n",
      "The philosophy of artificial intelligence is a branch of the philosophy of mind and the philosophy of computer science[1] that explores artificial intelligence and its implications for knowledge and understanding of intelligence, ethics, consciousness, epistemology, and free will.[2][3] Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; see artificial life) so the discipline is of considerable interest to philosophers.[4] These factors contributed to the emergence of the philosophy of artificial intelligence. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Progress_in_artificial_intelligence\n",
      "Progress in artificial intelligence (AI) refers to the advances, milestones, and breakthroughs that have been achieved in the field of artificial intelligence over time. AI is a multidisciplinary branch of computer science that aims to create machines and systems capable of performing tasks that typically require human intelligence. Artificial intelligence applications have been used in a wide range of fields including medical diagnosis, economic-financial applications, robot control, law, scientific discovery, video games, and toys. However, many AI applications are not perceived as AI:  \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[1][2] \"Many thousands of AI applications are deeply embedded in the infrastructure of every industry.\"[3] In the late 1990s and early 21st century, AI technology became widely used as elements of larger systems,[3][4] but the field was rarely credited for these successes at the time.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Statistical_learning_theory\n",
      "Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis.[1][2][3] Statistical learning theory deals with the statistical inference problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ensemble_learning\n",
      "In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.[1][2][3]\n",
      "Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Machine_learning_in_physics\n",
      "Applying classical methods of machine learning to the study of quantum systems is the focus of an emergent area of physics research. A basic example of this is quantum state tomography, where a quantum state is learned from measurement.[1] Other examples include learning Hamiltonians,[2][3] learning quantum phase transitions,[4][5] and automatically generating new quantum experiments.[6][7][8][9] Classical machine learning is effective at processing large amounts of experimental or calculated data in order to characterize an unknown quantum system, making its application useful in contexts including quantum information theory, quantum technologies development, and computational materials design. In this context, it can be used for example as a tool to interpolate pre-calculated interatomic potentials[10] or directly solving the Schrödinger equation with a variational method.[11]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Artificial_neuron\n",
      "An artificial neuron is a mathematical function conceived as a model of biological neurons in a neural network. Artificial neurons are the elementary units of artificial neural networks.[1] The artificial neuron receives one or more inputs (representing excitatory postsynaptic potentials and inhibitory postsynaptic potentials at neural dendrites) and sums them to produce an output (or activation, representing a neuron's action potential which is transmitted along its axon). Usually, each input is separately weighted (representing the synaptic weight), and the sum is often added to a term known as a bias (loosely corresponding to the threshold potential), before being passed through a non-linear function known as an activation function or transfer function[clarification needed]. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often monotonically increasing, continuous, differentiable and bounded. Non-monotonic, unbounded and oscillating activation functions with multiple zeros that outperform sigmoidal and ReLU-like activation functions on many tasks have also been recently explored. The thresholding function has inspired building logic gates referred to as threshold logic; applicable to building logic circuits resembling brain processing. For example, new devices such as memristors have been extensively used to develop such logic in recent times.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Fairness_(machine_learning)\n",
      "Fairness in machine learning refers to the various attempts at correcting algorithmic bias in automated decision processes based on machine learning models. Decisions made by computers after a machine-learning process may be considered unfair if they were based on variables considered sensitive. Examples of these kinds of variable include gender, ethnicity, sexual orientation, disability and more. As it is the case with many ethical concepts, definitions of fairness and bias are always controversial. In general, fairness and bias are considered relevant when the decision process impacts people's lives. In machine learning, the problem of algorithmic bias is well known and well studied. Outcomes may be skewed by a range of factors and thus might be considered unfair with respect to certain groups or individuals. An example would be the way social media sites deliver personalized news to consumers.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Recurrent_neural_network\n",
      "A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs[1][2][3] makes them applicable to tasks such as unsegmented, connected handwriting recognition[4] or speech recognition.[5][6] The term \"recurrent neural network\" is used to refer to the class of networks with an infinite impulse response, whereas \"convolutional neural network\" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior.[7] A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Rule-based_machine_learning\n",
      "Rule-based machine learning (RBML) is a term in computer science intended to encompass any machine learning method that identifies, learns, or evolves 'rules' to store, manipulate or apply.[1][2][3] The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[clarification needed][citation needed]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Adversarial_machine_learning\n",
      "Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks.[1] A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Natural_language_processing\n",
      "Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Artificial_intelligence_in_industry\n",
      "Industrial artificial intelligence, or industrial AI, usually refers to the application of artificial intelligence to industry. Unlike general artificial intelligence which is a frontier research discipline to build computerized systems that perform tasks requiring human intelligence, industrial AI is more concerned with the application of such technologies to address industrial pain-points for customer value creation, productivity improvement, cost reduction, site optimization, predictive analysis[1]  and insight discovery.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Machine_ethics\n",
      "Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence, otherwise known as artificial intelligent agents.[1] Machine ethics differs from other ethical fields related to engineering and technology.  Machine ethics should not be confused with computer ethics, which focuses on human use of computers. It should also be distinguished from the philosophy of technology, which concerns itself with the grander social effects of technology.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/AAAI_Conference_on_Artificial_Intelligence\n",
      "The AAAI Conference on Artificial Intelligence (AAAI) is one of the leading international academic conference in artificial intelligence held annually.[1][2][3] Along with ICML, NeurIPS and ICLR, it is one of the primary conferences of high impact in machine learning and artificial intelligence research.[4] It is supported by the Association for the Advancement of Artificial Intelligence. Precise dates vary from year to year, but paper submissions are generally due at the end of August to beginning of September, and the conference is generally held during the following February. The first AAAI was held in 1980 at Stanford University, Stanford California.[5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Reinforcement_learning\n",
      "Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\n",
      "\n",
      "https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning\n",
      "The International Conference on Machine Learning (ICML) is the leading international academic conference in machine learning. Along with NeurIPS and ICLR, it is one of the three primary conferences of high impact in machine learning and artificial intelligence research.[1] It is supported by the (IMLS). Precise dates vary year to year, but paper submissions are generally due at the end of January, and the conference is generally held the following July. The first ICML was held 1980 in Pittsburgh.[2][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Predictive_analytics\n",
      "Predictive analytics is a form of business analytics applying machine learning to generate a predictive model for certain business applications. As such, it encompasses a variety of statistical techniques from predictive modeling and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events.[1] It represents a major subset of machine learning applications; in some contexts, it is synonymous with machine learning.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Pattern_recognition\n",
      "Pattern recognition is the automated recognition of patterns and regularities in data. While similar, pattern recognition (PR) is not to be confused with pattern machines (PM) which may possess (PR) capabilities but their primary function is to distinguish and create emergent  pattern.   PR has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_learning_theory\n",
      "In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Multi-task_learning\n",
      "Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately.[1][2][3] Early versions of MTL were called \"hints\".[4][5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Boosting_(machine_learning)\n",
      "In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance[1] in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones.[2] Boosting is based on the question posed by Kearns and Valiant (1988, 1989):[3][4] \"Can a set of weak learners create a single strong learner?\" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Learning_classifier_system\n",
      "Learning classifier systems, or LCS, are a paradigm of rule-based machine learning methods that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning).[2]  Learning classifier systems seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions (e.g. behavior modeling,[3] classification,[4][5] data mining,[5][6][7] regression,[8] function approximation,[9] or game strategy).  This approach allows complex solution spaces to be broken up into smaller, simpler parts.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Kernel_machines\n",
      "In machine learning, kernel machines are a class of algorithms for pattern analysis, whose best known member is the support-vector machine (SVM). These methods involve using linear classifiers to solve nonlinear problems.[1] The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over all pairs of data points computed using inner products. The feature map in kernel machines is infinite dimensional but only requires a finite dimensional matrix from user-input according to the Representer theorem. Kernel machines are slow to compute for datasets larger than a couple of thousand examples without parallel processing. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Support_vector_machine\n",
      "In machine learning, support vector machines (SVMs, also support vector networks[1]) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995,[1] Vapnik et al., 1997[citation needed]) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n",
      "\n",
      "https://en.wikipedia.org/wiki/ML.NET\n",
      "ML.NET is a free software machine learning library for the C# and F# programming languages.[4][5][6] It also supports Python models when used together with NimbusML. The preview release of ML.NET included transforms for feature engineering like n-gram creation, and learners to handle binary classification, multi-class classification, and regression tasks.[7] Additional ML tasks like anomaly detection and recommendation systems have since been added, and other approaches like deep learning will be included in future versions.[8][9]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Automated_decision-making\n",
      "Automated decision-making (ADM) involves the use of data, machines and algorithms to make decisions in a range of contexts, including public administration, business, health, education, law, employment, transport, media and entertainment, with varying degrees of human oversight or intervention. ADM involves large-scale data from a range of sources, such as databases, text, social media, sensors, images or speech, that is processed using various technologies including computer software, algorithms, machine learning, natural language processing, artificial intelligence, augmented intelligence and robotics. The increasing use of automated decision-making systems (ADMS) across a range of contexts presents many benefits and challenges to human society requiring consideration of the technical, legal, ethical, societal, educational, economic and health consequences.[1][2][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/LightGBM\n",
      "LightGBM, short for light gradient-boosting machine, is a free and open-source distributed gradient-boosting framework for machine learning, originally developed by Microsoft.[4][5] It is based on decision tree algorithms and used for ranking, classification and other machine learning tasks. The development focus is on performance and scalability.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Speech_recognition\n",
      "\n",
      "Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.\n",
      "\n",
      "https://en.wikipedia.org/wiki/NeuroSolutions\n",
      "NeuroSolutions is a neural network development environment developed by NeuroDimension. It combines a modular, icon-based (component-based) network design interface with an implementation of advanced learning procedures, such as conjugate gradients, the Levenberg-Marquardt algorithm, and backpropagation through time.[citation needed] The software is used to design, train and deploy neural network (supervised learning and unsupervised learning) models to perform a wide variety of tasks such as data mining, classification, function approximation, multivariate regression and time-series prediction.[citation needed]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Robot_learning\n",
      "Robot learning is a research field at the intersection of machine learning and robotics. It studies techniques allowing a robot to acquire novel skills or adapt to its environment through learning algorithms. The embodiment of the robot, situated in a physical embedding, provides at the same time specific difficulties (e.g. high-dimensionality, real time constraints for collecting data and learning) and opportunities for guiding the learning process (e.g. sensorimotor synergies, motor primitives).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ensemble_Averaging\n",
      "In machine learning, particularly in the creation of artificial neural networks, ensemble averaging is the process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models \"average out.\"\n",
      "\n",
      "https://en.wikipedia.org/wiki/Large_language_model\n",
      "A large language model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by using massive amounts of data to learn billions of parameters during training and consuming large computational resources during their training and operation.[1] LLMs are artificial neural networks (mainly transformers[2]) and are (pre-)trained using self-supervised learning and semi-supervised learning.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Artificial_immune_system\n",
      "In artificial intelligence, artificial immune systems (AIS) are a class of computationally intelligent, rule-based machine learning systems inspired by the principles and processes of the vertebrate immune system. The algorithms are typically modeled after the immune system's characteristics of learning and memory for use in problem-solving.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Semantic_analysis_(machine_learning)\n",
      "In machine learning, semantic analysis of a corpus is the task of building structures that approximate concepts from a large set of documents. It generally does not involve prior semantic understanding of the documents. A metalanguage based on predicate logic can analyze the speech of humans.[1]: 93-  Another strategy to understand the semantics of a text is symbol grounding. If language is grounded, it is equal to recognizing a machine readable meaning. For the restricted domain of spatial analysis, a computer based language understanding system was demonstrated.[2]: 123 \n",
      "\n",
      "https://en.wikipedia.org/wiki/Semi-supervised_learning\n",
      "Weak supervision is a paradigm in machine learning, the relevance and notability of which increased with the advent of large language models due to large amount of data required to train them.  It is characterized by using a combination of a small amount of human-labeled data (exclusively used in more expensive and time-consuming supervised learning paradigm), followed by a large amount of unlabeled data (used exclusively in unsupervised learning paradigm). In other words, the desired output values are provided only for a subset of the training data. The remaining data is unlabeled or imprecisely labeled. Intuitively, it can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam. Technically, it could be viewed as performing clustering and then labeling the clusters with the labeled data, pushing the decision boundary away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Deep_learning_speech_synthesis\n",
      "Deep learning speech synthesis uses Deep Neural Networks (DNN) to produce\n",
      "artificial speech from text (text-to-speech) or spectrum (vocoder).\n",
      "The deep neural networks are trained using a large amount of recorded speech and, in the case of\n",
      "a text-to-speech system, the associated labels and/or input text.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Torch_(machine_learning)\n",
      "Torch is an open-source machine learning library, \n",
      "a scientific computing framework, and a scripting language based on Lua.[3] It provides LuaJIT interfaces to deep learning algorithms implemented in C. It was created at IDIAP at EPFL. Torch development moved in 2017 to PyTorch, a port of the library to Python.[4][5][better source needed]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Diffusion_model\n",
      "In machine learning, diffusion models, also known as diffusion probabilistic models or score-based generative models, are a class of generative models. The goal of diffusion models is to learn a diffusion process that generates the probability distribution of a given dataset. It mainly consists of three major components: the forward process, the reverse process, and the sampling procedure.[1] Three examples of generic diffusion modeling frameworks used in computer vision are denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Deep_learning\n",
      "Deep learning is the subset of machine learning methods which are based on artificial neural networks with representation learning. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Machine_learning_control\n",
      "Machine learning control (MLC) is a subfield of machine learning, intelligent control and control theory\n",
      "which solves optimal control problems with methods of machine learning.\n",
      "Key applications are complex nonlinear systems\n",
      "for which linear control theory methods are not applicable.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Active_learning_(machine_learning)\n",
      "Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs.[1][2][3] In statistics literature, it is sometimes also called optimal experimental design.[4] The information source is also called teacher or oracle.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Self-supervised_learning\n",
      "Self-supervised learning (SSL) is a paradigm in machine learning where a model is trained on a task using the data itself to generate supervisory signals, rather than relying on external labels provided by humans. In the context of neural networks, self-supervised learning aims to leverage inherent structures or relationships within the input data to create meaningful training signals. SSL tasks are designed so that solving it requires capturing essential features or relationships in the data. The input data is typically augmented or transformed in a way that creates pairs of related samples. One sample serves as the input, and the other is used to formulate the supervisory signal. This augmentation can involve introducing noise, cropping, rotation, or other transformations. Self-supervised learning more closely imitates the way humans learn to classify objects.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Federated_learning\n",
      "Federated learning (also known as collaborative learning) is a machine learning technique that trains an algorithm via multiple independent sessions, each using its own dataset. This approach stands in contrast to traditional centralized machine learning techniques where local datasets are merged into one training session, as well as to approaches that assume that local data samples are identically distributed.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Feature_vector\n",
      "In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon.[1] Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of \"feature\" is related to that of explanatory variable used in statistical techniques such as linear regression.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Meta_AI\n",
      "Meta AI is an artificial intelligence laboratory that belongs to Meta Platforms Inc. (formerly known as Facebook, Inc.)[1] Meta AI intends to develop various forms of artificial intelligence, improving augmented and artificial reality technologies.[2] Meta AI is an academic research laboratory focused on generating knowledge for the AI community.[3] This is in contrast to Facebook's Applied Machine Learning (AML) team, which focuses on practical applications of its products.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach\n",
      "Artificial Intelligence: A Modern Approach (AIMA) is a university textbook on artificial intelligence, written by Stuart J. Russell and Peter Norvig. It was first published in 1995 and the fourth edition of the book was released on 28 April 2020.[1] It is used in over 1400 universities worldwide[2] and has been called \"the most popular artificial intelligence textbook in the world\".[3] It is considered the standard text in the field of artificial intelligence.[4][5]\n",
      "The book is intended for an undergraduate audience but can also be used for graduate-level studies with the suggestion of adding some of the primary sources listed in the extensive bibliography.  Programs in the book are presented in pseudo code with implementations in Java, Python, Lisp, JavaScript and Scala available online.[6][7] There are also unsupported implementations in Prolog, C++, C#, and several other languages.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Physical_neural_network\n",
      "A physical neural network is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse or a higher-order (dendritic) neuron model.[1] \"Physical\" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches. More generally the term is applicable to other artificial neural networks in which a memristor or other electrically adjustable resistance material is used to emulate a neural synapse.[2][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Feature_learning\n",
      "In machine learning, feature learning or representation learning[2] is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform  a specific task.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Grammar_induction\n",
      "Grammar induction (or grammatical inference)[1] is the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Statistical_learning_in_language_acquisition\n",
      "Statistical learning is the ability for humans and other animals to extract statistical regularities from the world around them to learn about the environment. Although statistical learning is now thought to be a generalized learning mechanism, the phenomenon was first identified in human infant language acquisition.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Apprenticeship_learning\n",
      "In artificial intelligence, apprenticeship learning (or learning from demonstration or imitation learning) is the process of learning by observing an expert.[1][2] It can be viewed as a form of supervised learning, where the training dataset consists of task executions by a demonstration teacher.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Arthur_Samuel_(computer_scientist)\n",
      "Arthur Lee Samuel (December 5, 1901 – July 29, 1990)[3] was an American pioneer in the field of computer gaming and artificial intelligence.[2] He popularized the term \"machine learning\" in 1959.[4] The Samuel Checkers-playing Program was among the world's first successful self-learning programs, and as such a very early demonstration of the fundamental concept of artificial intelligence (AI).[5] He was also a senior member in the TeX community who devoted much time giving personal attention to the needs of users and wrote an early TeX manual in 1983.[6]\n",
      "\n",
      "https://en.wikipedia.org/wiki/S2CID_(identifier)\n",
      "Semantic Scholar is a research tool powered by artificial intelligence for scientific literature. It was developed at the Allen Institute for AI and publicly released in November 2015.[2] It uses advances in natural language processing to provide summaries for scholarly papers.[3] The Semantic Scholar team is actively researching the use of artificial intelligence in natural language processing, machine learning, human–computer interaction, and information retrieval.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Knowledge_graph_embedding\n",
      "In representation learning, knowledge graph embedding (KGE), also referred to as knowledge representation learning (KRL), or multi-relation learning,[1] is a machine learning task of learning a low-dimensional representation of a knowledge graph's entities and relations while preserving their semantic meaning.[1][2][3]  Leveraging their embedded representation, knowledge graphs (KGs) can be used for various applications such as link prediction, triple classification, entity recognition, clustering, and relation extraction.[1][4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Distributed_artificial_intelligence\n",
      "Distributed Artificial Intelligence (DAI) also called Decentralized Artificial Intelligence[1] is a subfield of artificial intelligence research dedicated to the development of distributed solutions for problems. DAI is closely related to and a predecessor of the field of multi-agent systems. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Kubeflow\n",
      "Kubeflow is an open-source platform for machine learning and MLOps on Kubernetes introduced by Google. The different stages in a typical machine learning lifecycle are represented with different software components in Kubeflow, including model development (Kubeflow Notebooks[4]), model training (Kubeflow Pipelines,[5] Kubeflow Training Operator[6]), model serving (KServe[a][7]), and automated machine learning (Katib[8]).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Trevor_Hastie\n",
      "Trevor John Hastie (born 27 June 1953) is an American statistician and computer scientist. He is currently serving as the John A. Overdeck Professor of Mathematical Sciences and Professor of Statistics at Stanford University.[1] Hastie is known for his contributions to applied statistics, especially in the field of machine learning, data mining, and bioinformatics. He has authored several popular books in statistical learning, including The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Hastie has been listed as an ISI Highly Cited Author in Mathematics by the ISI Web of Knowledge.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Automated_machine_learning\n",
      "Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Artificial_intelligence_in_healthcare\n",
      "Artificial intelligence in healthcare is a term used to describe the use of machine-learning algorithms and software, or artificial intelligence (AI), to copy human cognition in the analysis, presentation, and understanding of complex medical and health care data, or to exceed human capabilities by providing new ways to diagnose, treat, or prevent disease.[1][2] Specifically, AI is the ability of computer algorithms to approximate conclusions based solely on input data.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Quantum_machine_learning\n",
      "Quantum machine learning is the integration of quantum algorithms within machine learning programs.[1][2][3][4][5][6][7][8]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Unsupervised_learning\n",
      "Unsupervised learning is a paradigm in machine learning where, in contrast to supervised learning and semi-supervised learning, algorithms learn patterns exclusively from unlabeled data.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Generalization_(learning)\n",
      "Generalization is the concept that humans, other animals, and artificial neural networks use past learning in present situations of learning if the conditions in the situations are regarded as similar.[1] The learner uses generalized patterns, principles, and other similarities between past experiences and novel experiences to more efficiently navigate the world.[2] For example, if a person has learned in the past that every time they eat an apple, their throat becomes itchy and swollen, they might assume they are allergic to all fruit. When this person is offered a banana to eat, they reject it upon assuming they are also allergic to it through generalizing that all fruits cause the same reaction. Although this generalization about being allergic to all fruit based on experiences with one fruit could be correct in some cases, it may not be correct in all. Both positive and negative effects have been shown in education through learned generalization and its contrasting notion of discrimination learning.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Nature_Machine_Intelligence\n",
      "Nature Machine Intelligence is a monthly peer-reviewed scientific journal published by Nature Portfolio covering machine learning and artificial intelligence. The editor-in-chief is Liesbeth Venema.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research\n",
      "The Journal of Machine Learning Research is a peer-reviewed open access scientific journal covering machine learning. It was established in 2000 and the first editor-in-chief was Leslie Kaelbling.[1] The current editors-in-chief are Francis Bach (Inria) and David Blei (Columbia University).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Learning_curve_(machine_learning)\n",
      "In machine learning, a learning curve (or training curve) plots the optimal value of a model's loss function for a training set against this loss function evaluated on a validation data set with same parameters as produced the optimal function.[1] Synonyms include error curve, experience curve, improvement curve and generalization curve.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Differentiable_programming\n",
      "Differentiable programming is a programming paradigm in which a numeric computer program can be differentiated throughout via automatic differentiation.[1][2][3][4][5] This allows for gradient-based optimization of parameters in the program, often via gradient descent, as well as other learning approaches that are based on higher order derivative information. Differentiable programming has found use in a wide variety of areas, particularly scientific computing and artificial intelligence.[5] One of the early proposals to adopt such a framework in a systematic fashion to improve upon learning algorithms was made by the Advanced Concepts Team at the European Space Agency in early 2016.[6]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_engineering\n",
      "Computational Engineering is an emerging discipline that deals with the development and application of computational models for engineering, known as Computational Engineering Models[1] or CEM. Computational engineering uses computers to solve engineering design problems important to a variety of industries.[2] At this time, various different approaches are summarized under the term Computational Engineering, including using computational geometry and virtual design for engineering tasks,[3][4] often coupled with a simulation-driven approach[5] In Computational Engineering, algorithms solve mathematical and logical models[6] that describe engineering challenges, sometimes coupled with some aspect of AI, specifically Reinforcement Learning.[7]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Portal:Computer_programming\n",
      "Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks. It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages. Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit. Proficient programming usually requires expertise in several different subjects, including knowledge of the application domain, details of programming languages and generic code libraries, specialized algorithms, and formal logic.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
      "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.\n",
      "\n",
      "https://en.wikipedia.org/wiki/International_Joint_Conference_on_Artificial_Intelligence\n",
      "The International Joint Conference on Artificial Intelligence (IJCAI) is the leading conference in the field of artificial intelligence. The conference series has been organized by the nonprofit IJCAI Organization since 1969, making it the oldest premier AI conference series in the world.[1] It was held biennially in odd-numbered years from 1969 to 2015 and annually starting from 2016. More recently, IJCAI was held jointly every four years with ECAI since 2018 and PRICAI since 2020 to promote collaboration of AI researchers and practitioners. IJCAI covers a broad range of research areas in the field of AI. It is a large and highly selective conference, with only about 20% or less of the submitted papers accepted after peer review in the 5 years leading up to 2022.[2] A lower acceptance rate usually means better quality papers and a higher reputation conference.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Residual_neural_network\n",
      "A Residual Neural Network (a.k.a. Residual Network, ResNet)[1] is a deep learning model in which the weight layers learn residual functions with reference to the layer inputs. A Residual Network[1] is a network with skip connections that perform identity mappings, merged with the layer outputs by addition. It behaves like a Highway Network[2] whose gates are opened through strongly positive bias weights. This enables deep learning models with tens or hundreds of layers to train easily and approach better accuracy when going deeper. The identity skip connections, often referred to as \"residual connections\", are also used in the 1997 LSTM networks,[3] Transformer models (e.g., BERT, GPT models such as ChatGPT), the AlphaGo Zero system, the AlphaStar system, and the AlphaFold system.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Multimodal_learning\n",
      "Multimodal learning, in context of machine learning, is deep learning from a combination of various modalities of data, often arising in real-world applications. An example of multi-modal data is data that combines text (typically represented as feature vector) with imaging data consisting of pixel intensities and annotation tags. As these modalities have fundamentally different statistical properties, combining them is non-trivial, which is why specialized modelling strategies and algorithms are required.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_linguistics\n",
      "Computational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon linguistics, computer science, artificial intelligence, mathematics, logic, philosophy, cognitive science, cognitive psychology, psycholinguistics, anthropology and neuroscience, among others.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence\n",
      "In artificial intelligence, symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search.[1] Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems),  symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to seminal ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Artificial_intelligence_in_government\n",
      "Artificial intelligence (AI) has a range of uses in government. It can be used to further public policy objectives (in areas such as emergency services, health and welfare), as well as assist the public to interact with the government  (through the use of virtual assistants, for example). According to the Harvard Business Review, \"Applications of artificial intelligence to the public sector are broad and growing, with early experiments taking place around the world.\"[1] Hila Mehr from the Ash Center for Democratic Governance and Innovation at Harvard University notes that AI in government is not new, with postal services using machine methods in the late 1990s to recognise handwriting on envelopes to automatically route letters.[2] The use of AI in government comes with significant benefits, including efficiencies resulting in cost savings (for instance by reducing the number of front office staff), and reducing the opportunities for corruption.[3] However, it also carries risks.[citation needed][further explanation needed]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\n",
      "A transformer is a deep learning architecture, initially proposed in 2017, that relies on the parallel multi-head attention mechanism.[1] It is notable for requiring less training time than previous recurrent neural architectures, such as long short-term memory (LSTM),[2] and its later variation has been prevalently adopted for training large language models on large (language) datasets, such as the Wikipedia corpus and Common Crawl, by virtue of the parallelized processing of input sequence.[3]\n",
      "Input text is split into n-grams encoded as tokens and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished. Though the transformer paper was published in 2017, the softmax-based attention mechanism was proposed in 2014 for machine translation,[4][5] and the Fast Weight Controller, similar to a transformer, was proposed in 1992.[6][7][8]\n",
      "\n",
      "https://en.wikipedia.org/wiki/MIT_Computer_Science_and_Artificial_Intelligence_Laboratory\n",
      "Computer Science and Artificial Intelligence Laboratory (CSAIL) is a research institute at the Massachusetts Institute of Technology (MIT) formed by the 2003 merger of the Laboratory for Computer Science (LCS) and the Artificial Intelligence Laboratory (AI Lab). Housed within the Ray and Maria Stata Center, CSAIL is the largest on-campus laboratory as measured by research scope and membership. It is part of the Schwarzman College of Computing[1] but is also overseen by the MIT Vice President of Research.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Feedforward_neural_network\n",
      "A feedforward neural network (FNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers.[2] Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops,[2] in contrast to recurrent neural networks,[3] which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method[4][5][6][7][8] and are colloquially referred to as the \"vanilla\" neural networks.[9]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computer_graphics\n",
      "Computer graphics deals with generating images and art with the aid of computers. Today, computer graphics is a core technology in digital photography, film, video games, digital art, cell phone and computer displays, and many specialized applications. A great deal of specialized hardware and software has been developed, with the displays of most devices being driven by computer graphics hardware. It is a vast and recently developed area of computer science. The phrase was coined in 1960 by computer graphics researchers Verne Hudson and William Fetter of Boeing. It is often abbreviated as CG, or typically in the context of film as computer generated imagery (CGI). The non-artistic aspects of computer graphics are the subject of computer science research.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Deeplearning4j\n",
      "Eclipse Deeplearning4j is a programming library written in Java for the Java virtual machine (JVM).[2][3] It is a framework with wide support for deep learning algorithms.[4] Deeplearning4j includes implementations of the restricted Boltzmann machine, deep belief net, deep autoencoder, stacked denoising autoencoder and recursive neural tensor network, word2vec, doc2vec, and GloVe. These algorithms all include distributed parallel versions that integrate with Apache Hadoop and Spark.[5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ray_Solomonoff\n",
      "Ray Solomonoff (July 25, 1926 – December 7, 2009)[1][2] was the inventor of algorithmic probability,[3] his General Theory of Inductive Inference (also known as Universal Inductive Inference),[4] and was a founder of algorithmic information theory.[5] He was an originator of the branch of artificial intelligence based on machine learning, prediction and probability. He circulated the first report on non-semantic machine learning in 1956.[6]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Electrochemical_RAM\n",
      "Electrochemical Random-Access Memory (ECRAM) is a type of non-volatile memory (NVM) with multiple levels per cell (MLC) designed for deep learning analog acceleration.[1][2][3] An ECRAM cell is a three-terminal device composed of a conductive channel, an insulating electrolyte, an ionic reservoir, and metal contacts. The resistance of the channel is modulated by ionic exchange at the interface between the channel and the electrolyte upon application of an electric field. The charge-transfer process allows both for state retention in the absence of applied power, and for programming of multiple distinct levels, both differentiating ECRAM operation from that of a field-effect transistor (FET). The write operation is deterministic and can result in symmetrical potentiation and depression, making ECRAM arrays attractive for acting as artificial synaptic weights in physical implementations of artificial neural networks (ANN). The technological challenges include open circuit potential (OCP) and semiconductor foundry compatibility associated with energy materials. Universities, government laboratories, and corporate research teams have contributed to the development of ECRAM for analog computing. Notably, Sandia National Laboratories designed a lithium-based cell inspired by solid-state battery materials,[4] Stanford University built an organic proton-based cell,[5] and International Business Machines (IBM) demonstrated in-memory selector-free parallel programming for a logistic regression task in an array of metal-oxide ECRAM designed for insertion in the back end of line (BEOL).[6] In 2022, researchers at Massachusetts Institute of Technology built an inorganic, CMOS-compatible protonic technology that achieved near-ideal modulation characteristics using nanosecond fast pulses [7]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Supervised_learning\n",
      "Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data on expected output values.[1]  An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Explainable_artificial_intelligence\n",
      "Explainable AI (XAI), often overlapping with Interpretable AI, or Explainable Machine Learning (XML), either refers to an AI system over which it is possible for humans to retain intellectual oversight, or to the methods to achieve this.[1] The main focus is usually on the reasoning behind the decisions or predictions made by the AI[2] which are made more understandable and transparent.[3] XAI counters the \"black box\" tendency of machine learning, where even the AI's designers cannot explain why it arrived at a specific decision.[4][5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Neural_machine_translation\n",
      "Neural machine translation (NMT) is an approach to machine translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modeling and then translating entire sentences in a single integrated model.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Affective_computing\n",
      "Affective computing is the study and development of systems and devices that can recognize, interpret, process, and simulate human affects. It is an interdisciplinary field spanning computer science, psychology, and cognitive science.[1] While some core ideas in the field may be traced as far back as to early philosophical inquiries into emotion,[2] the more modern branch of computer science originated with Rosalind Picard's 1995 paper[3] on affective computing and her book Affective Computing[4] published by MIT Press.[5][6] One of the motivations for the research is the ability to give machines emotional intelligence, including to simulate empathy. The machine should interpret the emotional state of humans and adapt its behavior to them, giving an appropriate response to those emotions.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Neural_Designer\n",
      "Neural Designer is a software tool for machine learning based on neural networks, a main area of artificial intelligence research, and contains a graphical user interface which simplifies data entry and interpretation of results.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mlpack\n",
      "mlpack is a machine learning software library for C++, built on top of the Armadillo library and the ensmallen numerical optimization library.[3] mlpack has an emphasis on scalability, speed, and ease-of-use. Its aim is to make machine learning possible for novice users by means of a simple, consistent API, while simultaneously exploiting C++ language features to provide maximum performance and maximum flexibility for expert users.[4] Its intended target users are scientists and engineers.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Long_short-term_memory\n",
      "Long short-term memory (LSTM)[1] network is a recurrent neural network (RNN), aimed to deal with the vanishing gradient problem[2] present in traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus \"long short-term memory\".[1] It is applicable to classification, processing and predicting data based on time series, such as in handwriting,[3] speech recognition,[4][5] machine translation,[6][7] speech activity detection,[8] robot control,[9][10] video games,[11][12] and healthcare.[13]\n",
      "\n",
      "https://en.wikipedia.org/wiki/MuZero\n",
      "MuZero is a computer program developed by artificial intelligence research company DeepMind to master games without knowing their rules.[1][2][3] Its release in 2019 included benchmarks of its performance in go, chess, shogi, and a standard suite of Atari games. The algorithm uses an approach similar to AlphaZero. It matched AlphaZero's performance in chess and shogi, improved on its performance in Go (setting a new world record), and improved on the state of the art in mastering a suite of 57 Atari games (the Arcade Learning Environment), a visually-complex domain.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_geometry\n",
      "Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with a history stretching back to antiquity.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Shogun_(toolbox)\n",
      "Shogun is a free, open-source machine learning software library  written in C++. It offers numerous algorithms and data structures for machine learning problems. It offers interfaces for Octave, Python, R, Java, Lua, Ruby and C# using SWIG.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Spiking_neural_network\n",
      "Spiking neural networks (SNNs) are artificial neural networks that more closely mimic natural neural networks.[1] In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather transmit information only when a membrane potential—an intrinsic quality of the neuron related to its membrane electrical charge—reaches a specific value, called the threshold. When the membrane potential reaches the threshold, the neuron fires, and generates a signal that travels to other neurons which, in turn, increase or decrease their potentials in response to this signal. A neuron model  that fires at the moment of threshold crossing is also called a spiking neuron model.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Hugging_Face\n",
      "Hugging Face, Inc. is a French-American company that develops tools for building applications using machine learning, based in New York City. It is most notable for its transformers library built for natural language processing applications and its platform that allows users to share machine learning models and datasets and showcase their work.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Learning_to_rank\n",
      "Learning to rank[1] or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems.[2] Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. \"relevant\" or \"not relevant\") for each item. The goal of constructing the ranking model is to rank new, unseen lists in a similar way to rankings in the training data.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Similarity_learning\n",
      "Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Linear_classifier\n",
      "In the field of machine learning, the goal of statistical classification is to use an object's characteristics to identify which class (or group) it belongs to.  A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics.  An object's characteristics are also known as feature values and are typically presented to the machine in a vector called a feature vector. Such classifiers work well for practical problems such as document classification, and more generally for problems with many variables (features), reaching accuracy levels comparable to non-linear classifiers while taking less time to train and use.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/General_game_playing\n",
      "General game playing (GGP) is the design of artificial intelligence programs to be able to play more than one game successfully.[1][2][3] For many games like chess, computers are programmed to play these games using a specially designed algorithm, which cannot be transferred to another context. For instance, a chess-playing computer program cannot play checkers. General game playing is considered as a necessary milestone on the way to artificial general intelligence.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Neural_Turing_machine\n",
      "A neural Turing machine (NTM) is a recurrent neural network model of a Turing machine. The approach was published by Alex Graves et al. in 2014.[1] NTMs combine the fuzzy pattern matching capabilities of neural networks with the algorithmic power of programmable computers. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Music_and_artificial_intelligence\n",
      "Artificial intelligence and music (AIM) is a common subject in the International Computer Music Conference, the Computing Society Conference[1] and the International Joint Conference on Artificial Intelligence. The first International Computer Music Conference (ICMC) was held in 1974 at Michigan State University.[2] Current research includes the application of AI in music composition, performance, theory and digital sound processing.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Data_augmentation\n",
      "Data augmentation is a technique in machine learning used to reduce overfitting when training a machine learning model,[1] by training models on several slightly-modified copies of existing data.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Seq2seq\n",
      "Seq2seq is a family of machine learning approaches used for natural language processing.[1] Applications include language translation, image captioning, conversational models, and text summarization.[2]\n",
      "Seq2seq uses sequence transformation: it turns one sequence into another sequence.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Connectionism\n",
      "Connectionism (coined by Edward Thorndike in the 1930s) is the name of an approach to the study of human mental processes and cognition that utilizes mathematical models known as connectionist networks or artificial neural networks.[1] Connectionism has had many 'waves' since its beginnings.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Apache_Mahout\n",
      "Apache Mahout is a project of the Apache Software Foundation to produce free implementations of distributed or otherwise scalable machine learning algorithms focused primarily on linear algebra. In the past, many of the implementations use the Apache Hadoop platform, however today it is primarily focused on Apache Spark.[3][4] Mahout also provides Java/Scala libraries for common math operations (focused on linear algebra and statistics) and primitive Java collections. Mahout is a work in progress; a number of algorithms have been implemented.[5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/List_of_datasets_in_computer_vision_and_image_processing\n",
      "This is a list of datasets for machine learning research. It is part of the list of datasets for machine-learning research. These datasets consist primarily of images or videos for tasks such as object detection, facial recognition, and multi-label classification.\n",
      "\n",
      "https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations\n",
      "The International Conference on Learning Representations (ICLR) is a machine learning conference typically held in late April or early May each year. The conference includes invited talks as well as oral and poster presentations of refereed papers. Since its inception in 2013, ICLR has employed an open peer review process to referee paper submissions (based on models proposed by Yann LeCun[1]). In 2019, there were 1591 paper submissions, of which 500 accepted with poster presentations (31%) and 24 with oral presentations (1.5%).[2]. In 2021, there were 2997 paper submissions, of which 860 were accepted (29%).[3].\n",
      "\n",
      "https://en.wikipedia.org/wiki/Manifold_hypothesis\n",
      "The manifold hypothesis posits that many high-dimensional data sets that occur in the real world actually lie along low-dimensional latent manifolds inside that high-dimensional space.[1][2][3][4] As a consequence of the manifold hypothesis, many data sets that appear to initially require many variables to describe, can actually be described by a comparatively small number of variables, likened to the local coordinate system of the underlying manifold. It is suggested that this principle underpins the effectiveness of machine learning algorithms in describing high-dimensional data sets by considering a few common features.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Empirical_risk_minimization\n",
      "Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an algorithm will work in practice (the true \"risk\") because we don't know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the \"empirical\" risk).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Conditional_random_field\n",
      "Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighbouring\" samples, a CRF can take context into account. To do so, the predictions are modelled as a graphical model, which represents the presence of dependencies between the predictions. What kind of graph is used depends on the application. For example, in natural language processing, \"linear chain\" CRFs are popular, for which each prediction is dependent only on its immediate neighbours. In image processing, the graph typically connects locations to nearby and/or similar locations to enforce that they receive similar predictions.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Graphcore\n",
      "Graphcore Limited is a British semiconductor company that develops accelerators for AI and machine learning. It aims to make a massively parallel Intelligence Processing Unit (IPU) that holds the complete machine learning model inside the processor.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Machine_Learning_(journal)\n",
      "Machine Learning  is a peer-reviewed scientific journal, published since 1986.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      "Decision tree learning is a supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:WhatLinksHere/Machine_learning\n",
      "The following pages link to Machine learning \n",
      "\n",
      "https://en.wikipedia.org/wiki/Generative_model\n",
      "In statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent,[a] but three major types can be distinguished, following Jebara (2004):\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mila_(research_institute)\n",
      "Mila - Quebec AI Institute (originally Montreal Institute for Learning Algorithms) is a research institute in Montreal, Quebec, focusing mainly on machine learning research. Approximately 1000 students and researchers and 100 faculty members, were part of Mila in 2022.[1] Mila is part of the Pan-Canadian AI Strategy. [2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Attention_(machine_learning)\n",
      "Machine learning-based attention is a mechanism mimicking cognitive attention. It calculates \"soft\" weights for each word, more precisely for its embedding, in the context window. It can do it either in parallel (such as in transformers) or sequentially (such as recurrent neural networks). \"Soft\" weights can change during each runtime, in contrast to \"hard\" weights, which are (pre-)trained and fine-tuned and remain frozen afterwards.  \n",
      "\n",
      "https://en.wikipedia.org/wiki/Hyperparameter_optimization\n",
      "In machine learning, hyperparameter optimization[1] or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning\n",
      "Knowledge representation and reasoning (KRR, KR&R, KR²) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can use to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. Knowledge representation incorporates findings from psychology[1] about how humans solve problems and represent knowledge in order to design formalisms that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from logic to automate various kinds of reasoning, such as the application of rules or the relations of sets and subsets.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Relevance_vector_machine\n",
      "In mathematics, a Relevance Vector Machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification.[1]\n",
      "The RVM has an identical functional form to the support vector machine, but provides probabilistic classification.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mathematica\n",
      "Wolfram Mathematica is a software system with built-in libraries for several areas of technical computing that allow machine learning, statistics, symbolic computation, data manipulation, network analysis, time series analysis, NLP, optimization, plotting functions and various types of data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other programming languages. It was conceived by Stephen Wolfram, and is developed by Wolfram Research of Champaign, Illinois.[8][9] The Wolfram Language is the programming language used in Mathematica.[10] Mathematica 1.0 was released on June 23, 1988 in Champaign, Illinois and Santa Clara, California.[11][12][13]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Yooreeka\n",
      "Yooreeka is a library for data mining, machine learning, soft computing, and mathematical analysis. The project started with the code of the book \"Algorithms of the Intelligent Web\".[1] Although the term \"Web\" prevailed in the title, in essence, the algorithms are valuable in any software application.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Probably_approximately_correct_learning\n",
      "In computational learning theory, probably approximately correct (PAC) learning is a framework for mathematical analysis of machine learning. It was proposed in 1984 by Leslie Valiant.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Occam_learning\n",
      "In computational learning theory, Occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to probably approximately correct (PAC) learning, where the learner is evaluated on its predictive power of a test set.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Weka_(machine_learning)\n",
      "Waikato Environment for Knowledge Analysis (Weka) is a collection of machine learning and data analysis free software licensed under the GNU General Public License. It was developed at the University of Waikato, New Zealand and is the companion software to the book \"Data Mining: Practical Machine Learning Tools and Techniques\".[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/ECML_PKDD\n",
      "ECML PKDD, the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, is one of the leading[1][2] academic conferences on machine learning and knowledge discovery, held in Europe every year.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Astroinformatics\n",
      "Astroinformatics is an interdisciplinary field of study involving the combination of astronomy, data science, machine learning, informatics, and information/communications technologies.[2][3] The field is closely related to astrostatistics.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Cluster_analysis\n",
      "Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Loss_functions_for_classification\n",
      "In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to).[1]  Given \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "X\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\displaystyle {\\mathcal {X}}}\n",
      "\n",
      " as the space of all possible inputs (usually \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "X\n",
      "\n",
      "\n",
      "⊂\n",
      "\n",
      "\n",
      "R\n",
      "\n",
      "\n",
      "d\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\displaystyle {\\mathcal {X}}\\subset \\mathbb {R} ^{d}}\n",
      "\n",
      "), and \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Y\n",
      "\n",
      "\n",
      "=\n",
      "{\n",
      "−\n",
      "1\n",
      ",\n",
      "1\n",
      "}\n",
      "\n",
      "\n",
      "{\\displaystyle {\\mathcal {Y}}=\\{-1,1\\}}\n",
      "\n",
      " as the set of labels (possible outputs), a typical goal of classification algorithms is to find a function \n",
      "\n",
      "\n",
      "\n",
      "f\n",
      ":\n",
      "\n",
      "\n",
      "X\n",
      "\n",
      "\n",
      "→\n",
      "\n",
      "\n",
      "Y\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\displaystyle f:{\\mathcal {X}}\\to {\\mathcal {Y}}}\n",
      "\n",
      " which best predicts a label \n",
      "\n",
      "\n",
      "\n",
      "y\n",
      "\n",
      "\n",
      "{\\displaystyle y}\n",
      "\n",
      " for a given input \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x\n",
      "→\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\displaystyle {\\vec {x}}}\n",
      "\n",
      ".[2]  However, because of incomplete information, noise in the measurement, or probabilistic components in the underlying process, it is possible for the same \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x\n",
      "→\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\displaystyle {\\vec {x}}}\n",
      "\n",
      " to generate different \n",
      "\n",
      "\n",
      "\n",
      "y\n",
      "\n",
      "\n",
      "{\\displaystyle y}\n",
      "\n",
      ".[3]  As a result, the goal of the learning problem is to minimize expected loss (also known as the risk), defined as\n",
      "\n",
      "https://en.wikipedia.org/wiki/Whisper_(speech_recognition_system)\n",
      "Whisper is a machine learning model for speech recognition and transcription, created by OpenAI and first released as open-source software in September 2022.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Toronto_Declaration\n",
      "The Toronto Declaration: Protecting the Rights to Equality and Non-Discrimination in Machine Learning Systems is a declaration that advocates responsible practices for machine learning practitioners and governing bodies. It is a joint statement issued by groups including Amnesty International and Access Now, with other notable signatories including Human Rights Watch and The Wikimedia Foundation.[1] It was published at RightsCon on May 16, 2018.[2][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Random_forest\n",
      "\n",
      "Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned.[1][2] Random decision forests correct for decision trees' habit of overfitting to their training set.[3]: 587–588 \n",
      "\n",
      "https://en.wikipedia.org/wiki/PyTorch\n",
      "PyTorch is a machine learning framework based on the Torch library,[4][5][6] used for applications such as computer vision and natural language processing,[7] originally developed by Meta AI and now part of the Linux Foundation umbrella.[8][9][10][11] It is free and open-source software released under the modified BSD license. Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface.[12]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Domain-specific_language\n",
      "A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains. There are a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software, such as MUSH soft code. DSLs can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. Special-purpose computer languages have always existed in the computer age, but the term \"domain-specific language\" has become more popular due to the rise of domain-specific modeling. Simpler DSLs, particularly ones used by a single application, are sometimes informally called mini-languages.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Interpreter_(computing)\n",
      "In computer science, an interpreter is a computer program that directly executes instructions written in a programming or scripting language, without requiring them previously to have been compiled into a machine language program. An interpreter generally uses one of the following strategies for program execution:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Chinchilla_AI\n",
      "Chinchilla is a family of large language models developed by the research team at DeepMind, presented in March of 2022.[1] It is named \"chinchilla\" because it is a further development over a previous model family named Gopher. Both model families were trained in order to investigate the scaling laws of large language models.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action\n",
      "State–action–reward–state–action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was proposed by Rummery and Niranjan in a technical note[1] with the name \"Modified Connectionist Q-Learning\" (MCQ-L). The alternative name SARSA, proposed by Rich Sutton, was only mentioned as a footnote.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Force_control\n",
      "Force control is the control of the force with which a machine or the manipulator of a robot acts on an object or its environment. By controlling the contact force, damage to the machine as well as to the objects to be processed and injuries when handling people can be prevented. In manufacturing tasks, it can compensate for errors and reduce wear by maintaining a uniform contact force. Force control achieves more consistent results than position control, which is also used in machine control. Force control can be used as an alternative to the usual motion control, but is usually used in a complementary way, in the form of hybrid control concepts. The acting force for control is usually measured via force transducers or estimated via the motor current.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Memtransistor\n",
      "The memtransistor (a blend word from Memory Transfer Resistor) is an experimental multi-terminal passive electronic component that might be used in the construction of artificial neural networks.[1] It is a combination of the memristor and transistor technology.[2] This technology is different from the 1T-1R approach since the devices are merged into one single entity. Multiple memristers can be embedded with a single transistor, enabling it to more accurately model a neuron with its multiple synaptic connections. A neural network produced from these would provide hardware-based artificial intelligence with a good foundation.[1][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Machine_learning\n",
      "Machine learning is a branch of statistics and computer science which studies algorithms and architectures that learn from observed facts.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Scikit-learn\n",
      "scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language.[3]\n",
      "It features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Sentiment_analysis\n",
      "\n",
      "Sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. With the rise of deep language models, such as RoBERTa, also more difficult data domains can be analyzed, e.g., news texts where authors typically express their opinion/sentiment less explicitly.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computer_vision\n",
      "Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions.[1][2][3][4] Understanding in this context means the transformation of visual images (the input to the retina in the human analog) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Expert_system\n",
      "In artificial intelligence, an expert system is a computer system emulating the decision-making ability of a human expert.[1]\n",
      "Expert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural code.[2] The first expert systems were created in the 1970s and then proliferated in the 1980s.[3] Expert systems were among the first truly successful forms of artificial intelligence (AI) software.[4][5][6][7][8] \n",
      "An expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.\n",
      "\n",
      "https://en.wikipedia.org/wiki/AI_safety\n",
      "AI safety is an interdisciplinary field concerned with preventing accidents, misuse, or other harmful consequences that could result from artificial intelligence (AI) systems. It encompasses machine ethics and AI alignment, which aim to make AI systems moral and beneficial, and AI safety encompasses technical problems including monitoring systems for risks and making them highly reliable. Beyond AI research, it involves developing norms and policies that promote safety.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Association_rule_learning\n",
      "Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.[1] In any given transaction with a variety of items, association rules are meant to discover the rules that determine how or why certain items are connected.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence\n",
      "The ethics of artificial intelligence is the branch of the ethics of technology specific to artificially intelligent systems.[1] It is sometimes divided into a concern with the moral behavior of humans as they design, make, use and treat artificially intelligent systems, and a concern with the behavior of machines, in machine ethics. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Differentiable_neural_computer\n",
      "In artificial intelligence, a differentiable neural computer (DNC) is a memory augmented neural network architecture (MANN), which is typically (but not by definition) recurrent in its implementation. The model was published in 2016 by Alex Graves et al. of DeepMind.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Feature_engineering\n",
      "Feature engineering or feature extraction  or feature discovery is the process of extracting features (characteristics, properties, attributes) from raw data.[1] This can be done with deep learning networks such as convolutional neural networks that are able to learn features by themselves.[citation needed]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Situated_approach_(artificial_intelligence)\n",
      "In artificial intelligence research, the situated approach builds agents that are designed to behave effectively successfully in their environment. This requires designing AI \"from the bottom-up\" by focussing on the basic perceptual and motor skills required to survive. The situated approach gives a much lower priority to abstract reasoning or problem-solving skills.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Artificial_neural_networks\n",
      "This category are for articles about artificial neural networks (ANN).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Regression_analysis\n",
      "In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome' or 'response' variable, or a 'label' in machine learning parlance) and one or more independent variables (often called 'predictors', 'covariates', 'explanatory variables' or 'features'). The most common form of regression analysis is linear regression, in which one finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared differences between the true data and that line (or hyperplane). For specific mathematical reasons (see linear regression), this allows the researcher to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values. Less common forms of regression use slightly different procedures to estimate alternative location parameters (e.g., quantile regression or Necessary Condition Analysis[1]) or estimate the conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression).\n",
      "\n",
      "https://en.wikipedia.org/wiki/OpenNN\n",
      "OpenNN (Open Neural Networks Library) is a software library written in the C++ programming language which implements neural networks, a main area of deep learning research.[1] The library is open-source, licensed under the GNU Lesser General Public License.\n",
      "\n",
      "https://en.wikipedia.org/wiki/AI_winter\n",
      "\n",
      "In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1]  The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Dartmouth_workshop\n",
      "The Dartmouth Summer Research Project on Artificial Intelligence was a 1956 summer workshop widely considered[1][2][3] to be the founding event of artificial intelligence as a field.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Analysis_of_algorithms\n",
      "In computer science, the analysis of algorithms is the process of finding the computational complexity of algorithms—the amount of time, storage, or other resources needed to execute them. Usually, this involves determining a function that relates the size of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity). An algorithm is said to be efficient when this function's values are small, or grow slowly compared to a growth in the size of the input. Different inputs of the same size may cause the algorithm to have different behavior, so best, worst and average case descriptions might all be of practical interest.  When not otherwise specified, the function describing the performance of an algorithm is usually an upper bound, determined from the worst case inputs to the algorithm.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Programming_tool\n",
      "A programming tool or software development tool is a computer program that software developers use to create, debug, maintain, or otherwise support other programs and applications. The term usually refers to relatively simple programs, that can be combined to accomplish a task, much as one might use multiple hands to fix a physical object. The most basic tools are a source code editor and a compiler or interpreter, which are used ubiquitously and continuously. Other tools are used more or less depending on the language, development methodology, and individual engineer, often used for a discrete task, like a debugger or profiler. Tools may be discrete programs, executed separately – often from the command line – or may be parts of a single large program, called an integrated development environment (IDE). In many cases, particularly for simpler use, simple ad hoc techniques are used instead of a tool, such as print debugging instead of using a debugger, manual timing (of overall program or section of code) instead of a profiler, or tracking bugs in a  text file or spreadsheet instead of a bug tracking system.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine\n",
      "A restricted Boltzmann machine (RBM) (also called a restricted Sherrington–Kirkpatrick model with external field or restricted stochastic Ising–Lenz–Little model) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/SPSS_Modeler\n",
      "IBM SPSS Modeler is a data mining and text analytics software application from IBM. It is used to build predictive models and conduct other analytic tasks. It has a visual interface which allows users to leverage statistical and data mining algorithms without programming.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Activation_function\n",
      "Activation function of a node in an artificial neural network is a function that calculates the output of the node (based on its inputs and the weights on individual inputs). Nontrivial problems can be solved only using a nonlinear activation function.[1] Modern activation functions include the smooth version of the ReLU, the GELU, which was used in the 2018 BERT model,[2] the logistic (sigmoid) function used in the 2012 speech recognition model developed by Hinton et al,[3] the ReLU used in the 2012 AlexNet computer vision model and in the 2015 ResNet model. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Information_theory\n",
      "Information theory is the mathematical study of the quantification, storage, and communication of information.[1] The field was originally established by the works of Harry Nyquist and Ralph Hartley, in the 1920s, and Claude Shannon in the 1940s.[2]: vii  The field, in  applied mathematics, is at the intersection of probability theory, statistics, computer science, statistical mechanics, information engineering, and electrical engineering.\n",
      "\n",
      "https://en.wikipedia.org/wiki/BLOOM_(language_model)\n",
      "BigScience Large Open-science Open-access Multilingual Language Model (BLOOM[1]) is a transformer-based large language model. It was created by AI researchers to provide a free large language model for large-scale public access. Trained on around 366 billion tokens over March through July 2022, it is considered an alternative to OpenAI's GPT-3 with its 176 billion parameters. BLOOM uses a decoder-only transformer model architecture modified from Megatron-LM GPT-2.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Michael_I._Jordan\n",
      "Michael Irwin Jordan ForMemRS[6] (born February 25, 1956) is an American scientist, professor at the University of California, Berkeley and researcher in machine learning, statistics, and artificial intelligence.[7][8][9]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Vision_processing_unit\n",
      "A vision processing unit (VPU) is (as of 2023) an emerging class of microprocessor; it is a specific type of AI accelerator, designed to accelerate machine vision tasks.[1][2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Alan_Mackworth\n",
      "Alan Mackworth is a professor emeritus in the Department of Computer Science at the University of British Columbia. He is known as \"The Founding Father\" of RoboCup. He is a former president of the Association for the Advancement of Artificial Intelligence (AAAI) and former Canada Research Chair in Artificial Intelligence from 2001–2014.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Gated_recurrent_unit\n",
      "Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al.[1] The GRU is like a long short-term memory (LSTM) with a gating mechanism to input or forget certain features,[2] but lacks a context vector or output gate, resulting in fewer parameters than LSTM.[3] \n",
      "GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM.[4][5] GRUs showed that gating is indeed helpful in general, and Bengio's team came to no concrete conclusion on which of the two gating units was better.[6][7]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Vision_transformer\n",
      "A vision transformer (ViT) is a transformer designed for computer vision. Transformers were introduced in 2017,[1] and have found widespread use in natural language processing. In 2020, they were adapted for computer vision, yielding ViT.[2] The basic structure is to break down input images as a series of patches, then tokenized, before applying the tokens to a standard Transformer architecture.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Outline_of_machine_learning\n",
      "The following outline is provided as an overview of and topical guide to machine learning:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Data_mining\n",
      "Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Structured_prediction\n",
      "Structured prediction or structured (output) learning is an umbrella term for supervised machine learning techniques that involves predicting structured objects, rather than scalar discrete or real values.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Perceptron\n",
      "In machine learning, the perceptron (or McCulloch-Pitts neuron) is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.[1]  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Graph_neural_network\n",
      "A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs.[1][2][3][4][5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/AI_takeover\n",
      "An AI takeover is a hypothetical scenario in which artificial intelligence (AI) becomes the dominant form of intelligence on Earth, as computer programs or robots effectively take control of the planet away from the human species. Possible scenarios include replacement of the entire human workforce, takeover by a superintelligent AI, and the popular notion of a robot uprising. Stories of AI takeovers are very popular throughout science fiction. Some public figures, such as Stephen Hawking and Elon Musk, have advocated research into precautionary measures to ensure future superintelligent machines remain under human control.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Multi-agent_reinforcement_learning\n",
      "Multi-agent reinforcement learning (MARL) is a sub-field of reinforcement learning. It focuses on studying the behavior of multiple learning agents that coexist in a shared environment.[1] Each agent is motivated by its own rewards, and does actions to advance its own interests; in some environments these interests are opposed to the interests of other agents, resulting in complex group dynamics.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Platt_scaling\n",
      "In machine learning, Platt scaling or Platt calibration is a way of transforming the outputs of a classification model into a probability distribution over classes. The method was invented by John Platt in the context of support vector machines,[1]\n",
      "replacing an earlier method by Vapnik,\n",
      "but can be applied to other classification models.[2]\n",
      "Platt scaling works by fitting a logistic regression model to a classifier's scores.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Email_filtering\n",
      "Email filtering is the processing of email to organize it according to specified criteria. The term can apply to the intervention of human intelligence, but most often refers to the automatic processing of messages at an SMTP server, possibly applying anti-spam techniques. Filtering can be applied to incoming emails as well as to outgoing ones.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Swarm_intelligence\n",
      "Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/GPT-1\n",
      "Generative Pre-trained Transformer 1 (GPT-1) was the first of OpenAI's large language models following Google's invention of the transformer architecture in 2017.[2] In June 2018, OpenAI released a paper entitled \"Improving Language Understanding by Generative Pre-Training\",[3] in which they introduced that initial model along with the general concept of a generative pre-trained transformer.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/IEEE_Transactions_on_Pattern_Analysis_and_Machine_Intelligence\n",
      "IEEE Transactions on Pattern Analysis and Machine Intelligence (sometimes abbreviated as IEEE PAMI or simply PAMI) is a monthly peer-reviewed scientific journal published by the IEEE Computer Society. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems\n",
      "The Conference and Workshop on Neural Information Processing Systems (abbreviated as NeurIPS and formerly NIPS)  is a machine learning and computational neuroscience conference held every December. The conference is currently a double-track meeting (single-track until 2015) that includes invited talks as well as oral and poster presentations of refereed papers, followed by parallel-track workshops that up to 2013 were held at ski resorts.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Template_talk:Artificial_intelligence\n",
      "The current pic, which I have added to the template, is at top, and the previous one is at bottom. I do not think the old one was very good; it is an illustration of the contours of a human brain with a random circuit board overlaid on it. What circuit board? We don't know. It looks like there is supposed to be a pad for a CPU in the middle... and there is part of a ball grid array or something there... but there is also a gigantic randomly-shaped splotch of copper there, what is that for? I am confident that this is not an actual PCB, nor is it a plausible design for one, and I object to illustrating articles about artificial intelligence with a ridiculously fake image.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Temporal_difference_learning\n",
      "Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computability_theory\n",
      "\n",
      "Computability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability. In these areas, computability theory overlaps with proof theory and effective descriptive set theory.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Flux_(machine-learning_framework)\n",
      "Flux is an open-source machine-learning software library and ecosystem written in Julia.[1][6] Its current stable release is v0.14.5[4] . It has a layer-stacking-based interface for simpler models, and has a strong support on interoperability with other Julia packages instead of a monolithic design.[7] For example, GPU support is implemented transparently by CuArrays.jl[8] This is in contrast to some other machine learning frameworks which are implemented in other languages with Julia bindings, such as TensorFlow.jl, and thus are more limited by the functionality present in the underlying implementation, which is often in C or C++.[9] Flux joined NumFOCUS as an affiliated project in December of 2021.[10]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Biological_neural_network\n",
      "A neural circuit (also known as a biological neural network BNNs) is a population of neurons interconnected by synapses to carry out a specific function when activated.[1] Multiple neural circuits interconnect with one another to form large scale brain networks.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/David_Rumelhart\n",
      "David Everett Rumelhart (June 12, 1942 – March 13, 2011)[1] was an American psychologist who made many contributions to the formal analysis of human cognition, working primarily within the frameworks of mathematical psychology, symbolic artificial intelligence, and parallel distributed processing. He also admired formal linguistic approaches to cognition, and explored the possibility of formulating a formal grammar to capture the structure of stories.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Algorithmic_transparency\n",
      "Algorithmic transparency is the principle that the factors that influence the decisions made by algorithms should be visible, or transparent, to the people who use, regulate, and are affected by systems that employ those algorithms. Although the phrase was coined in 2016 by Nicholas Diakopoulos and Michael Koliska about the role of algorithms in deciding the content of digital journalism services,[1] the underlying principle dates back to the 1970s and the rise of automated systems for scoring consumer credit.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Computer_science\n",
      "Welcome to the WikiProject Computer science page. The goals of the project are to build a community of interest around computer science, and to provide a focal point for coordinating efforts to improve Wikipedia's computer science articles. The scope of the project includes all articles in the area of computer science, including computer programming and software engineering. \n",
      "\n",
      "https://en.wikipedia.org/wiki/SequenceL\n",
      "SequenceL is a general purpose functional programming language and auto-parallelizing (Parallel computing) compiler and tool set, whose primary design objectives are performance on multi-core processor hardware, ease of programming, platform portability/optimization, and code clarity and readability.  Its main advantage is that it can be used to write straightforward code that automatically takes full advantage of all the processing power available, without programmers needing to be concerned with identifying parallelisms, specifying vectorization, avoiding race conditions, and other challenges of manual directive-based programming approaches such as OpenMP.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Climate_Science\n",
      "Climatology (from Greek κλίμα, klima, \"slope\"; and -λογία, -logia) or climate science is the scientific study of Earth's climate, typically defined as weather conditions averaged over a period of at least 30 years.[1] Climate concerns the atmospheric condition during an extended to indefinite period of time; weather is the condition of the atmosphere during a relative brief period of time. The main topics of research are the study of climate variability, mechanisms of climate changes and modern climate change.[2][3] This topic of study is regarded as part of the atmospheric sciences and a subdivision of physical geography, which is one of the Earth sciences. Climatology includes some aspects of oceanography and biogeochemistry.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Anthropic\n",
      "Anthropic PBC is an American artificial intelligence (AI) startup company, founded by former members of OpenAI.[3][4] Anthropic develops general AI systems and large language models.[5] It is a public-benefit corporation, and has been connected to the effective altruism movement.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
      "In the context of artificial neural networks, the rectifier or ReLU (rectified linear unit) activation function[1][2] is an activation function defined as the positive part of its argument:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Chromosome_(genetic_algorithm)\n",
      "In genetic algorithms (GA), or more general, evolutionary algorithms (EA), a chromosome (also sometimes called a genotype) is a set of parameters which define a proposed solution of the problem that the evolutionary algorithm is trying to solve. The set of all solutions, also called individuals according to the biological model, is known as the population.[1][2] The genome of an individual consists of one, more rarely of several,[3][4] chromosomes and corresponds to the genetic representation of the task to be solved. A chromosome is composed of a set of genes, where a gene consists of one or more semantically connected parameters, which are often also called decision variables. They determine one or more phenotypic characteristics of the individual or at least have an influence on them.[2]  In the basic form of genetic algorithms, the chromosome is represented as a binary string,[5] while in later variants[6][7] and in EAs in general, a wide variety of other data structures are used.[8][9][10]\n",
      "\n",
      "https://en.wikipedia.org/wiki/DeepSpeed\n",
      "DeepSpeed is an open source deep learning optimization library for PyTorch.[1] The library is designed to reduce computing power and memory use and to train large distributed models with better parallelism on existing computer hardware.[2][3] DeepSpeed is optimized for low latency, high throughput training. It includes the Zero Redundancy Optimizer (ZeRO) for training models with 1 trillion or more parameters.[4] Features include mixed precision training, single-GPU, multi-GPU, and multi-node training as well as custom model parallelism. The DeepSpeed source code is licensed under MIT License and available on GitHub.[5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Peter_E._Hart\n",
      "Peter E. Hart (born 1941[2]) is an American computer scientist and entrepreneur. He was chairman and president of Ricoh Innovations, which he founded in 1997. He made significant contributions in the field of computer science in a series of widely cited publications from the years 1967 to 1975 while associated with the Artificial Intelligence Center of SRI International, a laboratory where he also served as director.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Outline_of_artificial_intelligence\n",
      "The following outline is provided as an overview of and topical guide to artificial intelligence:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Natural-language_understanding\n",
      "Natural-language understanding (NLU) or natural-language interpretation (NLI)[1] is a subtopic  of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Multilayer_perceptron\n",
      "A multilayer perceptron (MLP) is a misnomer for a modern feedforward artificial neural network, consisting of fully connected neurons with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not linearly separable.[1] It is a misnomer because the original perceptron used a Heaviside step function, instead of a nonlinear kind of activation function (used by modern networks).\n",
      "\n",
      "https://en.wikipedia.org/wiki/The_Master_Algorithm\n",
      "The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence\n",
      "\n",
      "\n",
      "\"Computing Machinery and Intelligence\" is a seminal paper written by Alan Turing on the topic of artificial intelligence. The paper, published in 1950 in Mind, was the first to introduce his concept of what is now known as the Turing test to the general public.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Probabilistic_classification\n",
      "In machine learning, a probabilistic classifier is a classifier that is able to predict, given an observation of an input, a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to. Probabilistic classifiers provide classification that can be useful in its own right[1] or when combining classifiers into ensembles.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Template:Artificial_intelligence\n",
      "This template shows topics in the area of artificial intelligence.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Modeling_language\n",
      "A modeling language is any artificial language that can be used to express data, information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure Programing language.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Outline_of_computer_science\n",
      "Computer science (also called computing science) is the study of the theoretical foundations of information and computation and their implementation and application in computer systems. One well known subject classification system for computer science is the ACM Computing Classification System devised by the Association for Computing Machinery.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Regularization_(mathematics)\n",
      "In mathematics, statistics, finance,[1] computer science, particularly in machine learning and inverse problems, regularization is a process that changes the result answer to be \"simpler\". It is often used to obtain results for ill-posed problems or to prevent overfitting.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mallet_(software_project)\n",
      "MALLET is a Java \"Machine Learning for Language Toolkit\".\n",
      "\n",
      "https://en.wikipedia.org/wiki/Inductive_programming\n",
      "Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Convolutional_neural_network\n",
      "Convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters (or kernel) optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections.[1][2] For example, for each neuron in the fully-connected layer 10,000 weights would be required for processing an image sized 100 × 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels,[3][4]  only 25 neurons are required to process 5x5-sized tiles.[5][6] Higher-layer features are extracted  from wider context windows, compared to lower-layer features.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Approximate_computing\n",
      "Approximate computing is an emerging paradigm for energy-efficient and/or high-performance design.[1] It includes a plethora of computation techniques that return a possibly inaccurate result rather than a guaranteed accurate result, and that can be used for applications where an approximate result is sufficient for its purpose.[2] One example of such situation is for a search engine where no exact answer may exist for a certain search query and hence, many answers may be acceptable. Similarly, occasional dropping of some frames in a video application can go undetected due to perceptual limitations of humans. Approximate computing is based on the observation that in many scenarios, although performing exact computation requires large amount of resources, allowing bounded approximation can provide disproportionate gains in performance and energy, while still achieving acceptable result accuracy.[clarification needed]  For example, in k-means clustering algorithm, allowing only 5% loss in classification accuracy can provide 50 times energy saving compared to the fully accurate classification.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Theoretical_computer_science\n",
      "Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, formal language theory, the lambda calculus and type theory.\n",
      "\n",
      "https://en.wikipedia.org/wiki/LLaMA\n",
      "LLaMA (Large Language Model Meta AI) is a family of large language models (LLMs), released by Meta AI starting in February 2023. \n",
      "\n",
      "https://en.wikipedia.org/wiki/MXNet\n",
      "Apache MXNet is an open-source deep learning software framework that trains and deploys deep neural networks. It is scalable, allows fast model training, and supports a flexible programming model and multiple programming languages (including C++, Python, Java, Julia, MATLAB, JavaScript, Go, R, Scala, Perl, and Wolfram Language). The MXNet library is portable and can scale to multiple GPUs[2] and machines. It was co-developed by Carlos Guestrin at the University of Washington (along with GraphLab).[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Semantics_(computer_science)\n",
      "In programming language theory, semantics is the rigorous mathematical study of the meaning of programming languages.[1] Semantics assigns computational meaning to valid strings in a programming language syntax. It is closely related to, and often crosses over with, the semantics of mathematical proofs.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Infer.NET\n",
      "Infer.NET is a free and open source .NET software library for machine learning.[2] It supports running Bayesian inference in graphical models and can also be used for probabilistic programming.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Alex_Graves_(computer_scientist)\n",
      "Alex Graves is a computer scientist. Before working as a research scientist at DeepMind, he earned a BSc in Theoretical Physics from the University of Edinburgh and a PhD in artificial intelligence under Jürgen Schmidhuber at IDSIA.[1] He was also a postdoc under Schmidhuber at the Technical University of Munich and under Geoffrey Hinton[2] at the University of Toronto.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Corinna_Cortes\n",
      "Corinna Cortes (born 31 March, 1961) is a Danish computer scientist known for her contributions to machine learning. She is a Vice President at Google Research in New York City.[3] Cortes is an ACM Fellow and a recipient of the Paris Kanellakis Award for her work on theoretical foundations of support vector machines.[4][5][3][6]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\n",
      "In statistics and machine learning, the bias–variance tradeoff describes the relationship between a model's complexity, the accuracy of its predictions, and how well it can make predictions on previously unseen data that were not used to train the model. In general, as we increase the number of tunable parameters in a model, it becomes more flexible, and can better fit a training data set. It is said to have lower error, or bias. However, for more flexible models, there will tend to be greater variance to the model fit each time we take a set of samples to create a new training data set. It is said that there is greater variance in the model's estimated parameters.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computer_animation\n",
      "Computer animation is the process used for digitally generating animations. The more general term computer-generated imagery (CGI) encompasses both static scenes (still images) and dynamic images (moving images), while computer animation only refers to moving images. Modern computer animation usually uses 3D computer graphics. The animation's target is sometimes the computer itself, while other times it is film.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory\n",
      "Vapnik–Chervonenkis theory (also known as VC theory) was developed during 1960–1990 by Vladimir Vapnik and Alexey Chervonenkis. The theory is a form of computational learning theory, which attempts to explain the learning process from a statistical point of view.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Rendering_(computer_graphics)\n",
      "Rendering or image synthesis is the process of generating a photorealistic or non-photorealistic image from a 2D or 3D model by means of a computer program.[citation needed]  The resulting image is referred to as the render.  Multiple models can be defined in a scene file containing objects in a strictly defined language or data structure.  The scene file contains geometry, viewpoint, texture, lighting, and shading information describing the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file. The term \"rendering\" is analogous to the concept of an artist's impression of a scene.  The term \"rendering\" is also used to describe the process of calculating effects in a video editing program to produce the final video output.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Sparse_coding\n",
      "Neural coding (or neural representation) is a neuroscience field concerned with characterising the hypothetical relationship between the stimulus and the individual or ensemble neuronal responses and the relationship among the electrical activity of the neurons in the ensemble.[1][2] Based on the theory that\n",
      "sensory and other information is represented in the brain by networks of neurons, it is thought that neurons can encode both digital and analog information.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/AlphaFold\n",
      "AlphaFold is an artificial intelligence (AI) program developed by DeepMind, a subsidiary of Alphabet, which performs predictions of protein structure.[1] The program is designed as a deep learning system.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Action_selection\n",
      "Action selection is a way of characterizing the most basic problem of intelligent systems: what to do next. In artificial intelligence and computational cognitive science, \"the action selection problem\" is typically associated with intelligent agents and animats—artificial systems that exhibit complex behaviour in an agent environment. The term is also sometimes used in ethology or animal behavior.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Keras\n",
      "Keras is an open-source library that provides a Python interface for artificial neural networks. Keras acts as an interface for the TensorFlow library.[citation needed]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Nils_Nilsson_(researcher)\n",
      "Nils John Nilsson (February 6, 1933 – April 23, 2019) was an American computer scientist. He was one of the founding researchers in the discipline of artificial intelligence.[2] He was the first Kumagai Professor of Engineering in computer science at Stanford University from 1991 until his retirement. He is particularly known for his contributions to search, planning, knowledge representation, and robotics.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Tensor_Processing_Unit\n",
      "Tensor Processing Unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google for neural network machine learning, using Google's own TensorFlow software.[1] Google began using TPUs internally in 2015, and in 2018 made them available for third party use, both as part of its cloud infrastructure and by offering a smaller version of the chip for sale.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
      "Cross-validation,[2][3][4] sometimes called rotation estimation[5][6][7] or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.\n",
      "Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.  In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set).[8][9] The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias[10] and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).\n",
      "\n",
      "https://en.wikipedia.org/wiki/WaveNet\n",
      "WaveNet is a deep neural network for generating raw audio. It was created by researchers at London-based AI firm DeepMind. The technique, outlined in a paper in September 2016,[1] is able to generate relatively realistic-sounding human-like voices by directly modelling waveforms using a neural network method trained with recordings of real speech. Tests with US English and Mandarin reportedly showed that the system outperforms Google's best existing text-to-speech (TTS) systems, although as of 2016 its text-to-speech synthesis still was less convincing than actual human speech.[2] WaveNet's ability to generate raw waveforms means that it can model any kind of audio, including music.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/KNIME\n",
      "KNIME (/naɪm/), the Konstanz Information Miner,[2] is a free and open-source data analytics, reporting and integration platform. KNIME integrates various components for machine learning and data mining through its modular data pipelining \"Building Blocks of Analytics\" concept. A graphical user interface and use of JDBC allows assembly of nodes blending different data sources, including preprocessing (ETL: Extraction, Transformation, Loading), for modeling, data analysis and visualization without, or with only minimal, programming.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computer_architecture\n",
      "In computer science and computer engineering, computer architecture is a description of the structure of a computer system made from component parts.[1] It can sometimes be a high-level description that ignores details of the implementation.[2] At a more detailed level, the description may include the instruction set architecture design, microarchitecture design, logic design, and implementation.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ubiquitous_computing\n",
      " Ubiquitous computing (or \"ubicomp\") is a concept in software engineering, hardware engineering and computer science where computing is made to appear anytime and everywhere. In contrast to desktop computing, ubiquitous computing can occur using any device, in any location, and in any format. A user interacts with the computer, which can exist in many different forms, including laptop computers, tablets, smart phones and terminals in everyday objects such as a refrigerator or a pair of glasses. The underlying technologies to support ubiquitous computing include Internet, advanced middleware, operating system, mobile code, sensors, microprocessors, new I/O and user interfaces, computer networks, mobile protocols, location and positioning, and new materials.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Control_variable_(programming)\n",
      "In computer science, control flow (or flow of control) is the order in which individual statements, instructions or function calls of an imperative program are executed or evaluated. The emphasis on explicit control flow distinguishes an imperative programming language from a declarative programming language.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Dimensionality_reduction\n",
      "Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with). Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time\n",
      "In theoretical computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a constant factor.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ehud_Shapiro\n",
      "Ehud Shapiro (Hebrew: אהוד שפירא; born 1955) is an Israeli scientist, artist, and entrepreneur, who is Professor of Computer Science and Biology at the Weizmann Institute of Science.[2] With international reputation, he made fundamental contributions to many scientific disciplines,[3] laying in each a long-term research agenda by asking a novel basic question and offering a first step towards answering it, including how to computerize the process of scientific discovery, by providing an algorithmic interpretation to Karl Popper's methodology of conjectures and refutations;[4][5] how to automate program debugging, by algorithms for fault localization;[6] how to unify parallel, distributed, and systems programming with a high-level logic-based programming language;[7] how to use the metaverse as a foundation for social networking;[8] how to devise molecular computers that can function as smart programmable drugs;[9][10] how to uncover the human cell lineage tree, via single-cell genomics;[11][12] how to support digital democracy, by devising an alternative architecture to the digital realm.[13][14]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Diffusion_process\n",
      "In probability theory and statistics, diffusion processes are a class of continuous-time Markov process with almost surely continuous sample paths. Diffusion process is stochastic in nature and hence is used to model many real-life stochastic systems. Brownian motion, reflected Brownian motion and Ornstein–Uhlenbeck processes are examples of diffusion processes. It is used heavily in statistical physics, statistical analysis, information theory, data science, neural networks, finance and marketing.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Machine_perception\n",
      "Machine perception is the capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them.[1][2][3] The basic method that the computers take in and respond to their environment is through the attached hardware.  Until recently input was limited to a keyboard, or a mouse, but advances in technology, both in hardware and software, have allowed computers to take in sensory input in a way similar to humans.[1][2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ontology_learning\n",
      "Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Inductive_bias\n",
      "The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered.[1]\n",
      "Inductive bias is anything which makes the algorithm learn one pattern instead of another pattern (e.g. step-functions in decision trees instead of continuous function in a linear regression model).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Programming_language_theory\n",
      "Programming language theory (PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of formal languages known as programming languages. Programming language theory is closely related to other fields including mathematics, software engineering, and linguistics. There are a number of academic conferences and journals in the area.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_statistics\n",
      "Computational statistics, or statistical computing, is the bond between statistics and computer science, and refers to the statistical methods that are enabled by using computational methods. It is the area of computational science (or scientific computing) specific to the mathematical science of statistics. This area is also developing rapidly, leading to calls that a broader concept of computing should be taught as part of general statistical education.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_biology\n",
      "Computational biology refers to the use of data analysis, mathematical modeling and computational simulations to understand biological systems and relationships.[1] An intersection of computer science, biology, and big data, the field also has foundations in applied mathematics, chemistry, and genetics.[2] It differs from biological computing, a subfield of computer science and engineering which uses bioengineering to build computers.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Google_JAX\n",
      "Google JAX is a machine learning framework for transforming numerical functions.[1][2][3] It is described as bringing together a modified version of autograd (automatic obtaining of the gradient function through differentiation of a function) and TensorFlow's XLA (Accelerated Linear Algebra). It is designed to follow the structure and workflow of NumPy as closely as possible and works with various existing frameworks such as TensorFlow and PyTorch.[4][5] The primary functions of JAX are:[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Model_of_computation\n",
      "In computer science, and more specifically in computability theory and computational complexity theory, a model of computation is a model which describes how an output of a mathematical function is computed given an input. A model describes how units of computations, memories, and communications are organized.[1] The computational complexity of an algorithm can be measured given a model of computation. Using a model allows studying the performance of algorithms independently of the variations that are specific to particular implementations and specific technology.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Microsoft_Cognitive_Toolkit\n",
      "Microsoft Cognitive Toolkit,[3] previously known as CNTK and sometimes styled as The Microsoft Cognitive Toolkit, is a deprecated[4] deep learning framework developed by Microsoft Research. Microsoft Cognitive Toolkit describes neural networks as a series of computational steps via a directed graph.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Human-in-the-loop\n",
      "Human-in-the-loop  or HITL is used in multiple contexts. It can be defined as a model requiring human interaction.[1][2] HITL is associated with modeling and simulation (M&S) in the live, virtual, and constructive taxonomy. HITL along with the related human-on-the-loop are also used in relation to lethal autonomous weapons.[3] Further, HITL is used in the context of machine learning.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Statistical_model\n",
      "A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process.[1] When referring specifically to probabilities, the corresponding term is probabilistic model.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Social_computing\n",
      "Social computing is an area of computer science that is concerned with the intersection of social behavior and computational systems. It is based on creating or recreating social conventions and social contexts through the use of software and technology. Thus, blogs, email, instant messaging, social network services, wikis, social bookmarking and other instances of what is often called social software illustrate ideas from social computing.   \n",
      "\n",
      "https://en.wikipedia.org/wiki/Prompt_engineering#In-context_learning\n",
      "Prompt engineering is the process of structuring text that can be interpreted and understood by a generative AI model.[1][2] A prompt is natural language text describing the task that an AI should perform.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Manifold_learning\n",
      "Nonlinear dimensionality reduction, also known as manifold learning, refers to various related techniques that aim to project high-dimensional data onto lower-dimensional latent manifolds, with the goal of either visualizing the data in the low-dimensional space, or learning the mapping (either from the high-dimensional space to the low-dimensional embedding or vice versa) itself.[1][2] The techniques described below can be understood as generalizations of linear decomposition methods used for dimensionality reduction, such as singular value decomposition and principal component analysis.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Self-play_(reinforcement_learning_technique)\n",
      "Self-play is a technique for improving the performance of reinforcement learning agents. Intuitively, agents learn to improve their performance by playing \"against themselves\".\n",
      "\n",
      "https://en.wikipedia.org/wiki/Logic_in_computer_science\n",
      "Logic in computer science covers the overlap between the field of logic and that of computer science. The topic can essentially be divided into three main areas:\n",
      "\n",
      "https://en.wikipedia.org/wiki/DeepDream\n",
      "DeepDream is a computer vision program created by Google engineer Alexander Mordvintsev that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dream-like appearance reminiscent of a psychedelic experience in the deliberately overprocessed images.[1][2][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Graphical_model\n",
      "A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statistics—particularly Bayesian statistics—and machine learning.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction\n",
      "Human–computer interaction (HCI) is research in the design and the use of computer technology, which focuses on the interfaces between people (users) and computers. HCI researchers observe the ways humans interact with computers and design technologies that allow humans to interact with computers in novel ways. A device that allows interaction between human being and a computer is known as a \"Human-computer Interface (HCI)\".\n",
      "\n",
      "https://en.wikipedia.org/wiki/BERT_(language_model)\n",
      "\n",
      "Bidirectional Encoder Representations from Transformers (BERT) is a family of language models introduced in October 2018 by researchers at Google.[1][2] A 2020 literature survey concluded that \"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\"[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_physics\n",
      "Computational physics is the study and implementation of numerical analysis to solve problems in physics.[1] Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science. It is sometimes regarded as a subdiscipline (or offshoot) of theoretical physics, but others consider it an intermediate branch between theoretical and experimental physics — an area of study which supplements both theory and experiment.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Self-organizing_map\n",
      "A self-organizing map (SOM) or self-organizing feature map (SOFM) is an unsupervised machine learning technique used to produce a low-dimensional (typically two-dimensional) representation of a higher dimensional data set while preserving the topological structure of the data. For example, a data set with \n",
      "\n",
      "\n",
      "\n",
      "p\n",
      "\n",
      "\n",
      "{\\displaystyle p}\n",
      "\n",
      " variables measured in \n",
      "\n",
      "\n",
      "\n",
      "n\n",
      "\n",
      "\n",
      "{\\displaystyle n}\n",
      "\n",
      " observations could be represented as clusters of observations with similar values for the variables. These clusters then could be visualized as a two-dimensional \"map\" such that observations in proximal clusters have more similar values than observations in distal clusters. This can make high-dimensional data easier to visualize and analyze.\n",
      "\n",
      "https://en.wikipedia.org/wiki/PaLM\n",
      "PaLM (Pathways Language Model) is a 540 billion parameter transformer-based large language model developed by Google AI.[1] Researchers also trained smaller versions of PaLM, 8 and 62 billion parameter models, to test the effects of model scale.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Sparse_dictionary_learning\n",
      "Sparse dictionary learning (also known as sparse coding or SDL) is a representation learning method which aims at finding a sparse representation of the input data in the form of a linear combination of basic elements as well as those basic elements themselves. These elements are called atoms and they compose a dictionary. Atoms in the dictionary are not required to be orthogonal, and they may be an over-complete spanning set. This problem setup also allows the dimensionality of the signals being represented to be higher than the one of the signals being observed. The above two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal but also provide an improvement in sparsity and flexibility of the representation.\n",
      "\n",
      "https://en.wikipedia.org/wiki/SpiNNaker\n",
      "SpiNNaker (spiking neural network architecture) is a massively parallel, manycore supercomputer architecture designed by the Advanced Processor Technologies Research Group (APT) at the Department of Computer Science, University of Manchester.[2]  It is composed of 57,600 processing nodes, each with 18 ARM9 processors (specifically ARM968) and 128 MB of mobile DDR SDRAM, totalling 1,036,800 cores and over 7 TB of RAM.[3]  The computing platform is based on spiking neural networks, useful in simulating the human brain (see Human Brain Project).[4][5][6][7][8][9][10][11][12]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Orange_(software)\n",
      "Orange is an open-source data visualization, machine learning and data mining toolkit. It features a visual programming front-end for explorative qualitative data analysis and interactive data visualization.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Image_compression\n",
      "Image compression is a type of data compression applied to digital images, to reduce their cost for storage or transmission. Algorithms may take advantage of visual perception and the statistical properties of image data to provide superior results compared with generic data compression methods which are used for other digital data.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Information_geometry\n",
      "Information geometry is an interdisciplinary field that applies the techniques of differential geometry to study probability theory and statistics. [1]  It studies statistical manifolds, which are Riemannian manifolds whose points correspond to probability distributions.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computer_accessibility\n",
      "Computer accessibility (also known as accessible computing) refers to the accessibility of a computer system to all people, regardless of disability type or severity of impairment. The term accessibility is most often used in reference to specialized hardware or software, or a combination of both, designed to enable the use of a computer by a person with a disability or impairment. Computer accessibility often has direct positive effects on people with disabilities.\n",
      "\n",
      "https://en.wikipedia.org/wiki/STATISTICA\n",
      "Statistica is an advanced analytics software package originally developed by StatSoft and currently maintained by TIBCO Software Inc.[1]\n",
      "Statistica provides data analysis, data management, statistics, data mining, machine learning, text analytics and data visualization procedures.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Principal_component_analysis\n",
      "Principal component analysis (PCA) is a popular technique for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data. Formally, PCA is a statistical technique for reducing the dimensionality of a dataset. This is accomplished by linearly transforming the data into a new coordinate system where (most of) the variation in the data can be described with fewer dimensions than the initial data. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points. Principal component analysis has applications in many fields such as population genetics, microbiome studies, and atmospheric science.[1]\n",
      "https://en.wikipedia.org/wiki/Netflix_Prize\n",
      "The Netflix Prize was an open competition for the best collaborative filtering algorithm to predict user ratings for films, based on previous ratings without any other information about the users or films, i.e. without the users being identified except by numbers assigned for the contest.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Syntactic_pattern_recognition\n",
      "Syntactic pattern recognition or structural pattern recognition is a form of pattern recognition, in which each object can be represented by a variable-cardinality set of symbolic, nominal features. This allows for representing pattern structures, taking into account more complex interrelationships between attributes than is possible in the case of flat, numerical feature vectors of fixed dimensionality, that are used in statistical classification.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Application_security\n",
      "Application security (short AppSec) includes all tasks that introduce a secure software development life cycle to development teams. Its final goal is to improve security practices and, through that, to find, fix and preferably prevent security issues within applications. It encompasses the whole application life cycle from requirements analysis, design, implementation, verification as well as maintenance.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ridge_regression\n",
      "Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated.[1] It has been used in many fields including econometrics, chemistry, and engineering.[2] Also known as Tikhonov regularization, named for Andrey Tikhonov, it is a method of regularization of ill-posed problems.[a] It is particularly useful to mitigate the problem of multicollinearity in linear regression, which commonly occurs in models with large numbers of parameters.[3] In general, the method provides improved efficiency in parameter estimation problems in exchange for a tolerable amount of bias (see bias–variance tradeoff).[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Statistical_manifold\n",
      "In mathematics, a statistical manifold is a Riemannian manifold, each of whose points is a probability distribution.  Statistical manifolds provide a setting for the field of information geometry.  The Fisher information metric provides a metric on these manifolds. Following this definition, the log-likelihood function is a differentiable map and the score is an inclusion.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Digital_library\n",
      "A digital library, also called an online library, an internet library, a digital repository,  a library without walls, or a digital collection, is an online database of digital objects that can include text, still images, audio, video, digital documents, or other digital media formats or a library accessible through the internet. Objects can consist of digitized content like print or photographs, as well as originally produced digital content like word processor files or social media posts. In addition to storing content, digital libraries provide means for organizing, searching, and retrieving the content contained in the collection. Digital libraries can vary immensely in size and scope, and can be maintained by individuals or organizations.[1] The digital content may be stored locally, or accessed remotely via computer networks. These information retrieval systems are able to exchange information with each other through interoperability and sustainability.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Network_security\n",
      "Network security consists of the policies, processes and practices adopted to prevent, detect and monitor unauthorized access, misuse, modification, or denial of a computer network and network-accessible resources.[1] Network security involves the authorization of access to data in a network, which is controlled by the network administrator. Users choose or are assigned an ID and password or other authenticating information that allows them access to information and programs within their authority. Network security covers a variety of computer networks, both public and private, that are used in everyday jobs: conducting transactions and communications among businesses, government agencies and individuals. Networks can be private, such as within a company, and others which might be open to public access. Network security is involved in organizations, enterprises, and other types of institutions. It does as its title explains: it secures the network, as well as protecting and overseeing operations being done. The most common and simple way of protecting a network resource is by assigning it a unique name and a corresponding password.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Backdoor_(computing)\n",
      "A backdoor is a typically covert method of bypassing normal authentication or encryption in a computer, product, embedded device (e.g. a home router), or its embodiment (e.g. part of a cryptosystem, algorithm, chipset, or even a \"homunculus computer\"—a tiny computer-within-a-computer such as that found in Intel's AMT technology).[1][2] Backdoors are most often used for securing remote access to a computer, or obtaining access to plaintext in cryptosystems. From there it may be used to gain access to privileged information like passwords, corrupt or delete data on hard drives, or transfer information within autoschediastic networks.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Peripheral\n",
      "A peripheral  device, or simply peripheral, is an auxiliary hardware device used to transfer information into and out of a computer.[1] The term peripheral device refers to all hardware components that are attached to a computer and are controlled by the computer system, but they are not the core components of the computer.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Automated_planning_and_scheduling\n",
      "Automated planning and scheduling, sometimes denoted as simply AI planning,[1] is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Word2vec\n",
      "Word2vec is a technique for natural language processing (NLP) published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that they capture the semantic and syntactic qualities of words; as such, a simple mathematical function (cosine similarity) can indicate the level of semantic similarity between the words represented by those vectors.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Hybrid_intelligent_system\n",
      "\n",
      "Hybrid intelligent system denotes a software system which employs, in parallel, a combination of methods and techniques from artificial intelligence subfields, such as:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Search_algorithm\n",
      "In computer science, a search algorithm is an algorithm designed to solve a search problem. Search algorithms work to retrieve information stored within particular data structure, or calculated in the search space of a problem domain, with either discrete or continuous values.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Robot_control\n",
      "Robotic control is the system that contributes to the movement of robots. This involves the mechanical aspects and programmable systems that makes it possible to control robots. Robotics can be controlled by various means including manual, wireless, semi-autonomous (a mix of fully automatic and wireless control), and fully autonomous (using artificial intelligence).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Topic_modeling\n",
      "In statistics and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear approximately equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.\n",
      "\n",
      "https://en.wikipedia.org/wiki/GPU\n",
      "A graphics processing unit (GPU) is a specialized electronic circuit initially designed to accelerate computer graphics and image processing (either on a video card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles). After their initial design, GPUs were found to be useful for non-graphic calculations involving embarrassingly parallel problems due to their parallel structure. Other non-graphical uses include the training of neural networks and cryptocurrency mining.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_social_science\n",
      "Computational social science is the academic sub-discipline concerned with computational approaches to the social sciences. This means that computers are used to model, simulate, and analyze social phenomena.  Fields include computational economics, computational sociology, cliodynamics, culturomics, nonprofit studies,[1] and the automated analysis of contents, in social and traditional media. It focuses on investigating social and behavioral relationships and interactions through social simulation, modeling, network analysis, and media analysis.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_anatomy\n",
      "Computational anatomy is an interdisciplinary field of biology focused on quantitative investigation and modelling of anatomical shapes variability.[1][2] It involves the development and application of mathematical, statistical and data-analytical methods for modelling and simulation of biological structures.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Neural_Computation_(journal)\n",
      "Neural Computation is a monthly peer-reviewed scientific journal covering all aspects of neural computation, including modeling the brain and the design and construction of neurally-inspired information processing systems. It was established in 1989 and is published by MIT Press. The editor-in-chief is Terrence J. Sejnowski (Salk Institute for Biological Studies).[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Linear_regression\n",
      "In statistics, linear regression is a linear approach for modelling a predictive relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables), which are measured without error. The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression.[1] This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.[2]If the explanatory variables are measured with error then errors-in-variables models are required, also known as measurement error models.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Inductive_logic_programming\n",
      "Inductive logic programming (ILP) is a subfield of symbolic artificial intelligence  which uses logic programming as a uniform representation for examples, background knowledge and hypotheses.  The term \"inductive\" here refers to philosophical (i.e. suggesting a theory to explain observed facts) rather than mathematical (i.e. proving a property for all members of a well-ordered set) induction. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesised logic program which entails all the positive and none of the negative examples.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Parallel_computing\n",
      "Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously.[1] Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing, but has gained broader interest due to the physical constraints preventing frequency scaling.[2] As power consumption (and consequently heat generation) by computers has become a concern in recent years,[3] parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Q-learning\n",
      "Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. It does not require a model of the environment (hence \"model-free\"), and it can handle problems with stochastic transitions and rewards without requiring adaptations [1].\n",
      "\n",
      "https://en.wikipedia.org/wiki/Automated_theorem_proving\n",
      "Automated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs. Automated reasoning over mathematical proof was a major impetus for the development of computer science.\n",
      "\n",
      "https://en.wikipedia.org/wiki/ADALINE\n",
      "ADALINE (Adaptive Linear Neuron or later Adaptive Linear Element) is an early single-layer artificial neural network and the name of the physical device that implemented this network.[1][2][3][4][5] The network uses memistors. It was developed by professor Bernard Widrow and his doctoral student Ted Hoff at Stanford University in 1960. It is based on the perceptron. It consists of a weight, a bias and a summation function.\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Artificial_intelligence_prompt_completion_by_dalle_mini.jpg\n",
      "Original file ‎(1,024 × 1,024 pixels, file size: 211 KB, MIME type: image/jpeg)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Software_engineering\n",
      "Software engineering is an engineering-based approach to software development.[1][2][3]\n",
      "A software engineer is a person who applies the engineering design process to design, develop, test, maintain, and evaluate computer software. The term programmer is sometimes used as a synonym, but may emphasize software implementation over design and can also lack connotations of engineering education or skills.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Rakesh_Agrawal_(computer_scientist)\n",
      "Rakesh Agrawal (हिन्दी - राकेश अग्रवाल) is a computer scientist who until recently was a Technical Fellow at the Microsoft Search Labs.[1] Rakesh is well known for developing fundamental data mining concepts and technologies and pioneering key concepts in data privacy, including Hippocratic Database, Sovereign Information Sharing, and Privacy-Preserving Data Mining. IBM's commercial data mining product, Intelligent Miner, grew out of his work. His research has been incorporated into other IBM products, including DB2 Mining Extender, DB2 OLAP Server and WebSphere Commerce Server, and has influenced several other commercial and academic products, prototypes and applications. His other technical contributions include Polyglot object-oriented type system, Alert active database system, Ode (Object database and environment), Alpha (extension of relational databases with generalized transitive closure), Nest distributed system, transaction management, and database machines.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Autoregressive_model\n",
      "In statistics, econometrics, and signal processing, an autoregressive (AR) model is a representation of a type of random process; as such, it is used to describe certain time-varying processes in nature, economics, behavior, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term); thus the model is in the form of a stochastic difference equation (or recurrence relation) which should not be confused with a differential equation. Together with the moving-average (MA) model, it is a special case and key component of the more general autoregressive–moving-average (ARMA) and autoregressive integrated moving average (ARIMA) models of time series, which have a more complicated stochastic structure; it is also a special case of the vector autoregressive model (VAR), which consists of a system of more than one interlocking stochastic difference equation in more than one evolving random variable.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Bayesian_network\n",
      "A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). While it is one of several forms of causal notation, causal networks are special cases of Bayesian networks. Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mehryar_Mohri\n",
      "Mehryar Mohri is a Professor and theoretical computer scientist[2] at the Courant Institute of Mathematical Sciences. He is also a Research Director \n",
      "at Google Research where he heads the Learning Theory team.\n",
      "\n",
      "https://en.wikipedia.org/wiki/GPT-2\n",
      "Generative Pre-trained Transformer 2 (GPT-2) is a large language model by OpenAI and the second in their foundational series of GPT models. GPT-2 was pre-trained on BookCorpus,[2] a dataset of over 7,000 self-published fiction books from various genres, and trained on a dataset of 8 million web pages.[3] It was partially released in February 2019, followed by full release of the 1.5-billion-parameter model on November 5, 2019.[4][5][6][7][8]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Concurrency_(computer_science)\n",
      "In computer science, concurrency is the ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or in partial order, without affecting the outcome.  This allows for parallel execution of the concurrent units, which can significantly improve overall speed of the execution in multi-processor and multi-core systems. In more technical terms, concurrency refers to the decomposability of a program, algorithm, or problem into order-independent or partially-ordered components or units of computation.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Oracle_Data_Mining\n",
      "Oracle Data Mining (ODM) is an option of Oracle Database Enterprise Edition. It contains several data mining and data analysis algorithms for classification, prediction, regression, associations, feature selection, anomaly detection, feature extraction, and specialized analytics. It provides means for the creation, management and operational deployment of data mining models inside the database environment.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Stochastic_process\n",
      "In probability theory and related fields, a stochastic (/stəˈkæstɪk/) or random process is a mathematical object usually defined as a sequence of random variables, where the index of the sequence has the interpretation of time. Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner. Examples include the growth of a bacterial population, an electrical current fluctuating due to thermal noise, or the movement of a gas molecule.[1][4][5] Stochastic processes have applications in many disciplines such as biology,[6] chemistry,[7] ecology,[8] neuroscience,[9] physics,[10] image processing, signal processing,[11] control theory,[12] information theory,[13] computer science,[14] and telecommunications.[15] Furthermore, seemingly random changes in financial markets have motivated the extensive use of stochastic processes in finance.[16][17][18]\n",
      "\n",
      "https://en.wikipedia.org/wiki/SAS_(software)#Components\n",
      "SAS (previously \"Statistical Analysis System\")[1] is a statistical software suite developed by SAS Institute for  data management, advanced analytics, multivariate analysis, business intelligence, criminal investigation,[2] and predictive analytics.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Hardware_acceleration\n",
      "Hardware acceleration is the use of computer hardware designed to perform specific functions more efficiently when compared to software running on a general-purpose central processing unit (CPU). Any transformation of data that can be calculated in software running on a generic CPU can also be calculated in custom-made hardware, or in some mix of both.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Project_Debater\n",
      "Project Debater is an IBM artificial intelligence project, designed to participate in a full live debate with expert human debaters.[1][2][3][4] It follows on from the Watson project which played Jeopardy![5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/K-SVD\n",
      "In applied mathematics, k-SVD is a dictionary learning algorithm for creating a dictionary for sparse representations, via a singular value decomposition approach. k-SVD is a generalization of the k-means clustering method, and it works by iteratively alternating between sparse coding the input data based on the current dictionary, and updating the atoms in the dictionary to better fit the data. It is structurally related to the expectation maximization (EM) algorithm.[1][2] k-SVD can be found widely in use in applications such as image processing, audio processing, biology, and document analysis.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Exploratory_data_analysis\n",
      "In statistics, exploratory data analysis (EDA) is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling and thereby contrasts traditional hypothesis testing. Exploratory data analysis has been promoted by John Tukey since 1970 to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA),[1][2] which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.\n",
      "\n",
      "https://en.wikipedia.org/wiki/U-Net\n",
      "U-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg.[1] The network is based on a fully convolutional neural network[2] whose architecture was modified and extended to work with fewer training images and to yield more precise segmentation. Segmentation of a 512 × 512 image takes less than a second on a modern GPU.\n",
      "\n",
      "https://en.wikipedia.org/wiki/GPT-J\n",
      "GPT-J or GPT-J-6B is an open-source large language model (LLM) developed by EleutherAI in 2021.[1] As the name suggests, it is a generative pre-trained transformer model designed to produce human-like text that continues from a prompt. The optional \"6B\" in the name refers to the fact that it has 6 billion parameters.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/ELKI\n",
      "ELKI (Environment for Developing KDD-Applications Supported by Index-Structures) is a data mining (KDD, knowledge discovery in databases) software framework developed for use in research and teaching. It was originally at the database systems research unit of Professor Hans-Peter Kriegel at the Ludwig Maximilian University of Munich, Germany, and now continued at the Technical University of Dortmund, Germany. It aims at allowing the development and evaluation of advanced data mining algorithms and their interaction with database index structures.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Leaf_node\n",
      "In computer science, a tree is a widely used abstract data type that represents a hierarchical tree structure with a set of connected nodes. Each node in the tree can be connected to many children (depending on the type of tree), but must be connected to exactly one parent,[1] except for the root node, which has no parent (i.e., the root node as the top-most node in the tree hierarchy). These constraints mean there are no cycles or \"loops\" (no node can be its own ancestor), and also that each child can be treated like the root node of its own subtree, making recursion a useful technique for tree traversal. In contrast to linear data structures, many trees cannot be represented by relationships between neighboring nodes (parent and children nodes of a node under consideration if they exists) in a single straight line (called edge or link between two adjacent nodes).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Enterprise_software\n",
      "Enterprise software, also known as enterprise application software (EAS), is computer software used to satisfy the needs of an organization rather than its individual users. Enterprise software is an integral part of a computer-based information system, handling a number of business operations, for example to enhance business and management reporting tasks, or support production operations and back office functions. Enterprise systems must process information at a relatively high speed.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Sample_(statistics)\n",
      "In statistics, quality assurance, and survey methodology, sampling is the selection of a subset or a statistical sample (termed sample for short) of individuals from within a statistical population to estimate characteristics of the whole population. Statisticians attempt to collect samples that are representative of the population. Sampling has lower costs and faster data collection compared to recording data from the entire population, and thus, it can provide insights in cases where it is infeasible to measure an entire population. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Echo_state_network\n",
      "An echo state network (ESN)[1][2] is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can produce or reproduce specific temporal patterns. The main interest of this network is that although its behavior is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.\n",
      "\n",
      "https://en.wikipedia.org/wiki/BIRCH\n",
      "BIRCH (balanced iterative reducing and clustering using hierarchies) is an unsupervised data mining algorithm used to perform hierarchical clustering over particularly large data-sets.[1] With modifications it can also be used to accelerate k-means clustering and Gaussian mixture modeling with the expectation–maximization algorithm.[2] An advantage of BIRCH is its ability to incrementally and dynamically cluster incoming, multi-dimensional metric data points in an attempt to produce the best quality clustering for a given set of resources (memory and time constraints). In most cases, BIRCH only requires a single scan of the database.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Networking_hardware\n",
      "\n",
      "Networking hardware, also known as network equipment or computer networking devices, are electronic devices that are required for communication and interaction between devices on a computer network. Specifically, they mediate data transmission in a computer network.[1] Units which are the last receiver or generate data are called hosts, end systems or data terminal equipment.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Isolation_forest\n",
      "Isolation Forest is an algorithm for data anomaly detection initially developed by Fei Tony Liu in 2008.[1] Isolation Forest detects anomalies using binary trees. The algorithm has a linear time complexity and a low memory requirement, which works well with high-volume data.[2][3]\n",
      "In essence, the algorithm relies upon the characteristics of anomalies, i.e., being few and different, in order to detect anomalies. No density estimation is performed in the algorithm. The algorithm is different from decision tree algorithms in that only the path-length measure or approximation is being used to generate the anomaly score, no leaf node statistics on class distribution or target value is needed.\n",
      "\n",
      "https://en.wikipedia.org/wiki/CURE_algorithm\n",
      "CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases[citation needed]. Compared with K-means clustering it is more robust to outliers and able to identify clusters having non-spherical shapes and size variances.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Enterprise_information_system\n",
      "An Enterprise Information System (EIS) is any kind of information system which improves the functions of enterprise business processes by integration. This means typically offering high quality of service, dealing with large volumes of data and capable of supporting some large and possibly complex organization or enterprise. An EIS must be able to be used by all parts and all levels of an enterprise.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/DBSCAN\n",
      "Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996.[1]\n",
      "It is a density-based clustering non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away).\n",
      "DBSCAN is one of the most common, and most commonly cited, clustering algorithms.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Network_scheduler\n",
      "A network scheduler, also called packet scheduler, queueing discipline (qdisc) or queueing algorithm, is an arbiter on a node in a packet switching communication network. It manages the sequence of network packets in the transmit and receive queues of the protocol stack and network interface controller. There are several network schedulers available for the different operating systems, that implement many of the existing network scheduling algorithms.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Pan-genome\n",
      "In the fields of molecular biology and genetics, a pan-genome (pangenome or supragenome) is the entire set of genes from all strains within a clade. More generally, it is the union of all the genomes of a clade.[2][3][4][5] The pan-genome can be broken down into a \"core pangenome\" that contains genes present in all individuals, a \"shell pangenome\" that contains genes present in two or more strains, and a \"cloud pangenome\" that contains genes only found in a single strain.[3][4][6] Some authors also refer to the cloud genome as \"accessory genome\" containing 'dispensable' genes present in a subset of the strains and strain-specific genes.[2][3][4] Note that the use of the term 'dispensable' has been questioned, at least in plant genomes, as accessory genes play \"an important role in genome evolution and in the complex interplay between the genome and the environment\".[5] The field of study of pangenomes is called pangenomics.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Edge_device\n",
      "In computer networking, an edge device is a device that provides an entry point into enterprise or service provider core networks. Examples include routers, routing switches, integrated access devices (IADs), multiplexers, and a variety of metropolitan area network (MAN) and wide area network (WAN) access devices.  Edge devices also provide connections into carrier and service provider networks. An edge device that connects a local area network to a high speed switch or backbone (such as an ATM switch) may be called an edge concentrator.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Template_talk:Machine_learning\n",
      "This section title and contents seem pretty much random to me. How are contents chosen? One regression, one random clustering algorithm, 4 standard classificators; but no decision tree; which is probably the grandfather of all classificators. --Chire (talk) 12:41, 22 October 2013 (UTC)Reply[reply]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:EditPage/Template:Machine_learning\n",
      "Copy and paste: – — ° ′ ″ ≈ ≠ ≤ ≥ ± − × ÷ ← → · §   Sign your posts on talk pages: ~~~~   Cite your sources: <ref></ref> \n",
      "\n",
      "https://en.wikipedia.org/wiki/Mathematical_optimization\n",
      "Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criterion, from some set of available alternatives.[1] It is generally divided into two subfields: discrete optimization and continuous optimization. Optimization problems arise in all quantitative disciplines from computer science and engineering[2] to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Watson_(computer)\n",
      "IBM Watson is a computer system capable of answering questions posed in natural language.[1] It was developed in IBM's DeepQA project by a research team led by principal investigator David Ferrucci.[2] Watson was named after IBM's founder and first CEO, industrialist Thomas J. Watson.[3][4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Predictive_modeling\n",
      "Predictive modelling uses statistics to predict outcomes.[1] Most often the event one wants to predict is in the future, but predictive modelling can be applied to any type of unknown event, regardless of when it occurred. For example, predictive models are often used to detect crimes and identify suspects, after the crime has taken place.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Theory_of_computation\n",
      "In theoretical computer science and mathematics, the theory of computation is the branch that deals with what problems can be solved on a model of computation, using an algorithm, how efficiently they can be solved or to what degree (e.g., approximate solutions versus precise ones).  The field is divided into three major branches: automata theory and formal languages, computability theory, and computational complexity theory, which are linked by the question: \"What are the fundamental capabilities and limitations of computers?\".[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_complexity_theory\n",
      "\n",
      "In theoretical computer science and mathematics, computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.\n",
      "\n",
      "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
      "In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951,[1] and later expanded by Thomas Cover.[2] It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Multilinear_subspace_learning\n",
      "Multilinear subspace learning is an approach for disentangling the causal factor of data formation and performing  dimensionality reduction.[1][2][3][4][5]   \n",
      "The Dimensionality reduction can be performed on a data tensor that contains a collection of observations have been vectorized,[1] or observations that are treated as matrices and concatenated into a data tensor.[6][7]  Here are some examples of data tensors whose observations are vectorized  or whose observations are matrices concatenated into data tensor images (2D/3D), video sequences (3D/4D), and hyperspectral cubes (3D/4D).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Automatic_differentiation\n",
      "In mathematics and computer algebra, automatic differentiation (auto-differentiation, autodiff, or AD), also called algorithmic differentiation, computational differentiation,[1][2] is a set of techniques to evaluate the partial derivative of a function specified by a computer program.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Logic_programming\n",
      "Logic programming is a programming, database and knowledge-representation and reasoning paradigm which is based on formal logic. A program, database or knowledge base in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain.  Major logic programming language families include Prolog, Answer Set Programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_economics\n",
      "Computational economics is an interdisciplinary research discipline that involves computer science, economics, and management science.[1]  This subject encompasses computational modeling of economic systems. Some of these areas are unique, while others established areas of economics by allowing robust data analytics and solutions of problems that would be arduous to research without computers and associated numerical methods.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Formal_methods\n",
      "In computer science, formal methods are mathematically rigorous techniques for the specification, development, analysis, and verification of software and hardware systems.[1] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Heuristic_(computer_science)\n",
      "In mathematical optimization and computer science, heuristic (from Greek εὑρίσκω \"I find, discover\") is a technique designed for problem solving more quickly when classic methods are too slow for finding an exact or approximate solution, or when classic methods fail to find any exact solution in a search space.  This is achieved by trading optimality, completeness, accuracy, or precision for speed.  In a way, it can be considered a shortcut.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Cheminformatics\n",
      "Cheminformatics (also known as chemoinformatics) refers to the use of physical chemistry theory with computer and information science techniques—so called \"in silico\" techniques—in application to a range of descriptive and prescriptive problems in the field of chemistry, including in its applications to biology and related molecular fields. Such in silico techniques are used, for example, by pharmaceutical companies and in academic settings to aid and inform the process of drug discovery, for instance in the design of well-defined combinatorial libraries of synthetic compounds, or to assist in structure-based drug design. The methods can also be used in chemical and allied industries, and such fields as environmental science and pharmacology, where chemical processes are involved or studied.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Statistical_inference\n",
      "Statistical inference is the process of using data analysis to infer properties of an underlying distribution of probability.[1] Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Statistical_classification\n",
      "In statistics, classification is the problem of identifying which of a set of categories (sub-populations) an observation (or observations) belongs to.  Examples are assigning a given email to the \"spam\" or \"non-spam\" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Sequence_mining\n",
      "Sequential pattern mining is a topic of data mining concerned with finding statistically relevant patterns between data examples where the values are delivered in a sequence.[1][2] It is usually presumed that the values are discrete, and thus time series mining is closely related, but usually considered a different activity.  Sequential pattern mining is a special case of structured data mining.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Programming_team\n",
      "A programming team is a team of people who develop or maintain computer software.[1]  They may be organised in numerous ways, but the egoless programming team and chief programmer team have been common structures.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Statistics\n",
      "Statistics (from German: Statistik, orig. \"description of a state, a country\")[1][2] is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data.[3][4][5] In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.[6]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Distributed_computing\n",
      "A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another.[1][2] Distributed computing is a field of computer science that studies distributed systems. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Multithreading_(computer_architecture)\n",
      "In computer architecture, multithreading is the ability of a central processing unit (CPU) (or a single core in a multi-core processor) to provide multiple threads of execution concurrently, supported by the operating system. This approach differs from multiprocessing. In a multithreaded application, the threads share the resources of a single or multiple cores, which include the computing units, the CPU caches, and the translation lookaside buffer (TLB).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Randomized_algorithm\n",
      "A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic or procedure. The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the \"average case\" over all possible choices of random determined by the random bits; thus either the running time, or the output (or both) are random variables.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_chemistry\n",
      "Computational chemistry is a branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into computer programs, to calculate the structures and properties of molecules, groups of molecules, and solids. It is essential because, apart from relatively recent results concerning the hydrogen molecular ion (dihydrogen cation, see references therein for more details), the quantum many-body problem cannot be solved analytically, much less in closed form.  While computational results normally complement the information obtained by chemical experiments, it can in some cases predict hitherto unobserved chemical phenomena. It is widely used in the design of new drugs and materials.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Caffe_(software)\n",
      "Caffe (Convolutional Architecture for Fast Feature Embedding) is a deep learning framework, originally developed at University of California, Berkeley. It is open source, under a BSD license.[4] It is written in C++, with a Python interface.[5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mean_shift\n",
      "Mean shift is a non-parametric feature-space mathematical analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm.[1] Application domains include cluster analysis in computer vision and image processing.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computing_platform\n",
      "A computing platform, digital platform,[1] or software platform is an environment in which software is executed. It may be the hardware or the operating system (OS), a web browser and associated application programming interfaces, or other underlying software, as long as the program code is executed. Computing platforms have different abstraction levels, including a computer architecture, an OS, or runtime libraries.[2] A computing platform is the stage on which computer programs can run.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Cybernetics\n",
      "Cybernetics is a transdisciplinary approach for exploring regulatory systems with feedback, their structures, constraints, and possibilities. Cybernetics is relevant to the study of systems, such as mechanical, physical, biological, cognitive, and social.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Logistic_regression\n",
      "In statistics, the logistic model (or logit model) is a statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables. In regression analysis, logistic regression[1] (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination). Formally, in binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled \"0\" and \"1\", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \"1\" can vary between 0 (certainly the value \"0\") and 1 (certainly the value \"1\"), hence the labeling;[2] the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See § Background and § Definition for formal mathematics, and § Example for a worked example.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Discrete_mathematics\n",
      "Discrete mathematics is the study of mathematical structures that can be considered \"discrete\" (in a way analogous to discrete variables, having a bijection with the set of natural numbers) rather than \"continuous\" (analogously to continuous functions). Objects studied in discrete mathematics include integers, graphs, and statements in logic.[1][2][3] By contrast, discrete mathematics excludes topics in \"continuous mathematics\" such as real numbers, calculus or Euclidean geometry. Discrete objects can often be enumerated by integers; more formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets[4] (finite sets or sets with the same cardinality as the natural numbers). However, there is no exact definition of the term \"discrete mathematics\".[5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm\n",
      "In statistics, an expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables.[1] The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.\n",
      "\n",
      "https://en.wikipedia.org/wiki/XGBoost\n",
      "XGBoost[2] (eXtreme Gradient Boosting) is an open-source software library which provides a regularizing gradient boosting framework for C++, Java, Python,[3] R,[4] Julia,[5] Perl,[6] and Scala. It works on Linux, Microsoft Windows,[7] and macOS.[8] From the project description, it aims to provide a \"Scalable, Portable and Distributed Gradient Boosting (GBM, GBRT, GBDT) Library\". It runs on a single machine, as well as the distributed processing frameworks Apache Hadoop, Apache Spark, Apache Flink, and Dask.[9][10]\n",
      "\n",
      "https://en.wikipedia.org/wiki/LIONsolver\n",
      "LIONsolver is an integrated software for data mining, business intelligence, analytics, and modeling and reactive business intelligence approach.[1] A non-profit version is also available as LIONoso.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Non-negative_matrix_factorization\n",
      "Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Template_talk:Differentiable_computing\n",
      "I don't want to remove the group because I didn't contribute anything to the template and removing stuff added by other users seems rude to me. But now using JS or GO for programming gradient descent based algorithm is not that uncommon, does it implies that wikipedia should also add JS/GO to the group. To put it simply, I don't think having a group for popular programming languages used for Differentiable computing helps anyone but rather might push the idea that Python is the language for AI ignoring many other important stuff, for example the C++ core of the python interface. I don't hate python/julia and not a Go/JS fanboy. -- 1e100 (talk) 11:16, 18 December 2021 (UTC)Reply[reply]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Data_collection\n",
      "Data collection or data gathering is the process of gathering and measuring information on targeted variables in an established system, which then enables one to answer relevant questions and evaluate outcomes. Data collection is a research component in all study fields, including physical and social sciences, humanities,[2] and business. While methods vary by discipline, the emphasis on ensuring accurate and honest collection remains the same. The goal for all data collection is to capture evidence that allows data analysis to lead to the formulation of credible answers to the questions that have been posed.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Software_configuration_management\n",
      "In software engineering, software configuration management (SCM or S/W CM; also expanded as source configuration management process and software change and configuration management[1]) is the task of tracking and controlling changes in the software, part of the larger cross-disciplinary field of configuration management.[2]  SCM practices include revision control and the establishment of baselines.  If something goes wrong, SCM can determine the \"what, when, why and who\" of the change.  If a configuration is working well, SCM can determine how to replicate it across many hosts.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Tomasz_Imieli%C5%84ski\n",
      "Tomasz Imieliński (born July 11, 1954, in Toruń, Poland) is a Polish-American computer scientist, most known in the areas of data mining, mobile computing, data extraction, and search engine technology. He is currently a professor of computer science at Rutgers University in New Jersey, United States.\n",
      "\n",
      "https://en.wikipedia.org/wiki/False_negative_rate\n",
      "A false positive is an error in binary classification in which a test result incorrectly indicates the presence of a condition (such as a disease when the disease is not present), while a false negative is the opposite error, where the test result incorrectly indicates the absence of a condition when it is actually present. These are the two kinds of errors in a binary test, in contrast to the two kinds of correct result (a true positive and a true negative). They are also known in medicine as a false positive (or false negative) diagnosis, and in statistical classification as a false positive (or false negative) error.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Factor_analysis\n",
      "Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors plus \"error\" terms, hence factor analysis can be thought of as a special case of errors-in-variables models.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\n",
      "t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map. It is based on Stochastic Neighbor Embedding originally developed by Geoffrey Hinton and Sam Roweis,[1] where Laurens van der Maaten proposed the t-distributed variant.[2] It is a nonlinear dimensionality reduction technique for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Crossover_(genetic_algorithm)\n",
      "In genetic algorithms and evolutionary computation, crossover, also called recombination, is a genetic operator used to combine the genetic information of two parents to generate new offspring. It is one way to stochastically generate new solutions from an existing population, and is analogous to the crossover that happens during sexual reproduction in biology. Solutions can also be generated by cloning an existing solution, which is analogous to asexual reproduction. Newly generated solutions may be mutated before being added to the population.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Operational_definition\n",
      "An operational definition specifies concrete, replicable procedures designed to represent a construct. In the words of American psychologist S.S. Stevens (1935), \"An operation is the performance which we execute in order to make known a concept.\"[1][2] For example, an operational definition of \"fear\" (the construct) often includes measurable physiologic responses that occur in response to a perceived threat. Thus, \"fear\" might be operationally defined as specified changes in heart rate, galvanic skin response, pupil dilation, and blood pressure.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/White-box_testing\n",
      "White-box testing (also known as clear box testing, glass box testing, transparent box testing, and structural testing) is a method of software testing that tests internal structures or workings of an application, as opposed to its functionality (i.e. black-box testing). In white-box testing, an internal perspective of the system is used to design test cases. The tester chooses inputs to exercise paths through the code and determine the expected outputs. This is analogous to testing nodes in a circuit, e.g. in-circuit testing (ICT).\n",
      "White-box testing can be applied at the unit, integration and system levels of the software testing process. Although traditional testers tended to think of white-box testing as being done at the unit level, it is used for integration and system testing more frequently today. It can test paths within a unit, paths between units during integration, and between subsystems during a system–level test. Though this method of test design can uncover many errors or problems, it has the potential to miss unimplemented parts of the specification or missing requirements. Where white-box testing is design-driven,[1] that is, driven exclusively by agreed specifications of how each component of software is required to behave (as in DO-178C and ISO 26262 processes), white-box test techniques can accomplish assessment for unimplemented or missing requirements.\n",
      "\n",
      "https://en.wikipedia.org/wiki/ROOT\n",
      "ROOT is an object-oriented computer program and library developed by CERN. It was originally designed for particle physics data analysis and contains several features specific to the field, but it is also used in other applications such as astronomy and data mining.  The latest minor release is 6.28, as of 2023-02-03.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Tomographic_reconstruction\n",
      "Tomographic reconstruction is a type of multidimensional inverse problem where the challenge is to yield an estimate of a specific system from a finite number of projections. The mathematical basis for tomographic imaging was laid down by Johann Radon. A notable example of applications is the reconstruction of computed tomography (CT) where cross-sectional images of patients are obtained in non-invasive manner. Recent developments have seen the Radon transform and its inverse used for tasks related to realistic object insertion required for testing and evaluating computed tomography use in airport security.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Information_security\n",
      "\n",
      "Information security, sometimes shortened to InfoSec,[1] is the practice of protecting information by mitigating information risks. It is part of information risk management.[2][3] It typically involves preventing or reducing the probability of unauthorized or inappropriate access to data or the unlawful use, disclosure, disruption, deletion, corruption, modification, inspection, recording, or devaluation of information.[citation needed] It also involves actions intended to reduce the adverse impacts of such incidents. Protected information may take any form, e.g., electronic or physical, tangible (e.g., paperwork), or intangible (e.g., knowledge).[4][5] Information security's primary focus is the balanced protection of data confidentiality, integrity, and availability (also known as the \"CIA\" triad) while maintaining a focus on efficient policy implementation, all without hampering organization productivity.[6] This is largely achieved through a structured risk management process that involves: \n",
      "\n",
      "https://en.wikipedia.org/wiki/Robert_Tibshirani\n",
      "Robert Tibshirani FRS FRSC (born July 10, 1956) is a professor in the Departments of Statistics and Biomedical Data Science at Stanford University. He was a professor at the University of Toronto from 1985 to 1998. In his work, he develops statistical tools for the analysis of complex datasets, most recently in genomics and proteomics.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Social_network\n",
      "A social network is a social structure made up of a set of social actors (such as individuals or organizations), sets of dyadic ties, and other social interactions between actors. The social network perspective provides a set of methods for analyzing the structure of whole social entities as well as a variety of theories explaining the patterns observed in these structures.[1] The study of these structures uses social network analysis to identify local and global patterns, locate influential entities, and examine network dynamics.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Interaction_design\n",
      "\n",
      "Interaction design, often abbreviated as IxD, is \"the practice of designing interactive digital products, environments, systems, and services.\"[1]: xxvii, 30  While interaction design has an interest in form (similar to other design fields), its main area of focus rests on behavior.[1]: xxvii, 30  Rather than analyzing how things are, interaction design synthesizes and imagines things as they could be. This element of interaction design is what characterizes IxD as a design field, as opposed to a science or engineering field.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/RCASE\n",
      "Root Cause Analysis Solver Engine (informally RCASE) is a proprietary algorithm developed from research originally at the Warwick Manufacturing Group (WMG) at Warwick University.[1][2] RCASE development commenced in 2003 to provide an automated version of root cause analysis, the method of problem solving that tries to identify the root causes of faults or problems.[3] RCASE is now owned by the spin-out company Warwick Analytics where it is being applied to automated predictive analytics software.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Covariance_function\n",
      "In probability theory and statistics, the covariance function describes how much two random variables change together (their covariance) with varying spatial or temporal separation. For a random field or stochastic process Z(x) on a domain D, a covariance function C(x, y) gives the covariance of the values of the random field at the two locations x and y:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Backpropagation\n",
      "As a machine-learning algorithm, backpropagation is a crucial step in a common method used to iteratively train a neural network model. It is used to calculate the necessary parameter adjustments, to gradually minimize error.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_mathematics\n",
      "\n",
      "Computational mathematics is an area of mathematics devoted to the interaction between mathematics and computer computation.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Evolutionary_algorithm\n",
      "In computational intelligence (CI), an evolutionary algorithm (EA) is a subset of evolutionary computation,[1] a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
      "In statistics, naive Bayes classifiers are a family of linear \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features (see Bayes classifier). They are among the simplest Bayesian network models,[1] but coupled with kernel density estimation, they can achieve high accuracy levels.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Reservoir_computing\n",
      "Reservoir computing is a framework for computation derived from recurrent neural network theory that maps input signals into higher dimensional computational spaces through the dynamics of a fixed, non-linear system called a reservoir.[1] After the input signal is fed into the reservoir, which is treated as a \"black box,\" a simple readout mechanism is trained to read the state of the reservoir and map it to the desired output.[1] The first key benefit of this framework is that training is performed only at the readout stage, as the reservoir dynamics are fixed.[1] The second is that the computational power of naturally available systems, both classical and quantum mechanical, can be used to reduce the effective computational cost.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Algorithmic_efficiency\n",
      "\n",
      "In computer science, algorithmic efficiency is a property of an algorithm which relates to the amount of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on the usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.\n",
      "\n",
      "https://en.wikipedia.org/wiki/AlexNet\n",
      "AlexNet is the name of a convolutional neural network (CNN) architecture, designed by Alex Krizhevsky in collaboration with Ilya Sutskever and Geoffrey Hinton, who was Krizhevsky's Ph.D. advisor at the University of Toronto.[1][2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Apache_SystemML\n",
      "Apache SystemDS (Previously, Apache SystemML) is an open source ML system for the end-to-end data science lifecycle. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Probabilistic_reasoning\n",
      "Probabilistic logic (also probability logic and probabilistic reasoning) involves the use of probability and logic to deal with uncertain situations. Probabilistic logic extends traditional logic truth tables with probabilistic expressions. A difficulty of probabilistic logics is their tendency to multiply the computational complexities of their probabilistic and logical components.  Other difficulties include the possibility of counter-intuitive results, such as in case of belief fusion in Dempster–Shafer theory. Source trust and epistemic uncertainty about the probabilities they provide, such as defined in subjective logic, are additional elements to consider. The need to deal with a broad variety of contexts and issues has led to many different proposals.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Markov_decision_process\n",
      "In mathematics, a Markov decision process (MDP) is a discrete-time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming. MDPs were known at least as early as the 1950s;[1] a core body of research on Markov decision processes resulted from Ronald Howard's 1960 book, Dynamic Programming and Markov Processes.[2] They are used in many disciplines, including robotics, automatic control, economics and manufacturing. The name of MDPs comes from the Russian mathematician Andrey Markov as they are an extension of Markov chains.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Functional_programming\n",
      "In computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions. It is a declarative programming paradigm in which function definitions are trees of expressions that map values to other values, rather than a sequence of imperative statements which update the running state of the program.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Market_basket_analysis\n",
      "Affinity analysis falls under the umbrella term of data mining which uncovers meaningful correlations between different entities according to their co-occurrence in a data set. In almost all systems and processes, the application of affinity analysis can extract significant knowledge about the unexpected trends[citation needed]. In fact, affinity analysis takes advantages of studying attributes that go together which helps uncover the hidden pattens in a big data through generating association rules. Association rules mining procedure is two-fold: first, it finds all frequent attributes in a data set and, then generates association rules satisfying some predefined criteria, support and confidence, to identify the most important relationships in the frequent itemset. The first step in the process is to count the co-occurrence of attributes in the data set. Next, a subset is created called the frequent itemset. The association rules mining takes the form of if a condition or feature (A) is present then another condition or feature (B) exists. The first condition or feature (A) is called antecedent and the latter (B) is known as consequent. This process is repeated until no additional frequent itemsets are found.  There are two important metrics for performing the association rules mining technique: support and confidence. Also, a priori algorithm is used to reduce the search space for the problem.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Uncertainty_quantification\n",
      "Uncertainty quantification (UQ) is the science of quantitative characterization and estimation of uncertainties in both computational and real world applications. It tries to determine how likely certain outcomes are if some aspects of the system are not exactly known. An example would be to predict the acceleration of a human body in a head-on crash with another car: even if the speed was exactly known, small differences in the manufacturing of individual cars, how tightly every bolt has been tightened, etc., will lead to different results that can only be predicted in a statistical sense.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Middleware\n",
      "Middleware is a type of computer software programme that provides services to software applications beyond those available from the operating system. It can be described as \"software glue\".[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Independent_component_analysis\n",
      "In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that at most one subcomponent is Gaussian and that the subcomponents are statistically independent from each other.[1] ICA is a special case of blind source separation. A common example application is the \"cocktail party problem\" of listening in on one person's speech in a noisy room.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Computer_science\n",
      "This category has the following 25 subcategories, out of 25 total.\n",
      "\n",
      "https://en.wikipedia.org/wiki/OpenAI_Five\n",
      "\n",
      "OpenAI Five is a computer program by OpenAI that plays the five-on-five video game Dota 2. Its first public appearance occurred in 2017, where it was demonstrated in a live one-on-one game against the professional player Dendi, who lost to it. The following year, the system had advanced to the point of performing as a full team of five, and began playing against and showing the capability to defeat professional teams.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Simulation-based_optimization\n",
      "Simulation-based optimization (also known as simply simulation optimization) integrates optimization techniques into simulation modeling and analysis. Because of the complexity of the simulation, the objective function may become difficult and expensive to evaluate. Usually, the underlying simulation model is stochastic, so that the objective function must be estimated using statistical estimation techniques (called output analysis in simulation methodology).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Gaussian_processes\n",
      "In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Word_processor\n",
      "A word processor (WP)[1][2] is a device or computer program that provides for input, editing, formatting, and output of text, often with some additional features.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Software_suite\n",
      "A software suite[1] (also known as an application suite) is a collection of computer programs (application software, or programming software) of related functionality, sharing a similar user interface and the ability to easily exchange data with each other.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Probability_theory\n",
      "Probability theory or probability calculus is the branch of mathematics concerned with probability. Although there are several different probability interpretations, probability theory treats the concept in a rigorous mathematical manner by expressing it through a set of axioms. Typically these axioms formalise probability in terms of a probability space, which assigns a measure taking values between 0 and 1, termed the probability measure, to a set of outcomes called the sample space. Any specified subset of the sample space is called an event.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Software_maintenance\n",
      "Software maintenance in software engineering is the modification of a software product after delivery to correct faults, to improve performance or other attributes.[1][2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Polynomial_regression\n",
      "In statistics, polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y |x). Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function E(y | x) is linear in the unknown parameters that are estimated from the data.  For this reason, polynomial regression is considered to be a special case of multiple linear regression.\n",
      "\n",
      "https://en.wikipedia.org/wiki/International_Conference_on_Intelligent_Robots_and_Systems\n",
      "IROS, the IEEE/RSJ International Conference on Intelligent Robots and Systems,[1] is an annual academic conference covering advances in robotics.[2] It is one of the premier conferences of its field (alongside ICRA, International Conference on Robotics and Automation) with an 'A' rating from the Australian Ranking of ICT Conferences obtained in 2010 and an 'A1' rating from the Brazilian ministry of education in 2012.[3][4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Matrix_decomposition\n",
      "In the mathematical discipline of linear algebra, a matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. There are many different matrix decompositions; each finds use among a particular class of problems.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Geographic_information_system\n",
      "A geographic information system (GIS) consists of integrated computer hardware and software that store, manage, analyze, edit, output, and visualize geographic data.[1][2] Much of this often happens within a spatial database, however, this is not essential to meet the definition of a GIS.[1] In a broader sense, one may consider such a system also to include human users and support staff, procedures and workflows, the body of knowledge of relevant concepts and methods, and institutional organizations.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Tensor\n",
      "In mathematics, a tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects related to a vector space. Tensors may map between different objects such as vectors, scalars, and even other tensors. There are many types of tensors, including scalars and vectors (which are the simplest tensors), dual vectors, multilinear maps between vector spaces, and even some operations such as the dot product. Tensors are defined independent of any basis, although they are often referred to by their components in a basis related to a particular coordinate system; those components form an array, which can be thought of as a high-dimensional matrix. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Network_service\n",
      "In computer networking, a network service is an application running at the network application layer and above, that provides data storage, manipulation, presentation, communication or other capability which is often implemented using a client–server or peer-to-peer architecture based on application layer network protocols.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Very_Large_Scale_Integration\n",
      "Very-large-scale integration (VLSI) is the process of creating an integrated circuit (IC) by combining millions or billions of MOS transistors onto a single chip. VLSI began in the 1970s when MOS integrated circuit (Metal Oxide Semiconductor) chips were developed and then widely adopted, enabling complex semiconductor and telecommunication technologies. The microprocessor and memory chips are VLSI devices.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Real-time_computing\n",
      "Real-time computing (RTC) is the computer science term for hardware and software systems subject to a \"real-time constraint\", for example from event to system response.[1] Real-time programs must guarantee response within specified time constraints, often referred to as \"deadlines\".[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Network_architecture\n",
      "Network architecture is the design of a computer network. It is a framework for the specification of a network's physical components and their functional organization and configuration, its operational principles and procedures, as well as communication protocols used.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Template_talk:Computer_science\n",
      "This part is a bit short. But does OLAP really belong into here? is it really \"major\"?\n",
      "\n",
      "https://en.wikipedia.org/wiki/ACM_Computing_Surveys\n",
      "ACM Computing Surveys is peer-reviewed quarterly scientific journal and is published by the Association for Computing Machinery. It publishes survey articles and tutorials related to computer science and computing. The journal was established in 1969 with William S. Dorn as founding editor-in-chief.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Neural_network_with_dark_background.png\n",
      "Original file ‎(1,280 × 1,039 pixels, file size: 837 KB, MIME type: image/png)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Operations_research\n",
      "\n",
      "Operations research (British English: operational research) (U.S. Air Force Specialty Code: Operations Analysis), often shortened to the initialism OR, is a discipline that deals with the development and application of analytical methods to improve decision-making.[1] The term management science is occasionally used as a synonym.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Microcontrollers\n",
      "A microcontroller (MC, UC, or μC) or microcontroller unit (MCU) is a small computer on a single integrated circuit. A microcontroller contains one or more CPUs (processor cores) along with memory and programmable input/output peripherals. Program memory in the form of ferroelectric RAM, NOR flash or OTP ROM is also often included on chip, as well as a small amount of RAM. Microcontrollers are designed for embedded applications, in contrast to the microprocessors used in personal computers or other general purpose applications consisting of various discrete chips.\n",
      "\n",
      "https://en.wikipedia.org/wiki/CiteSeerX_(identifier)\n",
      "CiteSeerX (formerly called CiteSeer) is a public search engine and digital library for scientific and academic papers, primarily in the fields of computer and information science.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Intrusion_detection\n",
      "An intrusion detection system (IDS; also intrusion prevention system or IPS) is a device or software application that monitors a network or systems for malicious activity or policy violations.[1] Any intrusion activity or violation is typically reported either to an administrator or collected centrally using a security information and event management (SIEM) system. A SIEM system combines outputs from multiple sources and uses alarm filtering techniques to distinguish malicious activity from false alarms.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Software_quality\n",
      "In the context of software engineering, software quality refers to two related but distinct notions:[citation needed]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Possibility_theory\n",
      "Possibility theory is a mathematical theory for dealing with certain types of uncertainty and is an alternative to probability theory. It uses measures of possibility and necessity between 0 and 1, ranging from impossible to possible and unnecessary to necessary, respectively. Professor Lotfi Zadeh first introduced possibility theory in 1978 as an extension of his theory of fuzzy sets and fuzzy logic. Didier Dubois and Henri Prade further contributed to its development. Earlier, in the 1950s, economist G. L. S. Shackle proposed the min/max algebra to describe degrees of potential surprise.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Data_quality\n",
      "Data quality refers to the state of qualitative or quantitative pieces of information. There are many definitions of data quality, but data is generally considered high quality if it is \"fit for [its] intended uses in operations, decision making and planning\".[1][2][3] Moreover, data is deemed of high quality if it correctly represents the real-world construct to which it refers. Furthermore, apart from these definitions, as the number of data sources increases, the question of internal data consistency becomes significant, regardless of fitness for use for any particular external purpose. People's views on data quality can often be in disagreement, even when discussing the same set of data used for the same purpose. When this is the case, data governance is used to form agreed upon definitions and standards for data quality. In such cases, data cleansing, including standardization, may be required in order to ensure data quality.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Dempster%E2%80%93Shafer_theory\n",
      "The theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory (DST), is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. First introduced by Arthur P. Dempster[1] in the context of statistical inference, the theory was later developed by Glenn Shafer into a general framework for modeling epistemic uncertainty—a mathematical theory of evidence.[2][3] The theory allows one to combine evidence from different sources and arrive at a degree of belief (represented by a mathematical object called belief function) that takes into account all the available evidence.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Softmax_function\n",
      "The softmax function, also known as softargmax[1]: 184  or normalized exponential function,[2]: 198  converts a vector of K real numbers into a probability distribution of K possible outcomes. It is a generalization of the logistic function to multiple dimensions, and used in multinomial logistic regression. The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes, based on Luce's choice axiom.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Jerome_H._Friedman\n",
      "Jerome Harold Friedman (born December 29, 1939) is an American statistician, consultant and Professor of Statistics at Stanford University, known for his contributions in the field of statistics and data mining.[2][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Neuron\n",
      "Within a nervous system, a neuron, neurone, or nerve cell is an electrically excitable cell that fires electric signals called action potentials across a neural network. Neurons communicate with other cells via synapses, which are specialized connections that commonly use minute amounts of chemical neurotransmitters to pass the electric signal from the presynaptic neuron to the target cell through the synaptic gap. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Continuous_production\n",
      "Continuous production is a flow production method used to manufacture, produce, or process materials without interruption.  Continuous production is called a continuous process or a continuous flow process because the materials, either dry bulk or fluids that are being processed are continuously in motion, undergoing chemical reactions or subject to mechanical or heat treatment.  Continuous processing is contrasted with batch production.\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Colored_neural_network.svg\n",
      "Original file ‎(SVG file, nominally 296 × 356 pixels, file size: 206 KB)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Leo_Breiman\n",
      "Leo Breiman (January 27, 1928 – July 5, 2005) was a distinguished statistician at the University of California, Berkeley. He was the recipient of numerous honors and awards,[citation needed] and was a member of the United States National Academy of Sciences.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Density_estimation\n",
      "In statistics, probability density estimation or simply density estimation is the construction of an estimate, based on observed data, of an unobservable underlying probability density function.  The unobservable density function is thought of as the density according to which a large population is distributed; the data are usually thought of as a random sample from that population.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
      "In medicine and statistics, sensitivity and specificity mathematically describe the accuracy of a test that reports the presence or absence of a medical condition. If individuals who have the condition are considered \"positive\" and those who do not are considered \"negative\", then sensitivity is a measure of how well a test can identify true positives and specificity is a measure of how well a test can identify true negatives:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Conditional_independence\n",
      "In probability theory, conditional independence describes situations wherein an observation is irrelevant or redundant when evaluating the certainty of a hypothesis. Conditional independence is usually formulated in terms of conditional probability, as a special case where the probability of the hypothesis given the uninformative observation is equal to the probability without. If \n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "\n",
      "\n",
      "{\\displaystyle A}\n",
      "\n",
      " is the hypothesis, and \n",
      "\n",
      "\n",
      "\n",
      "B\n",
      "\n",
      "\n",
      "{\\displaystyle B}\n",
      "\n",
      " and \n",
      "\n",
      "\n",
      "\n",
      "C\n",
      "\n",
      "\n",
      "{\\displaystyle C}\n",
      "\n",
      " are observations, conditional independence can be stated as an equality:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Articles_with_unsourced_statements_from_May_2022\n",
      "This category combines all articles with unsourced statements from May 2022 (2022-05) to enable us to work through the backlog more systematically. It is a member of Category:Articles with unsourced statements.\n",
      "To add an article to this category add     {{Citation needed|date=May 2022}} to the article. If you omit the date a bot will add it for you at some point.\n",
      "\n",
      "https://en.wikipedia.org/wiki/New_England_Journal_of_Medicine\n",
      "The New England Journal of Medicine (NEJM) is a weekly medical journal published by the Massachusetts Medical Society. It is among the most prestigious peer-reviewed medical journals[1][2] as well as the oldest continuously published one.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Errors_and_residuals\n",
      "In statistics and optimization, errors and residuals are two closely related and easily confused measures of the deviation of an observed value of an element of a statistical sample from its \"true value\" (not necessarily observable). The error of an observation is the deviation of the observed value from the true value of a quantity of interest (for example, a population mean). The residual is the difference between the observed value and the estimated value of the quantity of interest (for example, a sample mean). The distinction is most important in regression analysis, where the concepts are sometimes called the regression errors and regression residuals and where they lead to the concept of studentized residuals.\n",
      "In econometrics, \"errors\" are also called disturbances.[1][2][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Network_performance\n",
      "Network performance refers to measures of service quality of a network as seen by the customer.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Template:Computer_science\n",
      "This template's initial visibility currently defaults to autocollapse, meaning that if there is another collapsible item on the page (a navbox, sidebar, or table with the collapsible attribute), it is hidden apart from its title bar; if not, it is fully visible.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:All_articles_with_unsourced_statements\n",
      "\n",
      "This is a category to help keep count of the total number of articles with the {{citation needed}} template.  They should all be in one of the dated categories, which can be found at Category:Articles with unsourced statements.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Bank_fraud\n",
      "Bank fraud is the use of potentially illegal means to obtain money, assets, or other property owned or held by a financial institution, or to obtain money from depositors by fraudulently posing as a bank or other financial institution.[1] In many instances, bank fraud is a criminal offence. While the specific elements of particular banking fraud laws vary depending on jurisdictions, the term bank fraud applies to actions that employ a scheme or artifice, as opposed to bank robbery or theft. For this reason, bank fraud is sometimes considered a white-collar crime.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Short_description_is_different_from_Wikidata\n",
      "This category contains articles with short descriptions that do not match the description field on Wikidata. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Total_operating_characteristic\n",
      "The total operating characteristic (TOC)  is a statistical method to compare a Boolean variable versus a rank variable. TOC can measure the ability of an index variable to diagnose either presence or absence of a characteristic. The diagnosis of presence or absence depends on whether the value of the index is above a threshold. TOC considers multiple possible thresholds. Each threshold generates a two-by-two contingency table, which contains four entries: hits, misses, false alarms, and correct rejections.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Angoss\n",
      "Angoss Software Corporation, headquartered in Toronto, Ontario, Canada, with offices in the United States and UK, acquired by Datawatch and now owned by Altair, was a provider of predictive analytics systems through software licensing and services. Angoss' customers represent industries including finance, insurance, mutual funds, retail, health sciences, telecom and technology. The company was founded in 1984, and publicly traded on the TSX Venture Exchange from 2008-2013 under the ticker symbol ANC.[citation needed]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Portal:Current_events\n",
      "Edit instructions\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Computer_Retro.svg\n",
      "Original file ‎(SVG file, nominally 512 × 512 pixels, file size: 499 KB)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Learning\n",
      "Learning is the process of acquiring new or modifying existing knowledge, behaviors, skills, values, or preferences based on instruction.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Knowledge_discovery\n",
      "Knowledge extraction is the creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criterion is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Generalized_linear_model\n",
      "In statistics, a generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Computational_science\n",
      "\n",
      "Computational science, also known as scientific computing, technical computing or scientific computation (SC), is a division of science that uses advanced computing capabilities to understand and solve complex physical problems. This includes \n",
      "\n",
      "https://en.wikipedia.org/wiki/Mathematical_software\n",
      "Mathematical software is software used to model, analyze or calculate numeric, symbolic or geometric data.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Binary_classifier\n",
      "Binary classification is the task of classifying the elements of a set into two groups (each called class) on the basis of a classification rule. Typical binary classification problems include:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Robot_locomotion\n",
      "Robot locomotion is the collective name for the various methods that robots use to transport themselves from place to place.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Programming_paradigm\n",
      "Programming paradigms are a way to classify programming languages based on their features. Languages can be classified into multiple paradigms.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Dynamic_programming\n",
      "Dynamic programming is both a mathematical optimization method and an algorithmic paradigm. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Overfitting\n",
      "In mathematical modeling, overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably\".[1] An overfitted model is a mathematical model that contains more parameters than can be justified by the data.[2] In a mathematical sense, these parameters represent the degree of a polynomial. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.[3]: 45 \n",
      "\n",
      "https://en.wikipedia.org/wiki/Kernel_regression\n",
      "In statistics, kernel regression is a non-parametric technique to estimate the conditional expectation of a random variable. The objective is to find a non-linear relation between a pair of random variables X and Y.\n",
      "\n",
      "https://en.wikipedia.org/wiki/MOA_(Massive_Online_Analysis)\n",
      "Massive Online Analysis (MOA) is a free open-source software project specific for data stream mining with concept drift. It is written in Java and developed at the University of Waikato, New Zealand.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Behaviorism\n",
      "\n",
      "Behaviorism (also spelled behaviourism)[1] is a systematic approach to understanding the behavior of humans and other animals.[2] It assumes that behavior is either a reflex evoked by the pairing of certain antecedent stimuli in the environment, or a consequence of that individual's history, including especially reinforcement and punishment contingencies, together with the individual's current motivational state and controlling stimuli. Although behaviorists generally accept the important role of heredity in determining behavior, they focus primarily on environmental events.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Gradient_descent\n",
      "Gradient descent (also often called steepest descent) is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for finding a local minimum of a differentiable multivariate function\n",
      "\n",
      "https://en.wikipedia.org/wiki/Map_(mathematics)\n",
      "In mathematics, a map or mapping is a function in its general sense.[1]  These terms may have originated as from the process of making a geographical map: mapping the Earth surface to a sheet of paper.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
      "Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Software_design\n",
      "Software design is the process by which an agent creates a specification of a software artifact intended to accomplish goals, using a set of primitive components and subject to constraints.[1] The term is sometimes used broadly to refer to \"all the activity involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying\" the software, or more specifically \"the activity following requirements specification and before programming, as ... [in] a stylized software engineering process.\"[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Decision_making\n",
      "In psychology, decision-making (also spelled decision making and decisionmaking) is regarded as the cognitive process resulting in the selection of a belief or a course of action among several possible alternative options. It could be either rational or irrational. The decision-making process is a reasoning process based on assumptions of values, preferences and beliefs of the decision-maker.[1] Every decision-making process produces a final choice, which may or may not prompt action.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Fuzzy_logic\n",
      "\n",
      "Fuzzy logic is a form of many-valued logic in which the truth value of variables may be any real number between 0 and 1. It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false.[1] By contrast, in Boolean logic, the truth values of variables may only be the integer values 0 or 1.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Data_Cleaning\n",
      "Data cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data.[1] Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting or a data quality firewall.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Digital_art\n",
      "Digital art refers to any artistic work or practice that uses digital technology as part of the creative or presentation process. It can also refer to computational art that uses and engages with digital media.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Heuristic\n",
      "A heuristic (/hjʊˈrɪstɪk/; from Ancient Greek  εὑρίσκω (heurískō) 'to find, discover'), or heuristic technique, is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision.[1][2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Information_retrieval\n",
      "Information retrieval (IR) in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources.  Searches can be based on full-text or other content-based indexing. Information retrieval is the science[1] of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Linear_discriminant_analysis\n",
      "Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\n",
      "\n",
      "https://en.wikipedia.org/wiki/RapidMiner\n",
      "RapidMiner is a data science platform that analyses the collective impact of an organization's data. It was acquired by Altair Engineering in September 2022.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/ACM_Computing_Classification_System\n",
      "The ACM Computing Classification System (CCS) is a subject classification system for computing devised by the Association for Computing Machinery (ACM). The system is comparable to the Mathematics Subject Classification (MSC) in scope, aims, and structure, being used by the various ACM journals to organize subjects by area.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mixed_reality\n",
      "Mixed reality (MR) is a term used to describe the merging of a real-world environment and a computer-generated one. Physical and virtual objects may co-exist in mixed reality environments and interact in real time.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Fuzzy_clustering\n",
      "Fuzzy clustering (also referred to as soft clustering or soft k-means) is a form of clustering in which each data point can belong to more than one cluster.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Inference\n",
      "Inferences are steps in reasoning, moving from premises to logical consequences; etymologically, the word infer means to \"carry forward\". Inference is theoretically traditionally divided into deduction and induction, a distinction that in Europe dates at least to Aristotle (300s BCE). Deduction is inference deriving logical conclusions from premises known or assumed to be true, with the laws of valid inference being studied in logic. Induction is inference from particular evidence to a universal conclusion. A third type of inference is sometimes distinguished, notably by Charles Sanders Peirce, contradistinguishing abduction from induction.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Anomaly_detection\n",
      "In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behaviour.[1] Such examples may arouse suspicions of being generated by a different mechanism,[2] or appear inconsistent with the remainder of that set of data.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/IBM_Watson_Studio\n",
      "Watson Studio, formerly Data Science Experience or DSX, is IBM’s software platform for data science. The platform consists of a workspace that includes multiple collaboration and open-source tools for use in data science.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Automated_medical_diagnosis\n",
      "Computer-aided detection (CADe), also called computer-aided diagnosis (CADx), are systems that assist doctors in the interpretation of medical images. Imaging techniques in X-ray, MRI, Endoscopy, and ultrasound diagnostics yield a great deal of information that the radiologist or other medical professional has to analyze and evaluate comprehensively in a short time. CAD systems process digital images or videos for typical appearances and to highlight conspicuous sections, such as possible diseases, in order to offer input to support a decision taken by the professional.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Apache_Spark#MLlib_Machine_Learning_Library\n",
      "Apache Spark  is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Concurrent_computing\n",
      "Concurrent computing is a form of computing in which several computations are executed concurrently—during overlapping time periods—instead of sequentially—with one completing before the next starts.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Weight_(mathematics)\n",
      "The process of weighting involves emphasizing the contribution of particular aspects of a phenomenon (or of a set of data) over others to an outcome or result; thereby highlighting those aspects in comparison to others in the analysis. That is, rather than each variable in the data set contributing equally to the final result, some of the data is adjusted to make a greater contribution than others. This is analogous to the practice of adding (extra) weight to one side of a pair of scales in order to favour either the buyer or seller.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n",
      "Bootstrapping is any test or metric that uses random sampling with replacement (e.g. mimicking the sampling process), and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates.[1][2] This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.[3][4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Hierarchical_clustering\n",
      "In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis that seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two categories:\n",
      "\n",
      "https://en.wikipedia.org/wiki/K-means_clustering\n",
      "k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Naomi_Altman\n",
      "Naomi Altman is a statistician known for her work on kernel smoothing[KS] and kernel regression,[KR]\n",
      "and interested in applications of statistics to gene expression and genomics. She is a professor of statistics at Pennsylvania State University,[1] and a regular columnist for the \"Points of Significance\" column in Nature Methods.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Matrix_(mathematics)\n",
      "In mathematics, a matrix (pl.: matrices) is a rectangular array or table of numbers, symbols, or expressions, arranged in rows and columns, which is used to represent a mathematical object or a property of such an object.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Software_construction\n",
      "Software construction is a software engineering discipline. It is the detailed creation of working meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging. It is linked to all the other software engineering disciplines, most strongly to software design and software testing.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Bayesian_optimization\n",
      "Bayesian optimization is a sequential design strategy for global optimization of black-box functions[1][2][3]  that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ordinary_least_squares\n",
      "In statistics, ordinary least squares (OLS) is a type of linear least squares method for choosing the unknown parameters in a linear regression model (with fixed level-one effects of a linear function of a set of explanatory variables) by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the input dataset and the output of the (linear) function of the independent variable.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Database\n",
      "In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Conference_on_Knowledge_Discovery_and_Data_Mining\n",
      "SIGKDD, representing the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining, hosts an influential annual conference.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Information_system\n",
      "An information system (IS) is a formal, sociotechnical, organizational system designed to collect, process, store, and distribute information.[1] From a sociotechnical perspective, information systems are composed by four components: task, people, structure (or roles), and technology.[2] Information systems can be defined as an integration of components for collection, storage and processing of data of which the data is used to provide information, contribute to knowledge as well as digital products that facilitate decision making.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Imprecise_probability\n",
      "Imprecise probability generalizes probability theory to allow for partial probability specifications, and is applicable when information is scarce, vague, or conflicting, in which case a unique probability distribution may be hard to identify. Thereby, the theory aims to represent the available knowledge more accurately.  Imprecision is useful for dealing with expert elicitation, because:\n",
      "\n",
      "https://en.wikipedia.org/wiki/Dynamic_Bayesian_network\n",
      "A dynamic Bayesian network (DBN) is a Bayesian network (BN) which relates variables to each other over adjacent time steps. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Adaptive_website\n",
      "An adaptive website is a website that builds a model of user activity and modifies the information and/or presentation of information to the user in order to better address the user's needs.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Loss_function\n",
      "In mathematical optimization and decision theory, a loss function or cost function (sometimes also called an error function) [1] is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its opposite (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. The loss function could include terms from several levels of the hierarchy.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Random_variables\n",
      "A random variable (also called random quantity, aleatory variable, or stochastic variable) is a mathematical formalization of a quantity or object which depends on random events.[1] The term 'random variable' can be misleading as its mathematical definition is not actually random nor a variable,[2] but rather it is a function from possible outcomes (e.g., the possible upper sides of a flipped coin such as heads \n",
      "\n",
      "\n",
      "\n",
      "H\n",
      "\n",
      "\n",
      "{\\displaystyle H}\n",
      "\n",
      " and tails \n",
      "\n",
      "\n",
      "\n",
      "T\n",
      "\n",
      "\n",
      "{\\displaystyle T}\n",
      "\n",
      ") in a sample space (e.g., the set \n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "H\n",
      ",\n",
      "T\n",
      "}\n",
      "\n",
      "\n",
      "{\\displaystyle \\{H,T\\}}\n",
      "\n",
      ") to a measurable space (e.g., \n",
      "\n",
      "\n",
      "\n",
      "{\n",
      "−\n",
      "1\n",
      ",\n",
      "1\n",
      "}\n",
      "\n",
      "\n",
      "{\\displaystyle \\{-1,1\\}}\n",
      "\n",
      " in which 1 is corresponding to \n",
      "\n",
      "\n",
      "\n",
      "H\n",
      "\n",
      "\n",
      "{\\displaystyle H}\n",
      "\n",
      " and −1 is corresponding to \n",
      "\n",
      "\n",
      "\n",
      "T\n",
      "\n",
      "\n",
      "{\\displaystyle T}\n",
      "\n",
      ", respectively), often to the real numbers.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Social_software\n",
      "Social software, also known as social apps or social platform includes communications and interactive tools that are often based on the Internet. Communication tools typically handle capturing, storing and presenting communication, usually written but increasingly including audio and video as well. Interactive tools handle mediated interactions between a pair or group of users. They focus on establishing and maintaining a connection among users, facilitating the mechanics of conversation and talk.[1] Social software generally refers to software that makes collaborative behaviour, the organisation and moulding of communities, self-expression, social interaction and feedback possible for individuals. Another element of the existing definition of social software is that it allows for the structured mediation of opinion between people, in a centralized or self-regulating manner. The most improved area for social software is that Web 2.0 applications can all promote co-operation between people and the creation of online communities more than ever before. The opportunities offered by social software are instant connections and opportunities to learn.[2]An additional defining feature of social software is that apart from interaction and collaboration, it aggregates the collective behaviour of its users, allowing not only crowds to learn from an individual but individuals to learn from the crowds as well.[3] Hence, the interactions enabled by social software can be one-to-one, one-to-many, or many-to-many.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Process_control\n",
      "An industrial process control or simply process control in continuous production processes is a discipline that uses industrial control systems and control theory to achieve a production level of consistency, economy and safety which could not be achieved purely by human manual control. It is implemented widely in industries such as automotive, mining, dredging, oil refining, pulp and paper manufacturing, chemical processing and power generating plants.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Mutation_(genetic_algorithm)\n",
      "Mutation is a genetic operator used to maintain genetic diversity of the chromosomes of a population of a genetic or, more generally, an evolutionary algorithm (EA). It is analogous to biological mutation.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Medical_diagnostics\n",
      "Medical diagnosis (abbreviated Dx,[1] Dx, or Ds) is the process of determining which disease or condition explains a person's symptoms and signs. It is most often referred to as diagnosis with the medical context being implicit. The information required for diagnosis is typically collected from a history and physical examination of the person seeking medical care. Often, one or more diagnostic procedures, such as medical tests, are also done during the process. Sometimes the posthumous diagnosis is considered a kind of medical diagnosis.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Convolution\n",
      "In mathematics (in particular, functional analysis), convolution is a mathematical operation on two functions (f and g) that produces a third function (\n",
      "\n",
      "\n",
      "\n",
      "f\n",
      "∗\n",
      "g\n",
      "\n",
      "\n",
      "{\\displaystyle f*g}\n",
      "\n",
      ") that expresses how the shape of one is modified by the other. The term convolution refers to both the result function and to the process of computing it. It is defined as the integral of the product of the two functions after one is reflected about the y-axis and shifted. The choice of which function is reflected and shifted before the integral does not change the integral result (see commutativity). The integral is evaluated for all values of shift, producing the convolution function.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Nature_Methods\n",
      "Nature Methods is a monthly peer-reviewed scientific journal covering new scientific techniques. It was established in 2004 and is published by Springer Nature under the Nature Portfolio. Like other Nature journals, there is no external editorial board and editorial decisions are made by an in-house team, although peer review by external experts forms a part of the review process.[1] The editor-in-chief is Allison Doerr.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Health_informatics\n",
      "  \n",
      "  \n",
      "  \n",
      "\n",
      "https://en.wikipedia.org/wiki/User_behavior_analytics\n",
      "User behavior analytics (UBA) or user and entity behavior analytics (UEBA),[1] is the concept of analyzing the behavior of users, subjects, visitors, etc. for a specific purpose.[2] It allows cybersecurity tools to build a profile of each individual's normal activity, by looking at patterns of human behavior, and then highlighting deviations from that profile (or anomalies) that may indicate a potential compromise.[3][4][5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Document_management_system\n",
      "\n",
      "A document management system (DMS) is usually a computerized system used to store, share, track and manage files or documents. Some systems include history tracking where a log of the various versions created and modified by different users is recorded. The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Hardware_security\n",
      "Hardware security is a discipline originated from the cryptographic engineering and involves hardware design, access control, secure multi-party computation, secure key storage, ensuring code authenticity, measures to ensure that the supply chain that built the product is secure among other things.[1][2][3][4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Tensor_calculus\n",
      "In mathematics, tensor calculus, tensor analysis, or Ricci calculus is an extension of vector calculus to tensor fields (tensors that may vary over a manifold, e.g. in spacetime).\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Decision_Tree.jpg\n",
      "Decision_Tree.jpg ‎(457 × 473 pixels, file size: 17 KB, MIME type: image/jpeg)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Software_repository\n",
      "A software repository, or repo for short, is a storage location for software packages. Often a table of contents is also stored, along with metadata. A software repository is typically managed by source or version control, or repository managers. Package managers allow automatically installing and updating repositories, sometimes called \"packages\".\n",
      "\n",
      "https://en.wikipedia.org/wiki/Sensor\n",
      "A sensor is a device that produces an output signal for the purpose of sensing a physical phenomenon.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Ranking\n",
      "A ranking is a relationship between a set of items such that, for any two items, the first is either \"ranked higher than\", \"ranked lower than\", or \"ranked equal to\" the second.[1] In mathematics, this is known as a weak order or total preorder of objects. It is not necessarily a total order of objects because two different objects can have the same ranking. The rankings themselves are totally ordered. For example, materials are totally preordered by hardness, while degrees of hardness are totally ordered. If two items are the same in rank it is considered a tie.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Dependability\n",
      "In systems engineering, dependability is a measure of a system's availability, reliability, maintainability, and in some cases, other characteristics such as durability, safety and security.[1]  In real-time computing, dependability is the ability to provide services that can be trusted within a time-period.[2] The service guarantees must hold even when the system is subject to attacks or natural failures. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Differentiable_function\n",
      "In mathematics, a differentiable function of one real variable is a function whose derivative exists at each point in its domain. In other words, the graph of a differentiable function has a non-vertical tangent line at each interior point in its domain. A differentiable function is smooth (the function is locally well approximated as a linear function at each interior point) and does not contain any break, angle, or cusp.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Multimedia_database\n",
      "A Multimedia database (MMDB) is a collection of related for multimedia data.[1] The multimedia data include one or more primary media data types such as text, images, graphic objects (including drawings, sketches and illustrations) animation sequences, audio and video.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Multivariate_normal_distribution\n",
      "In probability theory and statistics, the multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions.  One definition is that a random vector is said to be k-variate normally distributed if every linear combination of its k components has a univariate normal distribution. Its importance derives mainly from the multivariate central limit theorem. The multivariate normal distribution is often used to describe, at least approximately, any set of (possibly) correlated real-valued random variables each of which clusters around a mean value.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Logical_conjunction\n",
      "In logic, mathematics and linguistics, and (\n",
      "\n",
      "\n",
      "\n",
      "∧\n",
      "\n",
      "\n",
      "{\\displaystyle \\wedge }\n",
      "\n",
      ") is the truth-functional operator of conjunction or logical conjunction. The logical connective of this operator is typically represented as \n",
      "\n",
      "\n",
      "\n",
      "∧\n",
      "\n",
      "\n",
      "{\\displaystyle \\wedge }\n",
      "\n",
      "[1] or \n",
      "\n",
      "\n",
      "\n",
      "&\n",
      "\n",
      "\n",
      "{\\displaystyle \\&}\n",
      "\n",
      " or \n",
      "\n",
      "\n",
      "\n",
      "K\n",
      "\n",
      "\n",
      "{\\displaystyle K}\n",
      "\n",
      " (prefix) or \n",
      "\n",
      "\n",
      "\n",
      "×\n",
      "\n",
      "\n",
      "{\\displaystyle \\times }\n",
      "\n",
      " or \n",
      "\n",
      "\n",
      "\n",
      "⋅\n",
      "\n",
      "\n",
      "{\\displaystyle \\cdot }\n",
      "\n",
      "[2] in which \n",
      "\n",
      "\n",
      "\n",
      "∧\n",
      "\n",
      "\n",
      "{\\displaystyle \\wedge }\n",
      "\n",
      " is the most modern and widely used.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Influence_diagram\n",
      "An influence diagram (ID) (also called a relevance diagram, decision diagram or a decision network) is a compact graphical and mathematical representation of a decision situation.  It is a generalization of a Bayesian network, in which not only probabilistic inference problems but also decision making problems (following the maximum expected utility criterion) can be modeled and solved.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Entailment\n",
      "Logical consequence (also entailment) is a fundamental concept in logic which describes the relationship between statements that hold true when one statement logically follows from one or more statements. A valid logical argument is one in which the conclusion is entailed by the premises, because the conclusion is the consequence of the premises. The philosophical analysis of logical consequence involves the questions: In what sense does a conclusion follow from its premises? and What does it mean for a conclusion to be a consequence of premises?[1] All of philosophical logic is meant to provide accounts of the nature of logical consequence and the nature of logical truth.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Sparse_matrix\n",
      "In numerical analysis and scientific computing, a sparse matrix or sparse array is a matrix in which most of the elements are zero.[1] There is no strict definition regarding the proportion of zero-value elements for a matrix to qualify as sparse but a common criterion is that the number of non-zero elements is roughly equal to the number of rows or columns. By contrast, if most of the elements are non-zero, the matrix is considered dense.[1] The number of zero-valued elements divided by the total number of elements (e.g., m × n for an m × n matrix) is sometimes referred to as the sparsity of the matrix.\n",
      "\n",
      "https://en.wikipedia.org/wiki/PMC_(identifier)\n",
      "PubMed Central (PMC) is a free digital repository that archives open access full-text scholarly articles that have been published in biomedical and life sciences journals. As one of the major research databases developed by the National Center for Biotechnology Information (NCBI), PubMed Central is more than a document repository. Submissions to PMC are indexed and formatted for enhanced metadata, medical ontology, and unique identifiers which enrich the XML structured data for each article.[1] Content within PMC can be linked to other NCBI databases and accessed via Entrez search and retrieval systems, further enhancing the public's ability to discover, read and build upon its biomedical knowledge.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Synapse\n",
      "In the nervous system, a synapse[1] is a structure that permits a neuron (or nerve cell) to pass an electrical or chemical signal to another neuron or to the target effector cell.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Springer_Science%2BBusiness_Media\n",
      "Springer Science+Business Media, commonly known as Springer, is a German multinational publishing company of books, e-books and peer-reviewed journals in science, humanities, technical and medical (STM) publishing.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Strongly_NP-hard\n",
      "In computational complexity, strong NP-completeness is a property of computational problems that is a special case of NP-completeness. A general computational problem may have numerical parameters.  For example, the input to the bin packing problem is a list of objects of specific sizes and a size for the bins that must contain the objects—these object sizes and bin size are numerical parameters.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Internet_fraud\n",
      "Internet fraud is a type of cybercrime fraud or deception which makes use of the Internet and could involve hiding of information or providing incorrect information for the purpose of tricking victims out of money, property, and inheritance.[1] Internet fraud is not considered a single, distinctive crime but covers a range of illegal and illicit actions that are committed in cyberspace.[1] It is, however, differentiated from theft since, in this case, the victim voluntarily and knowingly provides the information, money or property to the perpetrator.[2] It is also distinguished by the way it involves temporally and spatially separated offenders.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Manifold\n",
      "In mathematics, a manifold is a topological space that locally resembles Euclidean space near each point. More precisely, an \n",
      "\n",
      "\n",
      "\n",
      "n\n",
      "\n",
      "\n",
      "{\\displaystyle n}\n",
      "\n",
      "-dimensional manifold, or \n",
      "\n",
      "\n",
      "\n",
      "n\n",
      "\n",
      "\n",
      "{\\displaystyle n}\n",
      "\n",
      "-manifold for short, is a topological space with the property that each point has a neighborhood that is homeomorphic to an open subset of \n",
      "\n",
      "\n",
      "\n",
      "n\n",
      "\n",
      "\n",
      "{\\displaystyle n}\n",
      "\n",
      "-dimensional Euclidean space.\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Svm_max_sep_hyperplane_with_margin.png\n",
      "Original file ‎(800 × 862 pixels, file size: 78 KB, MIME type: image/png)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Local_outlier_factor\n",
      "In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Wikipedia:Contents\n",
      "\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Articles_with_short_description\n",
      "This category is for articles with short descriptions defined on Wikipedia by {{short description}} (either within the page itself or via another template).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Random_sample_consensus\n",
      "Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method.[1] It is a non-deterministic algorithm in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are allowed. The algorithm was first published by Fischler and Bolles at SRI International in 1981. They used RANSAC to solve the Location Determination Problem (LDP), where the goal is to determine the points in the space that project onto an image into a set of landmarks with known locations.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Real_number\n",
      "In mathematics, a real number is a number that can be used to measure a continuous one-dimensional quantity such as a distance, duration or temperature. Here, continuous means that pairs of values can have arbitrarily small differences.[a] Every real number can be almost uniquely represented by an infinite decimal expansion.[b][1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Oracle_Cloud#Platform_as_a_Service_(PaaS)\n",
      "Oracle Cloud is a cloud computing service offered by Oracle Corporation providing servers, storage, network, applications and services through a global network of Oracle Corporation managed data centers. The company allows these services to be provisioned on demand over the Internet.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Canonical_correlation\n",
      "In statistics, canonical-correlation analysis (CCA), also called canonical variates analysis, is a way of inferring information from cross-covariance matrices. If we have two vectors X = (X1, ..., Xn) and Y = (Y1, ..., Ym)  of random variables, and there are correlations among the variables, then canonical-correlation analysis will find linear combinations of X and Y which have maximum correlation with each other.[1] T. R. Knapp notes that \"virtually all of the commonly encountered parametric tests of significance can be treated as special cases of canonical-correlation analysis, which is the general procedure for investigating the relationships between two sets of variables.\"[2] The method was first introduced by Harold Hotelling in 1936,[3] although in the context of angles between flats the mathematical concept was published by Jordan in 1875.[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Chemical_synapse\n",
      "Chemical synapses are biological junctions through which neurons' signals can be sent to each other and to non-neuronal cells such as those in muscles or glands. Chemical synapses allow neurons to form circuits within the central nervous system. They are crucial to the biological computations that underlie perception and thought. They allow the nervous system to connect to and control other systems of the body.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Paraphrase\n",
      "A paraphrase (/ˈpærəˌfreɪz/) is a restatement of the meaning of a text or passage using other words. The term itself is derived via Latin paraphrasis, from Ancient Greek  παράφρασις (paráphrasis) 'additional manner of expression'. The act of paraphrasing is also called paraphrasis.\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:AI_hierarchy.svg\n",
      "Original file ‎(SVG file, nominally 399 × 399 pixels, file size: 8 KB)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Pricing\n",
      "Pricing is the process whereby a business sets the price at which it will sell its products and services, and may be part of the business's marketing plan. In setting prices, the business will take into account the price at which it could acquire the goods, the manufacturing cost, the marketplace, competition, market condition, brand, and quality of product.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Piecewise\n",
      "In mathematics, a piecewise-defined function (also called a piecewise function, a hybrid function, or definition by cases) is a function defined by multiple sub-functions, where each sub-function applies to a different interval in the domain.[1][2][3] Piecewise definition is actually a way of expressing the function, rather than a characteristic of the function itself.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Basis_function\n",
      "In mathematics,  a basis function is an element of a particular basis for a function space. Every function in the function space can be represented as a linear combination of basis functions, just as every vector in a vector space can be represented as a linear combination of basis vectors.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Indel\n",
      "Indel (insertion-deletion) is a molecular biology term for an insertion or deletion of bases in the genome of an organism. Indels ≥ 50 bases in length are classified as structural variants.[1][2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Product_placement\n",
      "Product placement, also known as embedded marketing,[1][2][3][4] is a marketing technique where references to specific brands or products are incorporated into another work, such as a film or television program, with specific promotional intent.  Much of this is done by loaning products, especially when expensive items, such as vehicles, are involved.[5]  In 2021, the agreements between brand owners and films and television programs were worth more than US$20 billion.[5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Security_service_(telecommunication)\n",
      "Security service is a service, provided by a layer of communicating open systems, which ensures adequate security of the systems or of data transfers[1] as defined by ITU-T X.800 Recommendation. \n",
      "X.800 and ISO 7498-2 (Information processing systems – Open systems interconnection – Basic Reference Model – Part 2: Security architecture)[2]  are technically aligned. This model is widely recognized [3]\n",
      "[4]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Articles_with_GND_identifiers\n",
      "This category has only the following subcategory.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Chartered_Financial_Analyst_(CFA)#Curriculum\n",
      "The Chartered Financial Analyst (CFA) program is a postgraduate professional certification offered internationally by the America based CFA Institute (formerly the Association for Investment Management and Research, or AIMR) to investment and financial professionals. The program teaches a wide range of subjects relating to advanced investment analysis—including security analysis, statistics, probability theory, fixed income, derivatives, economics, financial analysis, corporate finance, alternative investments, portfolio management—and provides a generalist knowledge of other areas of finance.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Help:Contents\n",
      "\n",
      "\n",
      "\n",
      "https://en.wikipedia.org/wiki/Financial_market\n",
      "A financial market is a market in which people trade financial securities and derivatives at low transaction costs. Some of the securities include stocks and bonds, raw materials and precious metals, which are known in the financial markets as commodities.\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Overfitted_Data.png\n",
      "Overfitted_Data.png ‎(377 × 256 pixels, file size: 14 KB, MIME type: image/png)\n",
      "\n",
      "https://en.wikipedia.org/wiki/False_positive_rate\n",
      "In statistics, when performing multiple comparisons, a false positive ratio (also known as fall-out or false alarm ratio) is the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positives) and the total number of actual negative events (regardless of classification).\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Linear_regression.svg\n",
      "Original file ‎(SVG file, nominally 438 × 289 pixels, file size: 71 KB)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Julia_Angwin\n",
      "Julia Angwin is a Pulitzer Prize-winning[1] American investigative journalist,[2] New York Times bestselling author, and entrepreneur. She was a co-founder and editor-in-chief of The Markup, a nonprofit newsroom that investigates the impact of technology on society. She was a senior reporter at ProPublica from 2014 to April 2018[3] and staff reporter at the New York bureau of The Wall Street Journal from 2000 to 2013. Angwin is author of non-fiction books, Stealing MySpace: The Battle to Control the Most Popular Website in America (2009) and Dragnet Nation (2014).[4] She is a winner and two-time finalist for the Pulitzer Prize in journalism.[5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:SpecialPages\n",
      "This page contains a list of special pages. Most of the content of these pages is automatically generated and cannot be edited. To suggest a change to the parts that can be edited, find the appropriate text on Special:AllMessages and then request your change on the talk page of the message (using {{editprotected}} to draw the attention of administrators).\n",
      "\n",
      "https://en.wikipedia.org/wiki/Goof\n",
      "A goof is a mistake. The term is also used in a number of specific senses: in cinema, it is an error or oversight during production that is visible in the released version of the film.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Structural_health_monitoring\n",
      "Structural health monitoring (SHM) involves the observation and analysis of a system over time using periodically sampled response measurements to monitor changes to the material and geometric properties of engineering structures such as bridges and buildings.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Springer_Nature\n",
      "Springer Nature or the Springer Nature Group[1][2] is a German-British academic publishing company created by the May 2015 merger of Springer Science+Business Media and Holtzbrinck Publishing Group's Nature Publishing Group, Palgrave Macmillan, and Macmillan Education.[3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Wikipedia:File_upload_wizard\n",
      "Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand copyright and the image use policy before proceeding.\n",
      "\n",
      "https://en.wikipedia.org/wiki/IEEE_Spectrum\n",
      "IEEE Spectrum is a magazine edited by the Institute of Electrical and Electronics Engineers.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:RecentChanges\n",
      "This is a list of recent changes to Wikipedia.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Hdl_(identifier)\n",
      "The Handle System is the Corporation for National Research Initiatives's proprietary registry assigning persistent identifiers, or handles, to information resources, and for resolving \"those handles into the information necessary to locate, access, and otherwise make use of the resources\".[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:SimpleBayesNetNodes.svg\n",
      "Original file ‎(SVG file, nominally 246 × 128 pixels, file size: 5 KB)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Webarchive_template_wayback_links\n",
      "The following 200 pages are in this category, out of approximately 507,488 total. This list may not reflect recent changes.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:BookSources/978-94-010-6610-5\n",
      "This page allows users to search multiple sources for a book given a 10- or 13-digit International Standard Book Number. Spaces and dashes in the ISBN do not matter.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Proper_generalized_decomposition\n",
      "The proper generalized decomposition (PGD) is an iterative numerical method for solving boundary value problems (BVPs), that is, partial differential equations constrained by a set of boundary conditions, such as the Poisson's equation or the Laplace's equation.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Haplotype\n",
      "A haplotype (haploid genotype) is a group of alleles in an organism that are inherited together from a single parent.[1][2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Point-of-sale\n",
      "The point of sale (POS) or point of purchase (POP) is the time and place at which a retail transaction is completed.  At the point of sale, the merchant calculates the amount owed by the customer, indicates that amount, may prepare an invoice for the customer (which may be a cash register printout), and indicates the options for the customer to make payment.  It is also the point at which a customer makes a payment to the merchant in exchange for goods or after provision of a service.  After receiving payment, the merchant may issue a receipt for the transaction, which is usually printed but can also be dispensed with or sent electronically.[1][2][3]\n",
      "\n",
      "https://en.wikipedia.org/wiki/OPTICS_algorithm\n",
      "Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based[1] clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and Jörg Sander.[2]\n",
      "Its basic idea is similar to DBSCAN,[3] but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. To do so, the points of the database are (linearly) ordered such that spatially closest points become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster. This is represented as a dendrogram.\n",
      "\n",
      "https://en.wikipedia.org/wiki/ISSN_(identifier)\n",
      "An International Standard Serial Number (ISSN) is an eight-digit serial number used to uniquely identify a serial publication, such as a magazine.[1] The ISSN is especially helpful in distinguishing between serials with the same title. ISSNs are used in ordering, cataloging, interlibrary loans, and other practices in connection with serial literature.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Bibcode_(identifier)\n",
      "The bibcode (also known as the refcode) is a compact identifier used by several astronomical data systems to uniquely specify literature references.\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:CLIPS.jpg\n",
      "Original file ‎(3,866 × 921 pixels, file size: 328 KB, MIME type: image/jpeg)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Raytheon_Company\n",
      "The Raytheon Company was a major U.S. defense contractor and industrial corporation with manufacturing concentrations in weapons and military and commercial electronics. It was previously involved in corporate and special-mission aircraft until early 2007. Raytheon was the world's largest producer of guided missiles.[3] In April 2020, the company merged with United Technologies Corporation to form Raytheon Technologies,[4] which, since July 2023, is known as RTX Corporation.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Machine_learning\n",
      "Enter a page name to see changes on pages linked to or from that page. (To see members of a category, enter Category:Name of category). Changes to pages on your Watchlist are shown in bold with a green bullet. See more at Help:Related changes.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Peptide_sequence\n",
      "Protein primary structure is the linear sequence of amino acids in a peptide or protein.[1] By convention, the primary structure of a protein is reported starting from the amino-terminal (N) end to the carboxyl-terminal (C) end. Protein biosynthesis is most commonly performed by ribosomes in cells. Peptides can also be synthesized in the laboratory. Protein primary structures can be directly sequenced, or inferred from DNA sequences.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:Random\n",
      "Inger Wikstrom (born 11 December 1939) is a Swedish pianist, composer and conductor.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
      "Sources: Fawcett (2006),[1] Piryonesi and El-Diraby (2020),[2]\n",
      "Powers (2011),[3] Ting (2011),[4] CAWCR,[5] D. Chicco & G. Jurman (2020, 2021, 2023),[6][7][8]  Tharwat (2018).[9] Balayla (2020)[10]\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Wikiquote-logo.svg\n",
      "Original file ‎(SVG file, nominally 300 × 355 pixels, file size: 1,012 bytes)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:MyContributions\n",
      "No changes were found matching these criteria.\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Regressions_sine_demo.svg\n",
      "Original file ‎(SVG file, nominally 900 × 450 pixels, file size: 582 KB)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Main_Page\n",
      "Florence Petty (1 December 1870 – 18 November 1948) was a Scottish social worker, cookery writer and broadcaster. During the 1900s she undertook social work in the deprived area of Somers Town in North London, demonstrating for working-class women how to cook inexpensive and nutritious foods. Much of the instruction was done in their homes. She published cookery-related works aimed at those also involved in social work, and a cookery book and pamphlet aimed at the public. From 1914 until the mid-1940s she toured Britain giving lecture-demonstrations of cost-efficient and nutritious ways to cook, including dealing with food shortages during the First World War. In the late 1920s and early 1930s, she was a BBC broadcaster on food and budgeting. Petty worked until she was in her seventies. She is considered to be a pioneer of social work innovations. Her approach to teaching the use of cheap nutritious food was a precursor to the method adopted by the Ministry of Food during the Second World War. (Full article...)\n",
      "\n",
      "https://en.wikipedia.org/wiki/Special:MyTalk\n",
      "People on Wikipedia can use this talk page to post a public message about edits made from the IP address you are currently using.\n",
      "\n",
      "https://en.wikipedia.org/wiki/File:Symbol_portal_class.svg\n",
      "Original file ‎(SVG file, nominally 180 × 185 pixels, file size: 12 KB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the links and paragraphs of the articles\n",
    "for wiki in wiki_list:\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
