{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the links, the titles and the paragraphs of the articles from the main article and sort them by similarity to the main article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web_scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'link': 'https://en.wikipedia.org/wiki/machine_learning',\n",
       " 'title': 'Machine learning',\n",
       " 'paragraph': 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\\n'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the main article\n",
    "requete = requests.get('https://en.wikipedia.org/wiki/machine_learning')\n",
    "page = BeautifulSoup(requete.text, 'html.parser')\n",
    "wiki_main = {}\n",
    "wiki_main['link'] = 'https://en.wikipedia.org/wiki/machine_learning'\n",
    "wiki_main['title'] = page.find('h1').text\n",
    "wiki_main['paragraph'] = page.find('p').text\n",
    "wiki_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the links of the articles\n",
    "links = page.find_all('a')\n",
    "http_links = [f\"{link.get('href')}\" for link in links if link.get('href') and link.get('href').startswith('/wiki')]  \n",
    "wiki_list = []\n",
    "wiki_dict_sans_doublon = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'link': '/wiki/Main_Page', 'title': 'Main Page', 'paragraph': 'December 4: Navy Day in India\\n'}, {'link': '/wiki/Wikipedia:Contents', 'title': 'Wikipedia:Contents', 'paragraph': '\\n'}, {'link': '/wiki/Portal:Current_events', 'title': 'Portal:Current events', 'paragraph': 'Edit instructions\\n'}, {'link': '/wiki/Help:Contents', 'title': 'Help:Contents', 'paragraph': '\\n\\n'}, {'link': '/wiki/Special:RecentChanges', 'title': 'Recent changes', 'paragraph': 'This is a list of recent changes to Wikipedia.\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of dictionaries containing the links, titles, and paragraphs of the articles\n",
    "for link in http_links:\n",
    "    wiki_dict = {}\n",
    "    requete = requests.get(\"https://en.wikipedia.org\" + link)\n",
    "    page = BeautifulSoup(requete.text, 'html.parser')\n",
    "    h_1 = page.find('h1')\n",
    "    p_1 = page.find('p')\n",
    "    if p_1 is not None and p_1.text not in wiki_dict_sans_doublon: # We filter the duplicates\n",
    "        wiki_dict_sans_doublon.append(p_1.text)\n",
    "        wiki_dict[\"link\"] = link\n",
    "        wiki_dict[\"title\"] = h_1.text\n",
    "        wiki_dict[\"paragraph\"] = p_1.text\n",
    "        wiki_list.append(wiki_dict)\n",
    "print(wiki_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAI client\n",
    "client = OpenAI()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding of the paragraph and title for the wiki_main article\n",
    "response = client.embeddings.create(input=wiki_main['title'] + wiki_main['paragraph'],\n",
    "                                    model=\"text-embedding-ada-002\")\n",
    "                                   \n",
    "wiki_main[\"embeddings\"] = response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(A, B):\n",
    "    #Find intersection of two sets\n",
    "    nominator = A.intersection(B)\n",
    "\n",
    "    #Find union of two sets\n",
    "    denominator = A.union(B)\n",
    "\n",
    "    #Take the ratio of sizes\n",
    "    similarity = len(nominator)/len(denominator)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding of the paragraph and title for each article in wiki_list\n",
    "for wiki in wiki_list:\n",
    "    response = client.embeddings.create(input=wiki['title'] + wiki['paragraph'],\n",
    "                                        model=\"text-embedding-ada-002\")\n",
    "            \n",
    "    wiki[\"embeddings\"] = response.data[0].embedding\n",
    "    # Calculate the similarity between the main article and each article in wiki_list using the dot product of their embeddings\n",
    "    wiki['similarity_embedding_dot_product'] = np.dot(wiki_main['embeddings'], wiki['embeddings'])\n",
    "    # Calculate the similarity between the main article and each article in wiki_list using the cosine similarity of their embeddings\n",
    "    wiki['similarity_embedding_cosine_similarity'] = np.dot(wiki_main['embeddings'], wiki['embeddings']) / (np.linalg.norm(wiki_main['embeddings']) * np.linalg.norm(wiki['embeddings']))\n",
    "    # Calculate the similarity between the main article and each article in wiki_list using the jaccard similarity of their embeddings\n",
    "    wiki['similarity_embedding_jaccard_similarity'] = jaccard_similarity(set(wiki_main['embeddings']), set(wiki['embeddings']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999546493132483\n",
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "Machine learning\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\n",
      "\n",
      "0.9274254193423537\n",
      "https://en.wikipedia.org/wiki/Category:Machine_learning\n",
      "Category:Machine learning\n",
      "Machine learning is a branch of statistics and computer science which studies algorithms and architectures that learn from observed facts.\n",
      "\n",
      "0.9115778859782548\n",
      "https://en.wikipedia.org/wiki/Automated_machine_learning\n",
      "Automated machine learning\n",
      "Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity using the dot product of their embeddings\n",
    "wiki_list.sort(key=lambda x: x['similarity_embedding_dot_product'], reverse=True)\n",
    "\n",
    "# Print the top 3 articles by similarity using the dot product of their embeddings\n",
    "for wiki in wiki_list[:3]:\n",
    "    print(wiki['similarity_embedding_dot_product'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6772509779919647\n",
      "https://en.wikipedia.org/wiki/Main_Page\n",
      "Main Page\n",
      "December 4: Navy Day in India\n",
      "\n",
      "0.6764058080086992\n",
      "https://en.wikipedia.org/wiki/File:Regressions_sine_demo.svg\n",
      "File:Regressions sine demo.svg\n",
      "Original file ‎(SVG file, nominally 900 × 450 pixels, file size: 582 KB)\n",
      "\n",
      "0.6596464515320439\n",
      "https://en.wikipedia.org/wiki/File:Symbol_portal_class.svg\n",
      "File:Symbol portal class.svg\n",
      "Original file ‎(SVG file, nominally 180 × 185 pixels, file size: 12 KB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity using the dot product of their embeddings\n",
    "wiki_list.sort(key=lambda x: x['similarity_embedding_dot_product'], reverse=True)\n",
    "\n",
    "# Print the bottom 3 articles by similarity using the dot product of their embeddings\n",
    "for wiki in wiki_list[-3:]:\n",
    "    print(wiki['similarity_embedding_dot_product'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999954589806071\n",
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "Machine learning\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\n",
      "\n",
      "0.9274253257534554\n",
      "https://en.wikipedia.org/wiki/Category:Machine_learning\n",
      "Category:Machine learning\n",
      "Machine learning is a branch of statistics and computer science which studies algorithms and architectures that learn from observed facts.\n",
      "\n",
      "0.9115778982802036\n",
      "https://en.wikipedia.org/wiki/Automated_machine_learning\n",
      "Automated machine learning\n",
      "Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity using the cosine similarity of their embeddings\n",
    "wiki_list.sort(key=lambda x: x['similarity_embedding_cosine_similarity'], reverse=True)\n",
    "\n",
    "# Print the top 3 articles by similarity using the cosine similarity of their embeddings\n",
    "for wiki in wiki_list[:3]:\n",
    "    print(wiki['similarity_embedding_cosine_similarity'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6772509442047826\n",
      "https://en.wikipedia.org/wiki/Main_Page\n",
      "Main Page\n",
      "December 4: Navy Day in India\n",
      "\n",
      "0.6764058327781174\n",
      "https://en.wikipedia.org/wiki/File:Regressions_sine_demo.svg\n",
      "File:Regressions sine demo.svg\n",
      "Original file ‎(SVG file, nominally 900 × 450 pixels, file size: 582 KB)\n",
      "\n",
      "0.6596464297870863\n",
      "https://en.wikipedia.org/wiki/File:Symbol_portal_class.svg\n",
      "File:Symbol portal class.svg\n",
      "Original file ‎(SVG file, nominally 180 × 185 pixels, file size: 12 KB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity using the cosine similarity of their embeddings\n",
    "wiki_list.sort(key=lambda x: x['similarity_embedding_cosine_similarity'], reverse=True)\n",
    "\n",
    "# Print the bottom 3 articles by similarity using the cosine similarity of their embeddings\n",
    "for wiki in wiki_list[-3:]:\n",
    "    print(wiki['similarity_embedding_cosine_similarity'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007079646017699115\n",
      "https://en.wikipedia.org/wiki/Cheminformatics\n",
      "Cheminformatics\n",
      "Cheminformatics (also known as chemoinformatics) refers to the use of physical chemistry theory with computer and information science techniques—so called \"in silico\" techniques—in application to a range of descriptive and prescriptive problems in the field of chemistry, including in its applications to biology and related molecular fields. Such in silico techniques are used, for example, by pharmaceutical companies and in academic settings to aid and inform the process of drug discovery, for instance in the design of well-defined combinatorial libraries of synthetic compounds, or to assist in structure-based drug design. The methods can also be used in chemical and allied industries, and such fields as environmental science and pharmacology, where chemical processes are involved or studied.[1]\n",
      "\n",
      "0.0007057163020465773\n",
      "https://en.wikipedia.org/wiki/Ontology_learning\n",
      "Ontology learning\n",
      "Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process.\n",
      "\n",
      "0.0007054673721340388\n",
      "https://en.wikipedia.org/wiki/SPSS_Modeler\n",
      "SPSS Modeler\n",
      "IBM SPSS Modeler is a data mining and text analytics software application from IBM. It is used to build predictive models and conduct other analytic tasks. It has a visual interface which allows users to leverage statistical and data mining algorithms without programming.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity using the jaccard similarity of their embeddings\n",
    "wiki_list.sort(key=lambda x: x['similarity_embedding_jaccard_similarity'], reverse=True)\n",
    "\n",
    "# Print the top 3 articles by similarity using the jaccard similarity of their embeddings\n",
    "for wiki in wiki_list[:3]:\n",
    "    print(wiki['similarity_embedding_jaccard_similarity'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "https://en.wikipedia.org/wiki/Main_Page\n",
      "Main Page\n",
      "December 4: Navy Day in India\n",
      "\n",
      "0.0\n",
      "https://en.wikipedia.org/wiki/File:Regressions_sine_demo.svg\n",
      "File:Regressions sine demo.svg\n",
      "Original file ‎(SVG file, nominally 900 × 450 pixels, file size: 582 KB)\n",
      "\n",
      "0.0\n",
      "https://en.wikipedia.org/wiki/File:Symbol_portal_class.svg\n",
      "File:Symbol portal class.svg\n",
      "Original file ‎(SVG file, nominally 180 × 185 pixels, file size: 12 KB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity using the jaccard similarity of their embeddings\n",
    "wiki_list.sort(key=lambda x: x['similarity_embedding_jaccard_similarity'], reverse=True)\n",
    "\n",
    "# Print the bottom 3 articles by similarity using the jaccard similarity of their embeddings\n",
    "for wiki in wiki_list[-3:]:\n",
    "    print(wiki['similarity_embedding_jaccard_similarity'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load the stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_en = stopwords.words('english')\n",
    "stop_words_ext = list(stop_en)\n",
    "vectorizer = CountVectorizer(stop_words=stop_words_ext, token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z_-]+\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cheminformatics', 'Cheminformatics (also known as chemoinformatics) refers to the use of physical chemistry theory with computer and information science techniques—so called \"in silico\" techniques—in application to a range of descriptive and prescriptive problems in the field of chemistry, including in its applications to biology and related molecular fields. Such in silico techniques are used, for example, by pharmaceutical companies and in academic settings to aid and inform the process of drug discovery, for instance in the design of well-defined combinatorial libraries of synthetic compounds, or to assist in structure-based drug design. The methods can also be used in chemical and allied industries, and such fields as environmental science and pharmacology, where chemical processes are involved or studied.[1]\\n', 'Ontology learning', \"Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process.\\n\", 'SPSS Modeler']\n"
     ]
    }
   ],
   "source": [
    "# Create the corpus by concatenating the title and the paragraph of each article\n",
    "corpus = []\n",
    "for wiki in wiki_list:\n",
    "    corpus.append(wiki['title'])\n",
    "    corpus.append(wiki['paragraph'])\n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1148x5285 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19729 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the vectorizer to the corpus\n",
    "vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector for the main article\n",
    "wiki_main['vector'] = vectorizer.transform([wiki_main['title'] + wiki_main['paragraph']]).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the similarity between the main article and each article in wiki_list using the dot product of their vectors\n",
    "for wiki in wiki_list:\n",
    "    wiki['vector'] = vectorizer.transform([wiki['title'] + wiki['paragraph']]).toarray()[0]\n",
    "    # Calculate the similarity between the main article and each article in wiki_list using the dot product of their vectors\n",
    "    wiki['similarity_vector_dot_product'] = np.dot(wiki_main['vector'], wiki['vector'])\n",
    "    # Calculate the similarity between the main article and each article in wiki_list using the cosine similarity of their vectors\n",
    "    wiki['similarity_vector_cosine_similarity'] = np.dot(wiki_main['vector'], wiki['vector']) / (np.linalg.norm(wiki_main['vector']) * np.linalg.norm(wiki['vector']))\n",
    "    # Calculate the similarity between the main article and each article in wiki_list using the jaccard similarity of their vectors\n",
    "    wiki['similarity_vector_jaccard_similarity'] = jaccard_similarity(set(wiki_main['vector']), set(wiki['vector']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "Machine learning\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\n",
      "\n",
      "31\n",
      "https://en.wikipedia.org/wiki/Online_machine_learning\n",
      "Online machine learning\n",
      "In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.\n",
      "\n",
      "29\n",
      "https://en.wikipedia.org/wiki/Meta-learning_(computer_science)\n",
      "Meta-learning (computer science)\n",
      "Meta learning[1][2]\n",
      "is a subfield of machine learning where automatic learning algorithms are applied to metadata about machine learning experiments. As of 2017, the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to learn.[1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity to the main article using the dot product of their vectors\n",
    "wiki_list.sort(key=lambda x: x['similarity_vector_dot_product'], reverse=True)\n",
    "\n",
    "# Print the top 3 articles by similarity to the main article using the dot product of their vectors\n",
    "for wiki in wiki_list[:3]:\n",
    "    print(wiki['similarity_vector_dot_product'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "https://en.wikipedia.org/wiki/Main_Page\n",
      "Main Page\n",
      "December 4: Navy Day in India\n",
      "\n",
      "0\n",
      "https://en.wikipedia.org/wiki/File:Regressions_sine_demo.svg\n",
      "File:Regressions sine demo.svg\n",
      "Original file ‎(SVG file, nominally 900 × 450 pixels, file size: 582 KB)\n",
      "\n",
      "0\n",
      "https://en.wikipedia.org/wiki/File:Symbol_portal_class.svg\n",
      "File:Symbol portal class.svg\n",
      "Original file ‎(SVG file, nominally 180 × 185 pixels, file size: 12 KB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity to the main article using the dot product of their vectors\n",
    "wiki_list.sort(key=lambda x: x['similarity_vector_dot_product'], reverse=True)\n",
    "\n",
    "# Print the bottom 3 articles by similarity to the main article using the dot product of their vectors\n",
    "for wiki in wiki_list[-3:]:\n",
    "    print(wiki['similarity_vector_dot_product'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "Machine learning\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\n",
      "\n",
      "0.3279680246763151\n",
      "https://en.wikipedia.org/wiki/Computational_learning_theory\n",
      "Computational learning theory\n",
      "In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.[1]\n",
      "\n",
      "0.3218393429334682\n",
      "https://en.wikipedia.org/wiki/Adversarial_machine_learning\n",
      "Adversarial machine learning\n",
      "Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks.[1] A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.[2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity to the main article using the cosine similarity of their vectors\n",
    "wiki_list.sort(key=lambda x: x['similarity_vector_cosine_similarity'], reverse=True)\n",
    "\n",
    "# Print the top 3 articles by similarity to the main article using the cosine similarity of their vectors\n",
    "for wiki in wiki_list[:3]:\n",
    "    print(wiki['similarity_vector_cosine_similarity'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "https://en.wikipedia.org/wiki/Main_Page\n",
      "Main Page\n",
      "December 4: Navy Day in India\n",
      "\n",
      "0.0\n",
      "https://en.wikipedia.org/wiki/File:Regressions_sine_demo.svg\n",
      "File:Regressions sine demo.svg\n",
      "Original file ‎(SVG file, nominally 900 × 450 pixels, file size: 582 KB)\n",
      "\n",
      "0.0\n",
      "https://en.wikipedia.org/wiki/File:Symbol_portal_class.svg\n",
      "File:Symbol portal class.svg\n",
      "Original file ‎(SVG file, nominally 180 × 185 pixels, file size: 12 KB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity to the main article using the cosine similarity of their vectors\n",
    "wiki_list.sort(key=lambda x: x['similarity_vector_cosine_similarity'], reverse=True)\n",
    "\n",
    "# Print the bottom 3 articles by similarity to the main article using the cosine similarity of their vectors\n",
    "for wiki in wiki_list[-3:]:\n",
    "    print(wiki['similarity_vector_cosine_similarity'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "Machine learning\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\n",
      "\n",
      "1.0\n",
      "https://en.wikipedia.org/wiki/Neural_Designer\n",
      "Neural Designer\n",
      "Neural Designer is a software tool for machine learning based on neural networks, a main area of artificial intelligence research, and contains a graphical user interface which simplifies data entry and interpretation of results.\n",
      "\n",
      "1.0\n",
      "https://en.wikipedia.org/wiki/Machine_Learning_(journal)\n",
      "Machine Learning (journal)\n",
      "Machine Learning  is a peer-reviewed scientific journal, published since 1986.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity to the main article using the jaccard similarity of their vectors\n",
    "wiki_list.sort(key=lambda x: x['similarity_vector_jaccard_similarity'], reverse=True)\n",
    "\n",
    "# Print the top 3 articles by similarity to the main article using the jaccard similarity of their vectors\n",
    "for wiki in wiki_list[:3]:\n",
    "    print(wiki['similarity_vector_jaccard_similarity'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n",
      "https://en.wikipedia.org/wiki/White-box_testing\n",
      "White-box testing\n",
      "White-box testing (also known as clear box testing, glass box testing, transparent box testing, and structural testing) is a method of software testing that tests internal structures or workings of an application, as opposed to its functionality (i.e. black-box testing). In white-box testing, an internal perspective of the system is used to design test cases. The tester chooses inputs to exercise paths through the code and determine the expected outputs. This is analogous to testing nodes in a circuit, e.g. in-circuit testing (ICT).\n",
      "White-box testing can be applied at the unit, integration and system levels of the software testing process. Although traditional testers tended to think of white-box testing as being done at the unit level, it is used for integration and system testing more frequently today. It can test paths within a unit, paths between units during integration, and between subsystems during a system–level test. Though this method of test design can uncover many errors or problems, it has the potential to miss unimplemented parts of the specification or missing requirements. Where white-box testing is design-driven,[1] that is, driven exclusively by agreed specifications of how each component of software is required to behave (as in DO-178C and ISO 26262 processes), white-box test techniques can accomplish assessment for unimplemented or missing requirements.\n",
      "\n",
      "0.3333333333333333\n",
      "https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence\n",
      "Philosophy of artificial intelligence\n",
      "The philosophy of artificial intelligence is a branch of the philosophy of mind and the philosophy of computer science[1] that explores artificial intelligence and its implications for knowledge and understanding of intelligence, ethics, consciousness, epistemology, and free will.[2][3] Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; see artificial life) so the discipline is of considerable interest to philosophers.[4] These factors contributed to the emergence of the philosophy of artificial intelligence. \n",
      "\n",
      "0.3333333333333333\n",
      "https://en.wikipedia.org/wiki/Knowledge_graph_embedding\n",
      "Knowledge graph embedding\n",
      "In representation learning, knowledge graph embedding (KGE), also referred to as knowledge representation learning (KRL), or multi-relation learning,[1] is a machine learning task of learning a low-dimensional representation of a knowledge graph's entities and relations while preserving their semantic meaning.[1][2][3]  Leveraging their embedded representation, knowledge graphs (KGs) can be used for various applications such as link prediction, triple classification, entity recognition, clustering, and relation extraction.[1][4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity to the main article using the jaccard similarity of their vectors\n",
    "wiki_list.sort(key=lambda x: x['similarity_vector_jaccard_similarity'], reverse=True)\n",
    "\n",
    "# Print the bottom 3 articles by similarity to the main article using the jaccard similarity of their vectors\n",
    "for wiki in wiki_list[-3:]:\n",
    "    print(wiki['similarity_vector_jaccard_similarity'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text vectorization + Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\Simplon datai\\simplon_datai_2025\\13-web_scrapping\\web-scrapping.ipynb Cell 31\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Simplon%20datai/simplon_datai_2025/13-web_scrapping/web-scrapping.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# # Create an embedding of the text vector for the main article\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Simplon%20datai/simplon_datai_2025/13-web_scrapping/web-scrapping.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# response = client.embeddings.create(input=wiki_main['vector'].tolist(),\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Simplon%20datai/simplon_datai_2025/13-web_scrapping/web-scrapping.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#                                     model=\"text-embedding-ada-002\")\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/Simplon%20datai/simplon_datai_2025/13-web_scrapping/web-scrapping.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m wiki_main[\u001b[39m\"\u001b[39m\u001b[39membeddings_vector\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mstack([wiki_main[\u001b[39m'\u001b[39;49m\u001b[39membeddings\u001b[39;49m\u001b[39m'\u001b[39;49m], wiki_main[\u001b[39m'\u001b[39;49m\u001b[39mvector\u001b[39;49m\u001b[39m'\u001b[39;49m]]) \n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Simplon datai\\simplon_datai_2025\\13-web_scrapping\\jo\\Lib\\site-packages\\numpy\\core\\shape_base.py:449\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[0;32m    447\u001b[0m shapes \u001b[39m=\u001b[39m {arr\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays}\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shapes) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 449\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mall input arrays must have the same shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    451\u001b[0m result_ndim \u001b[39m=\u001b[39m arrays[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mndim \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    452\u001b[0m axis \u001b[39m=\u001b[39m normalize_axis_index(axis, result_ndim)\n",
      "\u001b[1;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "# # Create an embedding of the text vector for the main article\n",
    "# response = client.embeddings.create(input=wiki_main['vector'].tolist(),\n",
    "#                                     model=\"text-embedding-ada-002\")\n",
    "                                   \n",
    "wiki_main[\"embeddings_vector\"] = np.stack([wiki_main['embeddings'], wiki_main['vector']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an embedding of the paragraph and title for each article in wiki_list\n",
    "# for wiki in wiki_list:\n",
    "#     response = client.embeddings.create(input=wiki['vector'].tolist(),\n",
    "#                                         model=\"text-embedding-ada-002\")\n",
    "                                    \n",
    "wiki[\"embeddings_vector\"] =  wiki['embeddings'] + wiki['vector'] \n",
    "# Calculate the similarity between the main article and each article in wiki_list using the dot product of their embeddings\n",
    "wiki['similarity_embedding_vector__dot_product'] = np.dot(wiki_main['embeddings_vector'], wiki['embeddings_vector'])\n",
    "# Calculate the similarity between the main article and each article in wiki_list using the cosine similarity of their embeddings\n",
    "wiki['similarity_embedding_vector__cosine_similarity'] = np.dot(wiki_main['embeddings_vector'], wiki['embeddings_vector']) / (np.linalg.norm(wiki_main['embeddings_vector']) * np.linalg.norm(wiki['embeddings_vector']))\n",
    "# Calculate the similarity between the main article and each article in wiki_list using the jaccard similarity of their embeddings\n",
    "wiki['similarity_embedding_vector_jaccard_similarity'] = jaccard_similarity(set(wiki_main['embeddings_vector']), set(wiki['embeddings_vector']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999384978688\n",
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "Machine learning\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\n",
      "\n",
      "0.9997666410099177\n",
      "https://en.wikipedia.org/wiki/Mean_shift\n",
      "Mean shift\n",
      "Mean shift is a non-parametric feature-space mathematical analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm.[1] Application domains include cluster analysis in computer vision and image processing.[2]\n",
      "\n",
      "0.9996989615378631\n",
      "https://en.wikipedia.org/wiki/Social_computing\n",
      "Social computing\n",
      "Social computing is an area of computer science that is concerned with the intersection of social behavior and computational systems. It is based on creating or recreating social conventions and social contexts through the use of software and technology. Thus, blogs, email, instant messaging, social network services, wikis, social bookmarking and other instances of what is often called social software illustrate ideas from social computing.   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity using the dot product of their embeddings vectors\n",
    "wiki_list.sort(key=lambda x: x['similarity_embedding_vector__dot_product'], reverse=True)\n",
    "\n",
    "# Print the top 3 articles by similarity using the dot product of their embeddings vectors\n",
    "for wiki in wiki_list[:3]:\n",
    "    print(wiki['similarity_embedding_vector__dot_product'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904951025012385\n",
      "https://en.wikipedia.org/wiki/Echo_state_network\n",
      "Echo state network\n",
      "An echo state network (ESN)[1][2] is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can produce or reproduce specific temporal patterns. The main interest of this network is that although its behavior is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.\n",
      "\n",
      "0.8890255243586205\n",
      "https://en.wikipedia.org/wiki/Vision_transformer\n",
      "Vision transformer\n",
      "A vision transformer (ViT) is a transformer designed for computer vision. Transformers were introduced in 2017,[1] and have found widespread use in natural language processing. In 2020, they were adapted for computer vision, yielding ViT.[2] The basic structure is to break down input images as a series of patches, then tokenized, before applying the tokens to a standard Transformer architecture.\n",
      "\n",
      "0.8794527829989941\n",
      "https://en.wikipedia.org/wiki/Isolation_forest\n",
      "Isolation forest\n",
      "Isolation Forest is an algorithm for data anomaly detection initially developed by Fei Tony Liu in 2008.[1] Isolation Forest detects anomalies using binary trees. The algorithm has a linear time complexity and a low memory requirement, which works well with high-volume data.[2][3]\n",
      "In essence, the algorithm relies upon the characteristics of anomalies, i.e., being few and different, in order to detect anomalies. No density estimation is performed in the algorithm. The algorithm is different from decision tree algorithms in that only the path-length measure or approximation is being used to generate the anomaly score, no leaf node statistics on class distribution or target value is needed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity using the dot product of their embeddings vectors\n",
    "wiki_list.sort(key=lambda x: x['similarity_embedding_vector__dot_product'], reverse=True)\n",
    "\n",
    "# Print the bottom 3 articles by similarity using the dot product of their embeddings vectors\n",
    "for wiki in wiki_list[-3:]:\n",
    "    print(wiki['similarity_embedding_vector__dot_product'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999997766233459\n",
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "Machine learning\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\n",
      "\n",
      "0.9997665618787897\n",
      "https://en.wikipedia.org/wiki/Mean_shift\n",
      "Mean shift\n",
      "Mean shift is a non-parametric feature-space mathematical analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm.[1] Application domains include cluster analysis in computer vision and image processing.[2]\n",
      "\n",
      "0.999698875240512\n",
      "https://en.wikipedia.org/wiki/Social_computing\n",
      "Social computing\n",
      "Social computing is an area of computer science that is concerned with the intersection of social behavior and computational systems. It is based on creating or recreating social conventions and social contexts through the use of software and technology. Thus, blogs, email, instant messaging, social network services, wikis, social bookmarking and other instances of what is often called social software illustrate ideas from social computing.   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity using the cosine similarity of their embeddings vectors\n",
    "wiki_list.sort(key=lambda x: x['similarity_embedding_vector__cosine_similarity'], reverse=True)\n",
    "\n",
    "# Print the top 3 articles by similarity using the cosine similarity of their embeddings vectors\n",
    "for wiki in wiki_list[:3]:\n",
    "    print(wiki['similarity_embedding_vector__cosine_similarity'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9049509810174128\n",
      "https://en.wikipedia.org/wiki/Echo_state_network\n",
      "Echo state network\n",
      "An echo state network (ESN)[1][2] is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can produce or reproduce specific temporal patterns. The main interest of this network is that although its behavior is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.\n",
      "\n",
      "0.8890254786238526\n",
      "https://en.wikipedia.org/wiki/Vision_transformer\n",
      "Vision transformer\n",
      "A vision transformer (ViT) is a transformer designed for computer vision. Transformers were introduced in 2017,[1] and have found widespread use in natural language processing. In 2020, they were adapted for computer vision, yielding ViT.[2] The basic structure is to break down input images as a series of patches, then tokenized, before applying the tokens to a standard Transformer architecture.\n",
      "\n",
      "0.8794526893529735\n",
      "https://en.wikipedia.org/wiki/Isolation_forest\n",
      "Isolation forest\n",
      "Isolation Forest is an algorithm for data anomaly detection initially developed by Fei Tony Liu in 2008.[1] Isolation Forest detects anomalies using binary trees. The algorithm has a linear time complexity and a low memory requirement, which works well with high-volume data.[2][3]\n",
      "In essence, the algorithm relies upon the characteristics of anomalies, i.e., being few and different, in order to detect anomalies. No density estimation is performed in the algorithm. The algorithm is different from decision tree algorithms in that only the path-length measure or approximation is being used to generate the anomaly score, no leaf node statistics on class distribution or target value is needed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity using the cosine similarity of their embeddings vectors\n",
    "wiki_list.sort(key=lambda x: x['similarity_embedding_vector__cosine_similarity'], reverse=True)\n",
    "\n",
    "# Print the bottom 3 articles by similarity using the cosine similarity of their embeddings vectors\n",
    "for wiki in wiki_list[-3:]:\n",
    "    print(wiki['similarity_embedding_vector__cosine_similarity'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002127659574468085\n",
      "https://en.wikipedia.org/wiki/Rule-based_machine_learning\n",
      "Rule-based machine learning\n",
      "Rule-based machine learning (RBML) is a term in computer science intended to encompass any machine learning method that identifies, learns, or evolves 'rules' to store, manipulate or apply.[1][2][3] The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[clarification needed][citation needed]\n",
      "\n",
      "0.0010634526763559022\n",
      "https://en.wikipedia.org/wiki/Theoretical_computer_science\n",
      "Theoretical computer science\n",
      "Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, formal language theory, the lambda calculus and type theory.\n",
      "\n",
      "0.0003563791874554526\n",
      "https://en.wikipedia.org/wiki/Programming_team\n",
      "Programming team\n",
      "A programming team is a team of people who develop or maintain computer software.[1]  They may be organised in numerous ways, but the egoless programming team and chief programmer team have been common structures.[2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity using the jaccard similarity of their embeddings vectors\n",
    "wiki_list.sort(key=lambda x: x['similarity_embedding_vector_jaccard_similarity'], reverse=True)\n",
    "\n",
    "# Print the top 3 articles by similarity using the jaccard similarity of their embeddings vectors\n",
    "for wiki in wiki_list[:3]:\n",
    "    print(wiki['similarity_embedding_vector_jaccard_similarity'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "https://en.wikipedia.org/wiki/Echo_state_network\n",
      "Echo state network\n",
      "An echo state network (ESN)[1][2] is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can produce or reproduce specific temporal patterns. The main interest of this network is that although its behavior is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.\n",
      "\n",
      "0.0\n",
      "https://en.wikipedia.org/wiki/Vision_transformer\n",
      "Vision transformer\n",
      "A vision transformer (ViT) is a transformer designed for computer vision. Transformers were introduced in 2017,[1] and have found widespread use in natural language processing. In 2020, they were adapted for computer vision, yielding ViT.[2] The basic structure is to break down input images as a series of patches, then tokenized, before applying the tokens to a standard Transformer architecture.\n",
      "\n",
      "0.0\n",
      "https://en.wikipedia.org/wiki/Isolation_forest\n",
      "Isolation forest\n",
      "Isolation Forest is an algorithm for data anomaly detection initially developed by Fei Tony Liu in 2008.[1] Isolation Forest detects anomalies using binary trees. The algorithm has a linear time complexity and a low memory requirement, which works well with high-volume data.[2][3]\n",
      "In essence, the algorithm relies upon the characteristics of anomalies, i.e., being few and different, in order to detect anomalies. No density estimation is performed in the algorithm. The algorithm is different from decision tree algorithms in that only the path-length measure or approximation is being used to generate the anomaly score, no leaf node statistics on class distribution or target value is needed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the wiki_list by similarity using the jaccard similarity of their embeddings vectors\n",
    "wiki_list.sort(key=lambda x: x['similarity_embedding_vector_jaccard_similarity'], reverse=True)\n",
    "\n",
    "# Print the bottom 3 articles by similarity using the jaccard similarity of their embeddings vectors\n",
    "for wiki in wiki_list[-3:]:\n",
    "    print(wiki['similarity_embedding_vector_jaccard_similarity'])\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
