{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the links, the titles and the paragraphs of the articles from the main article and sort them by similarity to the main article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web_scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'link': 'https://en.wikipedia.org/wiki/machine_learning',\n",
       " 'title': 'Machine learning',\n",
       " 'paragraph': 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\\n'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the main article\n",
    "requete = requests.get('https://en.wikipedia.org/wiki/machine_learning')\n",
    "page = BeautifulSoup(requete.text, 'html.parser')\n",
    "wiki_main = {}\n",
    "wiki_main['link'] = 'https://en.wikipedia.org/wiki/machine_learning'\n",
    "wiki_main['title'] = page.find('h1').text\n",
    "wiki_main['paragraph'] = page.find('p').text\n",
    "wiki_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the links of the articles\n",
    "links = page.find_all('a')\n",
    "http_links = [f\"{link.get('href')}\" for link in links if link.get('href') and link.get('href').startswith('/wiki')]  \n",
    "wiki_list = []\n",
    "wiki_dict_sans_doublon = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'link': '/wiki/Main_Page', 'title': 'Main Page', 'paragraph': 'Florence Petty (1\\xa0December 1870\\xa0â€“ 18\\xa0November 1948) was a Scottish social worker, cookery writer and broadcaster. During the 1900s she undertook social work in the deprived area of Somers Town in North London, demonstrating for working-class women how to cook inexpensive and nutritious foods. Much of the instruction was done in their homes. She published cookery-related works aimed at those also involved in social work, and a cookery book and pamphlet aimed at the public. From 1914 until the mid-1940s she toured Britain giving lecture-demonstrations of cost-efficient and nutritious ways to cook, including dealing with food shortages during the First World War. In the late 1920s and early 1930s, she was a BBC broadcaster on food and budgeting. Petty worked until she was in her seventies. She is considered to be a pioneer of social work innovations. Her approach to teaching the use of cheap nutritious food was a precursor to the method adopted by the Ministry of Food during the Second World War. (Full\\xa0article...)\\n'}, {'link': '/wiki/Wikipedia:Contents', 'title': 'Wikipedia:Contents', 'paragraph': '\\n'}, {'link': '/wiki/Portal:Current_events', 'title': 'Portal:Current events', 'paragraph': 'Edit instructions\\n'}, {'link': '/wiki/Help:Contents', 'title': 'Help:Contents', 'paragraph': '\\n\\n'}, {'link': '/wiki/Special:RecentChanges', 'title': 'Recent changes', 'paragraph': 'This is a list of recent changes to Wikipedia.\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of dictionaries containing the links, titles, and paragraphs of the articles\n",
    "for link in http_links:\n",
    "    wiki_dict = {}\n",
    "    requete = requests.get(\"https://en.wikipedia.org\" + link)\n",
    "    page = BeautifulSoup(requete.text, 'html.parser')\n",
    "    h_1 = page.find('h1')\n",
    "    p_1 = page.find('p')\n",
    "    if p_1 is not None and p_1.text not in wiki_dict_sans_doublon: # We filter the duplicates\n",
    "        wiki_dict_sans_doublon.append(p_1.text)\n",
    "        wiki_dict[\"link\"] = link\n",
    "        wiki_dict[\"title\"] = h_1.text\n",
    "        wiki_dict[\"paragraph\"] = p_1.text\n",
    "        wiki_list.append(wiki_dict)\n",
    "print(wiki_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAI client\n",
    "client = OpenAI()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding of the paragraph and title for the wiki_main article\n",
    "response = client.embeddings.create(\n",
    "    input=wiki_main['title'] + wiki_main['paragraph'],\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "wiki_main[\"embeddings\"] = response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding of the paragraph and title for each article in wiki_list\n",
    "for wiki in wiki_list:\n",
    "    response = client.embeddings.create(\n",
    "        input=wiki['title'] + wiki['paragraph'],\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    wiki[\"embeddings\"] = response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance between the main article and each article in wiki_list using the dot product of their embeddings\n",
    "for wiki in wiki_list:\n",
    "    wiki['distance_embedding'] = np.dot(wiki_main['embeddings'], wiki['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the wiki_list by distance to the main article\n",
    "wiki_list.sort(key=lambda x: x['distance_embedding'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "Machine learning\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Category:Machine_learning\n",
      "Category:Machine learning\n",
      "Machine learning is a branch of statistics and computer science which studies algorithms and architectures that learn from observed facts.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Automated_machine_learning\n",
      "Automated machine learning\n",
      "Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Deep_learning\n",
      "Deep learning\n",
      "Deep learning is the subset of machine learning methods which are based on artificial neural networks with representation learning. The adjective \"deep\" in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.[2]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Rule-based_machine_learning\n",
      "Rule-based machine learning\n",
      "Rule-based machine learning (RBML) is a term in computer science intended to encompass any machine learning method that identifies, learns, or evolves 'rules' to store, manipulate or apply.[1][2][3] The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[clarification needed][citation needed]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the links and paragraphs of the articles\n",
    "for wiki in wiki_list[:5]:\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load the stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_en = stopwords.words('english')\n",
    "stop_words_ext = list(stop_en)\n",
    "vectorizer = CountVectorizer(stop_words=stop_words_ext, token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z_-]+\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine learning', 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\\n', 'Category:Machine learning', 'Machine learning is a branch of statistics and computer science which studies algorithms and architectures that learn from observed facts.\\n', 'Automated machine learning']\n"
     ]
    }
   ],
   "source": [
    "# Create the corpus by concatenating the title and the paragraph of each article\n",
    "corpus = []\n",
    "for wiki in wiki_list:\n",
    "    corpus.append(wiki['title'])\n",
    "    corpus.append(wiki['paragraph'])\n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1150x5329 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19858 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the vectorizer to the corpus\n",
    "vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector for the main article\n",
    "wiki_main['vector'] = vectorizer.transform([wiki_main['title'] + wiki_main['paragraph']]).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance between the main article and each article in wiki_list using the dot product of their vectors\n",
    "for wiki in wiki_list:\n",
    "    wiki['vector'] = vectorizer.transform([wiki['title'] + wiki['paragraph']]).toarray()[0]\n",
    "    wiki['distance_vector'] = np.dot(wiki_main['vector'], wiki['vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the wiki_list by distance to the main article\n",
    "wiki_list.sort(key=lambda x: x['distance_vector'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "Machine learning\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.[2][3] Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture and medicine, where it is too costly to develop algorithms to perform the needed tasks.[4][5]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Online_machine_learning\n",
      "Online machine learning\n",
      "In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Meta-learning_(computer_science)\n",
      "Meta-learning (computer science)\n",
      "Meta learning[1][2]\n",
      "is a subfield of machine learning where automatic learning algorithms are applied to metadata about machine learning experiments. As of 2017, the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to learn.[1]\n",
      "\n",
      "https://en.wikipedia.org/wiki/Manifold_regularization\n",
      "Manifold regularization\n",
      "In machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is smooth: data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition.\n",
      "\n",
      "https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence\n",
      "Philosophy of artificial intelligence\n",
      "The philosophy of artificial intelligence is a branch of the philosophy of mind and the philosophy of computer science[1] that explores artificial intelligence and its implications for knowledge and understanding of intelligence, ethics, consciousness, epistemology, and free will.[2][3] Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; see artificial life) so the discipline is of considerable interest to philosophers.[4] These factors contributed to the emergence of the philosophy of artificial intelligence. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the links and paragraphs of the articles\n",
    "for wiki in wiki_list[:5]:\n",
    "    print(\"https://en.wikipedia.org\" + wiki['link'])\n",
    "    print(wiki['title'])\n",
    "    print(wiki['paragraph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
